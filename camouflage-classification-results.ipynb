{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761},{"sourceId":13514489,"sourceType":"datasetVersion","datasetId":8580489},{"sourceId":13517101,"sourceType":"datasetVersion","datasetId":8582404}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/poorvaahuja/camouflage-classification-results?scriptVersionId=274800129\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm","metadata":{"_uuid":"d0b94887-4b0f-4317-84c8-1a7777d760d0","_cell_guid":"53c610f7-732b-435e-9459-de2f9e3d80c7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:00.849612Z","iopub.execute_input":"2025-11-01T10:14:00.849973Z","iopub.status.idle":"2025-11-01T10:14:10.94302Z","shell.execute_reply.started":"2025-11-01T10:14:00.849949Z","shell.execute_reply":"2025-11-01T10:14:10.942239Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom collections import Counter","metadata":{"_uuid":"23ff4136-a399-4879-b737-c64e8a7b9c6d","_cell_guid":"63fe8b36-a02c-4487-a769-3e87f2989073","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:10.943869Z","iopub.execute_input":"2025-11-01T10:14:10.944399Z","iopub.status.idle":"2025-11-01T10:14:10.947933Z","shell.execute_reply.started":"2025-11-01T10:14:10.944369Z","shell.execute_reply":"2025-11-01T10:14:10.947096Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)","metadata":{"_uuid":"2072ad41-1769-40e0-9dc2-ff2324551b11","_cell_guid":"8c96bf9f-8a4a-4b5b-ba3c-cc2904b1ab58","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:10.949645Z","iopub.execute_input":"2025-11-01T10:14:10.949856Z","iopub.status.idle":"2025-11-01T10:14:11.020476Z","shell.execute_reply.started":"2025-11-01T10:14:10.94984Z","shell.execute_reply":"2025-11-01T10:14:11.019866Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 8         # adjust if OOM\nEPOCHS = 30\nNUM_WORKERS = 4         # set 0 if worker issues on Kaggle\nLR = 3e-4\nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True\n\n# Loss weights from PDF suggestion\nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# warmup epochs\nWARMUP_EPOCHS = 5\n\n# early stopping\nEARLY_STOPPING_PATIENCE = 15\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4","metadata":{"_uuid":"b9c000da-bf1a-4523-bbb1-7b28ff015f10","_cell_guid":"1ca1b022-43ef-4a4f-9bc8-9502667f799e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.021303Z","iopub.execute_input":"2025-11-01T10:14:11.021554Z","iopub.status.idle":"2025-11-01T10:14:11.026259Z","shell.execute_reply.started":"2025-11-01T10:14:11.021537Z","shell.execute_reply":"2025-11-01T10:14:11.025615Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Noise + Transform","metadata":{"_uuid":"32b3ea07-c3c9-4592-81c8-7e7c5f53b060","_cell_guid":"6b93dda5-4bd4-4cc5-897f-2e3f7ce8d878","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])","metadata":{"_uuid":"8610d19e-aa2c-4c65-8dd3-63efff268002","_cell_guid":"a36a568a-587b-4e7d-95ea-3a45d5fa1652","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.026973Z","iopub.execute_input":"2025-11-01T10:14:11.027197Z","iopub.status.idle":"2025-11-01T10:14:11.039758Z","shell.execute_reply.started":"2025-11-01T10:14:11.027173Z","shell.execute_reply":"2025-11-01T10:14:11.039078Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\nstrong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()]) # Placeholder for actual strong transform\nval_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\n# Function to try reading a file with multiple encodings\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            print(f\"Failed to read {file_path} with encoding {encoding}\")\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n    image_paths = []\n    labels = []\n    \n    # List of encodings to try\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break  # If successful, break out of the loop\n        except UnicodeDecodeError:\n            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n        except Exception as e:\n            print(f\"Error reading {info_file}: {e}\")\n            raise  # Raise the error if it's something unexpected\n    \n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            # Ensure label is always 0 or 1\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue # Skip malformed lines\n                \n            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n\n# MultiDataset class with proper encoding handling\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        # Handle the testing dataset (testing-dataset images and labels)\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n                self.samples.append((img_path, label)) \n        \n        # Process other datasets (COD10K, CAMO, etc.)\n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}\")\n\n            # Use the read_file_with_encoding function to handle different encodings\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    # Fallback classification if label is not an integer\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            # Map labels to binary (CAM=1, Non-CAM=0)\n            lbl = 1 if lbl == 1 else 0\n\n            found = False\n            search_subs = [\n                \"\",  # If image is directly in root_dir (less common)\n                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n            ]\n            \n            base_fname = os.path.basename(fname)  \n\n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n            if not found:\n                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"âœ… Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        \n        # 1. SAFELY UNPACK sample tuple (length 2 or 3)\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        # Define rdir for consistent mask lookup logic\n        if len(sample) == 3:\n            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n            rdir = sample[2]\n            # Root directory for testing-dataset (used as fallback for mask lookup)\n            testing_root = None \n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            \n            # Use the defined rdir for mask search\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    # Convert to binary mask: 0 or 1\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\ndef build_weighted_sampler(dataset):\n    \"\"\"\n    Builds a WeightedRandomSampler based on class imbalance.\n    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n    \"\"\"\n    \n    # Safely extract labels (always index 1, regardless of tuple length)\n    labels = [sample[1] for sample in dataset.samples]  \n    \n    counts = Counter(labels)\n    total = len(labels)\n    \n    # Ensure there are at least two classes to calculate class_weights\n    if len(counts) <= 1:\n        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n        weights = [1.0] * total\n    else:\n        # Calculate inverse frequency weights\n        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n        weights = [class_weights[lbl] for lbl in labels]\n        \n    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)","metadata":{"_uuid":"8a0980ba-9609-4279-9de2-cd19d8a166b9","_cell_guid":"0439529c-c10a-4627-b237-6d1ea7c53173","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.040538Z","iopub.execute_input":"2025-11-01T10:14:11.040739Z","iopub.status.idle":"2025-11-01T10:14:11.061111Z","shell.execute_reply.started":"2025-11-01T10:14:11.040716Z","shell.execute_reply":"2025-11-01T10:14:11.060504Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"0e0569d2-4f3f-47be-ba5a-80aac8370acb","_cell_guid":"b86311ae-c1bb-4f22-91c0-d3d43ff59d08","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \n# COD10K Info files\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\n# CAMO-COCO PATHS\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n\n# CAMO-COCO Info files\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\n# CAMO-COCO Root Directories\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\ntesting_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n\n# Split testing-dataset: 20% train (train_paths), 80% validation (val_paths)\n# testing dataset: NC4K+places365 (images and labels text file)\nfrom sklearn.model_selection import train_test_split # Make sure this import is present\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    testing_image_paths, testing_labels, test_size=0.8, random_state=42\n)\n\n# 1. All Root Directories\nALL_ROOT_DIRS = [\n    train_dir_cod,       \n    test_dir_cod,       \n    train_dir_camo_cam,  \n    train_dir_camo_noncam\n]\n\n# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\nALL_TRAIN_TXTS = [\n    train_cam_txt, train_noncam_txt, test_cam_txt, test_noncam_txt,\n    train_cam_txt2, train_noncam_txt2, test_cam_txt2, test_noncam_txt2,\n]\n\n# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\nALL_VAL_TXTS = []\n\n# Training Dataset: All external data (via ALL_TRAIN_TXTS) + 20% testing-dataset split (via train_paths)\ntrain_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS, \n    txt_files=ALL_TRAIN_TXTS,               \n    testing_image_paths=train_paths,        \n    testing_labels=train_labels,            \n    weak_transform=weak_tf, \n    strong_transform=strong_tf, \n    use_masks=USE_SEGMENTATION\n)\n\n# Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\nval_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS,  \n    txt_files=ALL_VAL_TXTS,                 \n    testing_image_paths=val_paths,          \n    testing_labels=val_labels,              \n    weak_transform=val_tf, \n    strong_transform=None, \n    use_masks=USE_SEGMENTATION\n)\n\n# Build Sampler and DataLoader\ntrain_sampler = build_weighted_sampler(train_ds)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nprint(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))\n","metadata":{"_uuid":"2977418b-5a9b-4b00-bd2b-6e6d1380c03a","_cell_guid":"8612016c-9564-42f6-be27-48e77e56c6eb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.061891Z","iopub.execute_input":"2025-11-01T10:14:11.062075Z","iopub.status.idle":"2025-11-01T10:15:18.966571Z","shell.execute_reply.started":"2025-11-01T10:14:11.062059Z","shell.execute_reply":"2025-11-01T10:15:18.965997Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Backbones","metadata":{"_uuid":"9147384c-1b50-4cf1-93a9-da0d89a0615f","_cell_guid":"27410294-010a-4911-a7fe-8047a41f96b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats","metadata":{"_uuid":"0562f881-a9a2-425b-84f7-b87f9f1de827","_cell_guid":"7c590a73-9329-416b-b0e9-3cc63b6c61f7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:18.967314Z","iopub.execute_input":"2025-11-01T10:15:18.967535Z","iopub.status.idle":"2025-11-01T10:15:18.973608Z","shell.execute_reply.started":"2025-11-01T10:15:18.967518Z","shell.execute_reply":"2025-11-01T10:15:18.973058Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SwinExtractor(nn.Module):\n    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n    def forward(self, x):\n        return self.model(x)","metadata":{"_uuid":"4b565d5e-4ecb-457c-af1a-b89084719789","_cell_guid":"602ab64f-0bcd-48fd-9c92-789c331c1ced","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:18.975859Z","iopub.execute_input":"2025-11-01T10:15:18.976061Z","iopub.status.idle":"2025-11-01T10:15:18.98853Z","shell.execute_reply.started":"2025-11-01T10:15:18.976045Z","shell.execute_reply":"2025-11-01T10:15:18.987998Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction,4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)","metadata":{"_uuid":"77d2613c-fd8c-4bc9-a03a-3fbfa9258379","_cell_guid":"595c7ef4-f65e-4c97-818d-db3dea350d48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:18.989136Z","iopub.execute_input":"2025-11-01T10:15:18.989338Z","iopub.status.idle":"2025-11-01T10:15:19.000335Z","shell.execute_reply.started":"2025-11-01T10:15:18.989324Z","shell.execute_reply":"2025-11-01T10:15:18.999706Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        if H.shape[2:] != X.shape[2:]:\n            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n        g = self.g_fc(H)\n        return g * H + (1 - g) * X","metadata":{"_uuid":"a7ab254f-2a14-444a-bd57-e45aa8620035","_cell_guid":"3691f4eb-fe5e-4a90-8355-281bfb975eee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.001047Z","iopub.execute_input":"2025-11-01T10:15:19.00129Z","iopub.status.idle":"2025-11-01T10:15:19.013597Z","shell.execute_reply.started":"2025-11-01T10:15:19.001265Z","shell.execute_reply":"2025-11-01T10:15:19.013006Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, d_cnn, d_swin, d_out):\n        super().__init__()\n        self.q = nn.Linear(d_cnn, d_out)\n        self.k = nn.Linear(d_swin, d_out)\n        self.v = nn.Linear(d_swin, d_out)\n        self.scale = d_out ** -0.5\n    def forward(self, feat_cnn, feat_swin):\n        B, Cc, H, W = feat_cnn.shape\n        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n        if feat_swin.dim() == 4:\n            Bs, Cs, Hs, Ws = feat_swin.shape\n            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n        else:\n            kv = feat_swin\n        K = self.k(kv)\n        V = self.v(kv)\n        Q = self.q(q)\n        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, V)\n        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n        return out","metadata":{"_uuid":"95e9bd01-a25d-40bd-95d3-0d52401e3fd5","_cell_guid":"8e151f36-8629-46d4-8978-9dc6f44a874d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.01421Z","iopub.execute_input":"2025-11-01T10:15:19.014652Z","iopub.status.idle":"2025-11-01T10:15:19.026114Z","shell.execute_reply.started":"2025-11-01T10:15:19.014635Z","shell.execute_reply":"2025-11-01T10:15:19.025414Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Segmentation Decoder","metadata":{"_uuid":"fdfd48ad-a120-426f-812d-eab414adf245","_cell_guid":"8d1a2d8c-d55a-4a1c-8242-9619a3169e93","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class SegDecoder(nn.Module):\n    def __init__(self, in_channels_list, mid_channels=128):\n        super().__init__()\n        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n        self.out = nn.Conv2d(mid_channels, 1, 1)\n    def forward(self, feat_list):\n        target_size = feat_list[0].shape[2:]\n        ups = []\n        for f, p in zip(feat_list, self.projs):\n            x = p(f)\n            if x.shape[2:] != target_size:\n                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n            ups.append(x)\n        x = torch.cat(ups, dim=1)\n        x = self.conv(x)\n        x = self.out(x)\n        return x","metadata":{"_uuid":"59784957-96f9-43f9-ab4c-ba4c5069c266","_cell_guid":"5ea62e96-bc4e-4218-a0ba-7009414c6204","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.026836Z","iopub.execute_input":"2025-11-01T10:15:19.027135Z","iopub.status.idle":"2025-11-01T10:15:19.04381Z","shell.execute_reply.started":"2025-11-01T10:15:19.027107Z","shell.execute_reply":"2025-11-01T10:15:19.043241Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Probing Backbones","metadata":{"_uuid":"f9354086-efdb-49de-9956-72bdc5c1c804","_cell_guid":"226cd599-783c-4f41-8d35-ebcf350381b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"dnet = DenseNetExtractor().to(device).eval()\nmnet = MobileNetExtractor().to(device).eval()\nsnet = SwinExtractor().to(device).eval()\nwith torch.no_grad():\n    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n    featsA = dnet(dummy)\n    featsB = mnet(dummy)\n    featsS = snet(dummy)\nchA = [f.shape[1] for f in featsA]\nchB = [f.shape[1] for f in featsB]\nchS = [f.shape[1] for f in featsS]\nprint(\"DenseNet channels:\", chA)\nprint(\"MobileNet channels:\", chB)\nprint(\"Swin channels:\", chS)","metadata":{"_uuid":"c92e84d5-b62f-4c5c-baff-5897fd5bc9fe","_cell_guid":"2a3f1c47-b68b-4563-becc-b0d6171967fa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.044574Z","iopub.execute_input":"2025-11-01T10:15:19.044837Z","iopub.status.idle":"2025-11-01T10:15:23.668509Z","shell.execute_reply.started":"2025-11-01T10:15:19.044816Z","shell.execute_reply":"2025-11-01T10:15:23.667892Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fusion Model (DenseNet + MobileNet + Swin cross attention)","metadata":{"_uuid":"42955826-da73-49eb-a2b6-1eb868ac3be0","_cell_guid":"93c9df34-5655-4f06-8252-ebbc742d4db7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class FusionWithSwin(nn.Module):\n    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n        super().__init__()\n        self.backA = DenseNetExtractor()\n        self.backB = MobileNetExtractor()\n        self.backS = SwinExtractor()\n        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n        self.L = L\n        self.d = d\n        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n        self.reduce = nn.Conv2d(d * L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n        self.use_seg = use_seg\n        if self.use_seg:\n            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n\n        # Domain head for DANN (simple MLP)\n        self.domain_head = nn.Sequential(\n            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, x, grl_lambda=0.0):\n        fa = self.backA(x)\n        fb = self.backB(x)\n        fs = self.backS(x)\n        fused_feats = []\n        aligned_for_dec = []\n        for i in range(self.L):\n            a = self.alignA[i](fa[i])\n            a = self.cbamA[i](a)\n            b = self.alignB[i](fb[i])\n            b = self.cbamB[i](b)\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused = self.gates[i](a, b)\n            swin_feat = fs[i]\n            swin_att = self.cross_atts[i](fused, swin_feat)\n            if swin_att.shape[2:] != fused.shape[2:]:\n                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n            fused = fused + swin_att\n            fused_feats.append(fused)\n            aligned_for_dec.append(fused)\n        target = fused_feats[-1]\n        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.reduce(concat)\n        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n        logits = self.classifier(z)\n        out = {\"logits\": logits, \"feat\": z}\n        if self.use_seg:\n            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n\n        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n        if grl_lambda > 0.0:\n            # GRL implemented outside (we'll pass z through GRL function)\n            pass\n        out[\"domain_logits\"] = self.domain_head(z)\n        return out\n\n# instantiate model\nmodel = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\nprint(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)","metadata":{"_uuid":"71df8e36-6644-423d-86c1-0b9cc366e4b4","_cell_guid":"e1370fdf-9622-4230-b0ee-8c6cfe884584","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:23.669252Z","iopub.execute_input":"2025-11-01T10:15:23.669567Z","iopub.status.idle":"2025-11-01T10:15:24.915804Z","shell.execute_reply.started":"2025-11-01T10:15:23.669548Z","shell.execute_reply":"2025-11-01T10:15:24.915019Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss_logits(pred_logits, target):\n    pred = torch.sigmoid(pred_logits)\n    target = target.float()\n    inter = (pred * target).sum(dim=(1,2,3))\n    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n    return 1.0 - dice.mean()\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nseg_bce = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)","metadata":{"_uuid":"66ed0d4e-2188-4630-b4b8-da16939ceaf1","_cell_guid":"8981b77b-e686-4024-a7ef-106fc71c5573","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.916629Z","iopub.execute_input":"2025-11-01T10:15:24.917161Z","iopub.status.idle":"2025-11-01T10:15:24.926959Z","shell.execute_reply.started":"2025-11-01T10:15:24.917136Z","shell.execute_reply":"2025-11-01T10:15:24.926108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Supervised contrastive Loss\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        # features: [N, D], labels: [N]\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        # remove diagonal\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        # for each i, positive samples are where mask==1 (excluding self)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        # avoid divide by zero\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        # average only across anchors that have positives\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\nsupcon_loss_fn = SupConLoss(temperature=0.07)","metadata":{"_uuid":"27aad2d9-4477-4430-b87c-2805045736df","_cell_guid":"6ab89b5b-ccc8-4af3-9590-bc566204b65a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.927665Z","iopub.execute_input":"2025-11-01T10:15:24.927896Z","iopub.status.idle":"2025-11-01T10:15:24.944316Z","shell.execute_reply.started":"2025-11-01T10:15:24.927871Z","shell.execute_reply":"2025-11-01T10:15:24.943656Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Domain Adversarial: Gradient Reversal Layer (GRL)\n\nfrom torch.autograd import Function\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)","metadata":{"_uuid":"ffb8ae99-636a-43c6-9797-f85ea97f4b17","_cell_guid":"937a9a70-b252-4638-91b4-0acbcd8214f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.945141Z","iopub.execute_input":"2025-11-01T10:15:24.945299Z","iopub.status.idle":"2025-11-01T10:15:24.959356Z","shell.execute_reply.started":"2025-11-01T10:15:24.945285Z","shell.execute_reply":"2025-11-01T10:15:24.958815Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizer + scheduler + mixed precision + clipping\n# -----------------------------\n# param groups: smaller LR for backbones, larger for heads\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2},\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# warmup + cosine schedule\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        # cosine from warmup -> total\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# -----------------------------\n# Mixup & CutMix helpers\n# -----------------------------\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)   # use builtin int\n    cut_h = int(H * cut_rat)   # use builtin int\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted","metadata":{"_uuid":"73e24310-551f-4554-bb03-48f305bb7ae3","_cell_guid":"5d0a6d0b-5ee0-459f-a9c4-f361ce6591ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.960063Z","iopub.execute_input":"2025-11-01T10:15:24.960255Z","iopub.status.idle":"2025-11-01T10:15:24.978605Z","shell.execute_reply.started":"2025-11-01T10:15:24.960237Z","shell.execute_reply":"2025-11-01T10:15:24.977996Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{"_uuid":"98de1d78-23d6-4f6e-8856-1c2bf4801903","_cell_guid":"8cefc78d-dfc4-4a80-853c-de8cdbaa0092","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"best_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    # mix_info: (mode, y_a, y_b, lam) or None\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        # mixup/cutmix: soft labels\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            # focal is not designed for soft labels; approximate by weighted CE\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\nhistory = {\n    'train_loss': [], 'train_f1': [], 'train_acc': [],\n    'val_loss': [], 'val_f1': [], 'val_acc': []\n}\nfor epoch in range(1, EPOCHS+1):\n    # --- CHANGED: Improved freeze/unfreeze strategy ---\n    # We set FREEZE_EPOCHS = 5 in cell 5 (to match WARMUP_EPOCHS)\n    if epoch <= FREEZE_EPOCHS:\n        # Freeze ALL backbone parameters\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['backA', 'backB', 'backS']):\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        # Unfreeze all parameters *once* after freeze period\n        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n    # --- End of change ---\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    # 1. Initialize zero_grad at the start of the epoch\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n        imgs = weak_imgs\n\n        # optionally apply mixup/cutmix on imgs (on weak view)\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n            out = model(imgs)  # returns logits, feat, seg, domain_logits\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            domain_logits = out.get(\"domain_logits\", None)\n\n            # classification loss (label-smoothing or focal)\n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # segmentation loss if available & mask present\n            seg_loss = 0.0\n            if USE_SEGMENTATION and (masks is not None):\n                seg_pred = out[\"seg\"]\n                seg_loss = seg_loss_fn(seg_pred, masks)\n            # supcon loss on features (use features from weak)\n            supcon_loss = supcon_loss_fn(feat, labels)\n\n            # consistency: forward strong view and compare predictions\n            out_strong = model(strong_imgs)\n            logits_strong = out_strong[\"logits\"]\n            probs_weak = F.softmax(logits.detach(), dim=1)\n            probs_strong = F.softmax(logits_strong, dim=1)\n            # L2 between probability vectors (could be KL)\n            cons_loss = F.mse_loss(probs_weak, probs_strong)\n            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n            dom_loss = 0.0\n\n            # --- CHANGED: Use GAMMA_SEG to weight segmentation loss ---\n            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n\n            # 2. Scale the loss by accumulation steps to average the gradients\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        # Perform backward pass (gradients are accumulated until step is called)\n        scaler.scale(total_loss).backward()\n\n        # 3. Optimizer step only every ACCUMULATION_STEPS batches\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            # gradient clipping before step\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() # Prepare for next accumulation cycle\n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS # Re-scale back for correct loss tracking\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # 4. Take a final step if there are remaining gradients (i.e., last batch was not a multiple of ACCUMULATION_STEPS)\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    # metrics (rest of the code remains the same)\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                # --- CHANGED: Use GAMMA_SEG in validation loss calculation as well ---\n                loss += GAMMA_SEG * seg_loss_fn(seg_out, masks)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    history['train_loss'].append(running_loss / max(1, n_batches))\n    history['train_f1'].append(f1)\n    history['train_acc'].append(acc)\n    history['val_loss'].append(val_loss / max(1, len(val_loader)))\n    history['val_f1'].append(vf1)\n    history['val_acc'].append(vacc)\n\n    # early stopping & save best\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)","metadata":{"_uuid":"8f0812c9-bc6d-452f-b8dd-b4983872595a","_cell_guid":"4e9e207b-133f-40e0-8941-e99fb09ea18c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.979402Z","iopub.execute_input":"2025-11-01T10:15:24.979616Z","execution_failed":"2025-11-01T12:02:10.303Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test-time augmentation (TTA) helper\n# -----------------------------\ndef tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n    model.eval()\n    logits_accum = None\n    with torch.no_grad():\n        for s in scales:\n            tf = transforms.Compose([\n                transforms.Resize((s, s)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n            x = tf(img_pil).unsqueeze(0).to(device)\n            out = model(x)\n            logits = out[\"logits\"]\n            if flip:\n                x_f = torch.flip(x, dims=[3])\n                logits_f = model(x_f)[\"logits\"]\n                logits = (logits + logits_f) / 2.0\n            if logits_accum is None:\n                logits_accum = logits\n            else:\n                logits_accum += logits\n    logits_accum /= len(scales)\n    return logits_accum","metadata":{"_uuid":"651a579b-350f-4e7b-8a86-656d77f18664","_cell_guid":"69b67bd8-3130-4ad2-a32d-5c37cc0d2cd0","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-11-01T12:02:10.304Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grad-CAM helper (very simple)\n# -----------------------------\ndef get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n    \"\"\"\n    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n    Returns upsampled heatmap (H,W) normalized in [0,1].\n    \"\"\"\n    model.eval()\n    # find layer\n    target_module = None\n    for name, module in model.named_modules():\n        if name == layer_name:\n            target_module = module\n            break\n    if target_module is None:\n        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, input, output):\n        activations.append(output.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    h1 = target_module.register_forward_hook(forward_hook)\n    h2 = target_module.register_full_backward_hook(backward_hook)\n\n    out = model(input_tensor)\n    logits = out[\"logits\"]\n    if target_class is None:\n        target_class = logits.argmax(1).item()\n    loss = logits[:, target_class].sum()\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    act = activations[0]  # [B,C,H,W]\n    grad = gradients[0]   # [B,C,H,W]\n    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n    cam = F.relu(cam)\n    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n    cam = cam.squeeze().cpu().numpy()\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h1.remove(); h2.remove()\n    return cam","metadata":{"_uuid":"5e4aa318-69be-4863-bbf3-ff3c2c037fbd","_cell_guid":"83d30302-59ed-41f9-8b5d-eb7dae9b98b0","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-11-01T12:02:10.304Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"pip install matplotlib seaborn scikit-learn graphviz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import graphviz\nfrom graphviz import Digraph\n\n# Create a new directed graph\ndot = Digraph(comment='FusionWithSwin Model Architecture')\ndot.attr(rankdir='TB', nodesep='0.5', ranksep='1.0') # Top-to-Bottom layout\n\n# Define node styles\ndot.attr('node', shape='box', style='filled', fillcolor='lightblue', fontname='Helvetica')\ndot.attr('edge', fontname='Helvetica')\n\n# 1. Inputs\nwith dot.subgraph(name='cluster_input') as c:\n    c.attr(label='Input', style='filled', color='lightgrey')\n    c.node('InputImg', 'Input Image (x)\\n[B, 3, 224, 224]')\n\n# 2. Backbones\nwith dot.subgraph(name='cluster_backbones') as c:\n    c.attr(label='Feature Extractors', style='filled', color='lightgrey')\n    c.node('DenseNet', 'DenseNetExtractor (backA)')\n    c.node('MobileNet', 'MobileNetExtractor (backB)')\n    c.node('Swin', 'SwinExtractor (backS)')\n\n# 3. Fusion Path (DenseNet + MobileNet)\nwith dot.subgraph(name='cluster_cnn_fusion') as c:\n    c.attr(label='CNN Fusion Path (L stages)', style='filled', color='azure')\n    c.node('AlignA', 'Align (Conv 1x1)')\n    c.node('AlignB', 'Align (Conv 1x1)')\n    c.node('CBAMA', 'CBAM-lite (A)')\n    c.node('CBAMB', 'CBAM-lite (B)')\n    c.node('Gated', 'GatedFusion')\n    \n    # Connections\n    dot.edge('AlignA', 'CBAMA')\n    dot.edge('AlignB', 'CBAMB')\n    dot.edge('CBAMA', 'Gated')\n    dot.edge('CBAMB', 'Gated')\n\n# 4. Cross-Attention\nwith dot.subgraph(name='cluster_cross_attn') as c:\n    c.attr(label='Cross-Attention Fusion (L stages)', style='filled', color='honeydew')\n    c.node('CrossAttn', 'CrossAttention')\n    c.node('Add', 'Element-wise Add')\n    c.node('Concat', 'Feature Concat (L levels)')\n    \n    # Connections\n    dot.edge('Gated', 'CrossAttn', label='Q (from CNN)')\n    dot.edge('CrossAttn', 'Add')\n    dot.edge('Gated', 'Add', label='Shortcut')\n    dot.edge('Add', 'Concat')\n\n# 5. Output Heads\nwith dot.subgraph(name='cluster_outputs') as c:\n    c.attr(label='Output Heads', style='filled', color='lightgrey')\n    c.node('Reduce', 'Reduce (Conv 1x1)')\n    c.node('GAP', 'GlobalAvgPool2d (z)')\n    c.node('Classifier', 'Classifier Head (logits)')\n    c.node('DomainHead', 'Domain Head (domain_logits)')\n    c.node('SegDecoder', 'Segmentation Decoder (seg)')\n    \n    # Connections\n    dot.edge('Concat', 'Reduce')\n    dot.edge('Reduce', 'GAP')\n    dot.edge('GAP', 'Classifier')\n    dot.edge('GAP', 'DomainHead', label='GRL')\n    dot.edge('Add', 'SegDecoder', label='Fused Features') # From fusion stages\n\n# Global Connections\ndot.edge('InputImg', 'DenseNet')\ndot.edge('InputImg', 'MobileNet')\ndot.edge('InputImg', 'Swin')\n\ndot.edge('DenseNet', 'AlignA', label='fa[i]')\ndot.edge('MobileNet', 'AlignB', label='fb[i]')\ndot.edge('Swin', 'CrossAttn', label='K, V (from Swin fs[i])')\n\n\n# Render the graph\ndot.render('model_architecture', view=True, format='png')\nprint(\"Model architecture flowchart saved as 'model_architecture.png'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Place this AFTER your main training loop\nepochs_ran = len(history['train_loss'])\nepochs = range(1, epochs_ran + 1)\n\nplt.figure(figsize=(14, 6))\n\n# Plot Loss\nplt.subplot(1, 2, 1)\nplt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\nplt.plot(epochs, history['val_loss'], 'r-s', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot F1-Score\nplt.subplot(1, 2, 2)\nplt.plot(epochs, history['train_f1'], 'b-o', label='Training F1-Score')\nplt.plot(epochs, history['val_f1'], 'r-s', label='Validation F1-Score')\nplt.title('Training and Validation F1-Score')\nplt.xlabel('Epoch')\nplt.ylabel('F1-Score (Macro)')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.savefig('training_curves.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport pandas as pd\n\n# Load the best model weights\nprint(f\"Loading best model from {SAVE_PATH} (Epoch {best_epoch})\")\ncheckpoint = torch.load(SAVE_PATH, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state'])\nmodel.to(device)\nmodel.eval()\n\n# Get predictions from the BEST model\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for weak_imgs, _, labels, _ in tqdm(val_loader, desc=\"Generating Confusion Matrix\"):\n        imgs = weak_imgs.to(device)\n        out = model(imgs)\n        logits = out[\"logits\"]\n        all_preds.extend(logits.argmax(1).cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Generate confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nclass_names = ['Non-CAM', 'CAM'] # Assuming 0=Non-CAM, 1=CAM\n\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})\nplt.title('Confusion Matrix (Best Model)')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.savefig('confusion_matrix.png')\nplt.show()\n\n# Optional: Print detailed classification report\nprint(\"\\nClassification Report (Best Model):\\n\")\nprint(classification_report(all_labels, all_preds, target_names=class_names, zero_division=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# Get features (z) from the BEST model\nall_feats = []\nall_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for weak_imgs, _, labels, _ in tqdm(val_loader, desc=\"Extracting features for t-SNE\"):\n        imgs = weak_imgs.to(device)\n        out = model(imgs)\n        all_feats.append(out[\"feat\"].cpu())\n        all_labels.append(labels.cpu())\n\nall_feats = torch.cat(all_feats).numpy()\nall_labels = torch.cat(all_labels).numpy()\n\nprint(f\"Running t-SNE on {all_feats.shape[0]} samples...\")\ntsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=1000)\ntsne_results = tsne.fit_transform(all_feats)\n\n# Plot t-SNE\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=all_labels, cmap='coolwarm', alpha=0.6)\nplt.title('t-SNE Visualization of Learned Features (z)')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, ['Non-CAM', 'CAM'], title='Classes')\nplt.grid(True, linestyle='--', alpha=0.3)\nplt.savefig('tsne_features.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inverse normalization for plotting\ndef denormalize(tensor):\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n    return tensor * std + mean\n\n# Get a sample from the validation dataset (which uses val_tf)\n# We need to re-load the PIL image to apply weak/strong transforms\nsample_idx = 10 # Pick any sample\nimg_path, label = val_ds.samples[sample_idx][0], val_ds.samples[sample_idx][1]\nimg_pil = Image.open(img_path).convert(\"RGB\")\n\n# Apply the transforms\n# Note: Your train_ds transforms are defined globally as weak_tf and strong_tf\nimg_val_tensor = val_tf(img_pil)\nimg_weak_tensor = weak_tf(img_pil)\nimg_strong_tensor = strong_tf(img_pil)\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle(f'Data Augmentation Example (Class: {\"CAM\" if label==1 else \"Non-CAM\"})', fontsize=16)\n\n# Original (from val_tf)\naxes[0].imshow(denormalize(img_val_tensor).permute(1, 2, 0).clip(0, 1))\naxes[0].set_title('Original (Validation Transform)')\naxes[0].axis('off')\n\n# Weak Augmentation\naxes[1].imshow(denormalize(img_weak_tensor).permute(1, 2, 0).clip(0, 1))\naxes[1].set_title('Weak Augmentation (weak_tf)')\naxes[1].axis('off')\n\n# Strong Augmentation\naxes[2].imshow(denormalize(img_strong_tensor).permute(1, 2, 0).clip(0, 1))\naxes[2].set_title('Strong Augmentation (strong_tf)')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.savefig('augmentation_example.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the best model\ncheckpoint = torch.load(SAVE_PATH, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state'])\nmodel.to(device)\nmodel.eval()\n\n# Get a batch from the validation loader\ndata_iter = iter(val_loader)\nweak_imgs, _, labels, masks = next(data_iter)\n\n# Select a few images to show\nnum_to_show = 4\nimgs_to_plot = weak_imgs[:num_to_show].to(device)\nmasks_to_plot = masks[:num_to_show].to(device)\n\n# Get model predictions\nwith torch.no_grad():\n    out = model(imgs_to_plot)\n    seg_preds = out['seg'] # These are logits\n    seg_probs = torch.sigmoid(seg_preds) # Probabilities [0, 1]\n    seg_binary = (seg_probs > 0.5).float() # Binary mask\n\n# Move to CPU for plotting\nimgs_to_plot = imgs_to_plot.cpu()\nmasks_to_plot = masks_to_plot.cpu()\nseg_binary = seg_binary.cpu()\n\nfig, axes = plt.subplots(num_to_show, 3, figsize=(15, 5 * num_to_show))\nfig.suptitle('Qualitative Segmentation Results (Best Model)', fontsize=16, y=1.02)\n\nfor i in range(num_to_show):\n    # Denormalize image\n    img = denormalize(imgs_to_plot[i]).permute(1, 2, 0).clip(0, 1)\n    \n    # 1. Original Image\n    axes[i, 0].imshow(img)\n    axes[i, 0].set_title(f'Original Image (Sample {i})')\n    axes[i, 0].axis('off')\n\n    # 2. Ground Truth Mask\n    axes[i, 1].imshow(masks_to_plot[i].squeeze(), cmap='gray')\n    axes[i, 1].set_title('Ground Truth Mask')\n    axes[i, 1].axis('off')\n\n    # 3. Predicted Mask\n    axes[i, 2].imshow(seg_binary[i].squeeze(), cmap='gray')\n    axes[i, 2].set_title('Predicted Mask (Threshold 0.5)')\n    axes[i, 2].axis('off')\n\nplt.tight_layout()\nplt.savefig('segmentation_results.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2 # OpenCV is often used for heatmap overlay\n\n# Load the best model\ncheckpoint = torch.load(SAVE_PATH, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state'])\nmodel.to(device)\nmodel.eval()\n\n# Get a sample\ndata_iter = iter(val_loader)\nweak_imgs, _, labels, _ = next(data_iter)\n\n# Select one image\nimg_tensor = weak_imgs[0].unsqueeze(0).to(device) # [1, 3, 224, 224]\nlabel = labels[0].item()\n\n# Use your provided Grad-CAM function\n# Let's target the last DenseNet block as you did\nheatmap_np = get_gradcam_heatmap(\n    model, \n    img_tensor, \n    target_class=label, \n    layer_name='backA.features.denseblock4' # Or try 'backS.model.stages.3'\n)\n\n# Convert tensor image back to plottable format\nimg_np = denormalize(img_tensor.squeeze().cpu()).permute(1, 2, 0).numpy().clip(0, 1)\n# Convert to 8-bit for OpenCV\nimg_bgr = cv2.cvtColor((img_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n\n# Resize heatmap and apply colormap\nheatmap_resized = cv2.resize(heatmap_np, (img_bgr.shape[1], img_bgr.shape[0]))\nheatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n\n# Superimpose heatmap\nsuperimposed_img = cv2.addWeighted(heatmap_colored, 0.4, img_bgr, 0.6, 0)\nsuperimposed_img_rgb = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nfig.suptitle(f'Grad-CAM (Target Class: {\"CAM\" if label==1 else \"Non-CAM\"})', fontsize=16)\n\naxes[0].imshow(img_np)\naxes[0].set_title('Original Image')\naxes[0].axis('off')\n\naxes[1].imshow(heatmap_resized, cmap='jet')\naxes[1].set_title('Grad-CAM Heatmap')\naxes[1].axis('off')\n\naxes[2].imshow(superimposed_img_rgb)\naxes[2].set_title('Overlayed Image')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.savefig('gradcam_result.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}