{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761},{"sourceId":13514489,"sourceType":"datasetVersion","datasetId":8580489},{"sourceId":13517101,"sourceType":"datasetVersion","datasetId":8582404}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/poorvaahuja/camouflage-classification?scriptVersionId=273485443\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom collections import Counter\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)\n\nIMG_SIZE = 224\nBATCH_SIZE = 8          # adjust if OOM\nEPOCHS = 30             # CHANGED: Was 20. Train longer.\nNUM_WORKERS = 4         # set 0 if worker issues on Kaggle\nLR = 3e-4\nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True\n\n# Loss weights from PDF suggestion\nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3       # CHANGED: Was 0.2. Increased weight for contrastive loss.\nETA_CONS = 0.1\nGAMMA_SEG = 0.5         # ADDED: New weight for auxiliary segmentation loss.\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# warmup epochs\nWARMUP_EPOCHS = 5\n\n# early stopping\nEARLY_STOPPING_PATIENCE = 15\nFREEZE_EPOCHS = 5       # CHANGED: Was 10. Align with WARMUP_EPOCHS for a standard fine-tuning strategy.\nACCUMULATION_STEPS = 4  # CHANGED: Was 2. Increase effective batch size to 8*4=32.\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nweak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\nstrong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()]) # Placeholder for actual strong transform\nval_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\n# Function to try reading a file with multiple encodings\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            print(f\"Failed to read {file_path} with encoding {encoding}\")\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n    image_paths = []\n    labels = []\n    \n    # List of encodings to try\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break  # If successful, break out of the loop\n        except UnicodeDecodeError:\n            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n        except Exception as e:\n            print(f\"Error reading {info_file}: {e}\")\n            raise  # Raise the error if it's something unexpected\n    \n    # Process the lines if file was successfully read\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            # Ensure label is always 0 or 1\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue # Skip malformed lines\n                \n            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n\n# MultiDataset class with proper encoding handling\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        # Handle the testing dataset (testing-dataset images and labels)\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n                self.samples.append((img_path, label)) \n        \n        # Process other datasets (COD10K, CAMO, etc.)\n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}\")\n\n            # Use the read_file_with_encoding function to handle different encodings\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    # Fallback classification if label is not an integer\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            # Map labels to binary (CAM=1, Non-CAM=0)\n            lbl = 1 if lbl == 1 else 0\n\n            found = False\n            search_subs = [\n                \"\",  # If image is directly in root_dir (less common)\n                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n            ]\n            \n            base_fname = os.path.basename(fname)  \n\n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n            if not found:\n                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        \n        # 1. SAFELY UNPACK sample tuple (length 2 or 3)\n        img_path = sample[0]\n        lbl = sample[1]\n        \n        # Define rdir for consistent mask lookup logic\n        if len(sample) == 3:\n            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n            rdir = sample[2]\n            # Root directory for testing-dataset (used as fallback for mask lookup)\n            testing_root = None \n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            \n            # Use the defined rdir for mask search\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    # Convert to binary mask: 0 or 1\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\ndef build_weighted_sampler(dataset):\n    \"\"\"\n    Builds a WeightedRandomSampler based on class imbalance.\n    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n    \"\"\"\n    \n    # Safely extract labels (always index 1, regardless of tuple length)\n    labels = [sample[1] for sample in dataset.samples]  \n    \n    counts = Counter(labels)\n    total = len(labels)\n    \n    # Ensure there are at least two classes to calculate class_weights\n    if len(counts) <= 1:\n        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n        weights = [1.0] * total\n    else:\n        # Calculate inverse frequency weights\n        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n        weights = [class_weights[lbl] for lbl in labels]\n        \n    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n\ninfo_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \n# COD10K Info files\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\n# CAMO-COCO PATHS\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n\n# CAMO-COCO Info files\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\n# CAMO-COCO Root Directories\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\ntesting_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n\n# Split testing-dataset: 20% train (train_paths), 80% validation (val_paths)\n# testing dataset: NC4K+places365 (images and labels text file)\nfrom sklearn.model_selection import train_test_split # Make sure this import is present\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    testing_image_paths, testing_labels, test_size=0.8, random_state=42\n)\n\n# 1. All Root Directories\nALL_ROOT_DIRS = [\n    train_dir_cod,       \n    test_dir_cod,       \n    train_dir_camo_cam,  \n    train_dir_camo_noncam\n]\n\n# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\nALL_TRAIN_TXTS = [\n    train_cam_txt, train_noncam_txt, test_cam_txt, test_noncam_txt,\n    train_cam_txt2, train_noncam_txt2, test_cam_txt2, test_noncam_txt2,\n]\n\n# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\nALL_VAL_TXTS = []\n\n# Training Dataset: All external data (via ALL_TRAIN_TXTS) + 20% testing-dataset split (via train_paths)\ntrain_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS, \n    txt_files=ALL_TRAIN_TXTS,               \n    testing_image_paths=train_paths,        \n    testing_labels=train_labels,            \n    weak_transform=weak_tf, \n    strong_transform=strong_tf, \n    use_masks=USE_SEGMENTATION\n)\n\n# Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\nval_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS,  \n    txt_files=ALL_VAL_TXTS,                 \n    testing_image_paths=val_paths,          \n    testing_labels=val_labels,              \n    weak_transform=val_tf, \n    strong_transform=None, \n    use_masks=USE_SEGMENTATION\n)\n\n# Build Sampler and DataLoader\ntrain_sampler = build_weighted_sampler(train_ds)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nprint(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))\n\nclass DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats\n\nclass SwinExtractor(nn.Module):\n    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n    def forward(self, x):\n        return self.model(x)\n\nclass CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction,4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)\n\nclass GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        if H.shape[2:] != X.shape[2:]:\n            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n        g = self.g_fc(H)\n        return g * H + (1 - g) * X\n\nclass CrossAttention(nn.Module):\n    def __init__(self, d_cnn, d_swin, d_out):\n        super().__init__()\n        self.q = nn.Linear(d_cnn, d_out)\n        self.k = nn.Linear(d_swin, d_out)\n        self.v = nn.Linear(d_swin, d_out)\n        self.scale = d_out ** -0.5\n    def forward(self, feat_cnn, feat_swin):\n        B, Cc, H, W = feat_cnn.shape\n        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n        if feat_swin.dim() == 4:\n            Bs, Cs, Hs, Ws = feat_swin.shape\n            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n        else:\n            kv = feat_swin\n        K = self.k(kv)\n        V = self.v(kv)\n        Q = self.q(q)\n        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, V)\n        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n        return out\n\nclass SegDecoder(nn.Module):\n    def __init__(self, in_channels_list, mid_channels=128):\n        super().__init__()\n        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n        self.out = nn.Conv2d(mid_channels, 1, 1)\n    def forward(self, feat_list):\n        target_size = feat_list[0].shape[2:]\n        ups = []\n        for f, p in zip(feat_list, self.projs):\n            x = p(f)\n            if x.shape[2:] != target_size:\n                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n            ups.append(x)\n        x = torch.cat(ups, dim=1)\n        x = self.conv(x)\n        x = self.out(x)\n        return x\n\ndnet = DenseNetExtractor().to(device).eval()\nmnet = MobileNetExtractor().to(device).eval()\nsnet = SwinExtractor().to(device).eval()\nwith torch.no_grad():\n    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n    featsA = dnet(dummy)\n    featsB = mnet(dummy)\n    featsS = snet(dummy)\nchA = [f.shape[1] for f in featsA]\nchB = [f.shape[1] for f in featsB]\nchS = [f.shape[1] for f in featsS]\nprint(\"DenseNet channels:\", chA)\nprint(\"MobileNet channels:\", chB)\nprint(\"Swin channels:\", chS)\n\nclass FusionWithSwin(nn.Module):\n    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n        super().__init__()\n        self.backA = DenseNetExtractor()\n        self.backB = MobileNetExtractor()\n        self.backS = SwinExtractor()\n        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n        self.L = L\n        self.d = d\n        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n        self.reduce = nn.Conv2d(d * L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n        self.use_seg = use_seg\n        if self.use_seg:\n            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n\n        # Domain head for DANN (simple MLP)\n        self.domain_head = nn.Sequential(\n            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, x, grl_lambda=0.0):\n        fa = self.backA(x)\n        fb = self.backB(x)\n        fs = self.backS(x)\n        fused_feats = []\n        aligned_for_dec = []\n        for i in range(self.L):\n            a = self.alignA[i](fa[i])\n            a = self.cbamA[i](a)\n            b = self.alignB[i](fb[i])\n            b = self.cbamB[i](b)\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused = self.gates[i](a, b)\n            swin_feat = fs[i]\n            swin_att = self.cross_atts[i](fused, swin_feat)\n            if swin_att.shape[2:] != fused.shape[2:]:\n                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n            fused = fused + swin_att\n            fused_feats.append(fused)\n            aligned_for_dec.append(fused)\n        target = fused_feats[-1]\n        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.reduce(concat)\n        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n        logits = self.classifier(z)\n        out = {\"logits\": logits, \"feat\": z}\n        if self.use_seg:\n            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n\n        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n        if grl_lambda > 0.0:\n            # GRL implemented outside (we'll pass z through GRL function)\n            pass\n        out[\"domain_logits\"] = self.domain_head(z)\n        return out\n\n# instantiate model\nmodel = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\nprint(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)\n\nclass LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss_logits(pred_logits, target):\n    pred = torch.sigmoid(pred_logits)\n    target = target.float()\n    inter = (pred * target).sum(dim=(1,2,3))\n    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n    return 1.0 - dice.mean()\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nseg_bce = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\n#Supervised contrastive Loss\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        # features: [N, D], labels: [N]\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        # remove diagonal\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        # for each i, positive samples are where mask==1 (excluding self)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        # avoid divide by zero\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        # average only across anchors that have positives\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\nsupcon_loss_fn = SupConLoss(temperature=0.07)\n\n# Domain Adversarial: Gradient Reversal Layer (GRL)\n\nfrom torch.autograd import Function\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)\n\n# Optimizer + scheduler + mixed precision + clipping\n# -----------------------------\n# param groups: smaller LR for backbones, larger for heads\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2},\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# warmup + cosine schedule\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        # cosine from warmup -> total\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# -----------------------------\n# Mixup & CutMix helpers\n# -----------------------------\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)   # use builtin int\n    cut_h = int(H * cut_rat)   # use builtin int\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n\nbest_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    # mix_info: (mode, y_a, y_b, lam) or None\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        # mixup/cutmix: soft labels\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            # focal is not designed for soft labels; approximate by weighted CE\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\n\nfor epoch in range(1, EPOCHS+1):\n    # --- CHANGED: Improved freeze/unfreeze strategy ---\n    # We set FREEZE_EPOCHS = 5 in cell 5 (to match WARMUP_EPOCHS)\n    if epoch <= FREEZE_EPOCHS:\n        # Freeze ALL backbone parameters\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['backA', 'backB', 'backS']):\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        # Unfreeze all parameters *once* after freeze period\n        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n    # --- End of change ---\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    # 1. Initialize zero_grad at the start of the epoch\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n        imgs = weak_imgs\n\n        # optionally apply mixup/cutmix on imgs (on weak view)\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n            out = model(imgs)  # returns logits, feat, seg, domain_logits\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            domain_logits = out.get(\"domain_logits\", None)\n\n            # classification loss (label-smoothing or focal)\n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # segmentation loss if available & mask present\n            seg_loss = 0.0\n            if USE_SEGMENTATION and (masks is not None):\n                seg_pred = out[\"seg\"]\n                seg_loss = seg_loss_fn(seg_pred, masks)\n            # supcon loss on features (use features from weak)\n            supcon_loss = supcon_loss_fn(feat, labels)\n\n            # consistency: forward strong view and compare predictions\n            out_strong = model(strong_imgs)\n            logits_strong = out_strong[\"logits\"]\n            probs_weak = F.softmax(logits.detach(), dim=1)\n            probs_strong = F.softmax(logits_strong, dim=1)\n            # L2 between probability vectors (could be KL)\n            cons_loss = F.mse_loss(probs_weak, probs_strong)\n            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n            dom_loss = 0.0\n\n            # --- CHANGED: Use GAMMA_SEG to weight segmentation loss ---\n            total_loss = clf_loss + GAMMA_SEG * seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n\n            # 2. Scale the loss by accumulation steps to average the gradients\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        # Perform backward pass (gradients are accumulated until step is called)\n        scaler.scale(total_loss).backward()\n\n        # 3. Optimizer step only every ACCUMULATION_STEPS batches\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            # gradient clipping before step\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() # Prepare for next accumulation cycle\n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS # Re-scale back for correct loss tracking\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # 4. Take a final step if there are remaining gradients (i.e., last batch was not a multiple of ACCUMULATION_STEPS)\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    # metrics (rest of the code remains the same)\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                # --- CHANGED: Use GAMMA_SEG in validation loss calculation as well ---\n                loss += GAMMA_SEG * seg_loss_fn(seg_out, masks)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    # early stopping & save best\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)\n\n# Test-time augmentation (TTA) helper\n# -----------------------------\ndef tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n    model.eval()\n    logits_accum = None\n    with torch.no_grad():\n        for s in scales:\n            tf = transforms.Compose([\n                transforms.Resize((s, s)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n            x = tf(img_pil).unsqueeze(0).to(device)\n            out = model(x)\n            logits = out[\"logits\"]\n            if flip:\n                x_f = torch.flip(x, dims=[3])\n                logits_f = model(x_f)[\"logits\"]\n                logits = (logits + logits_f) / 2.0\n            if logits_accum is None:\n                logits_accum = logits\n            else:\n                logits_accum += logits\n    logits_accum /= len(scales)\n    return logits_accum\n\n# Grad-CAM helper (very simple)\n# -----------------------------\ndef get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n    \"\"\"\n    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n    Returns upsampled heatmap (H,W) normalized in [0,1].\n    \"\"\"\n    model.eval()\n    # find layer\n    target_module = None\n    for name, module in model.named_modules():\n        if name == layer_name:\n            target_module = module\n            break\n    if target_module is None:\n        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, input, output):\n        activations.append(output.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    h1 = target_module.register_forward_hook(forward_hook)\n    h2 = target_module.register_full_backward_hook(backward_hook)\n\n    out = model(input_tensor)\n    logits = out[\"logits\"]\n    if target_class is None:\n        target_class = logits.argmax(1).item()\n    loss = logits[:, target_class].sum()\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    act = activations[0]  # [B,C,H,W]\n    grad = gradients[0]   # [B,C,H,W]\n    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n    cam = F.relu(cam)\n    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n    cam = cam.squeeze().cpu().numpy()\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h1.remove(); h2.remove()\n    return cam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T06:24:04.584342Z","iopub.execute_input":"2025-11-04T06:24:04.584995Z","iopub.status.idle":"2025-11-04T17:09:49.946988Z","shell.execute_reply.started":"2025-11-04T06:24:04.584972Z","shell.execute_reply":"2025-11-04T17:09:49.946032Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n✅ Loaded 14150 samples from 4 root directories.\n✅ Loaded 6606 samples from 4 root directories.\nTotal Train samples: 14150 Total Val samples: 6606\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n100%|██████████| 77.4M/77.4M [00:00<00:00, 210MB/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 184MB/s]\n","output_type":"stream"},{"name":"stdout","text":"DenseNet channels: [256, 512, 1792, 1920]\nMobileNet channels: [24, 40, 80, 112]\nSwin channels: [56, 28, 14, 7]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3890398912.py:703: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","output_type":"stream"},{"name":"stdout","text":"Model parameters (M): 51.586615\n","output_type":"stream"},{"name":"stderr","text":"Train 1/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 1/30: 100%|██████████| 1769/1769 [12:51<00:00,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 3.9568 Acc: 0.5082 Prec: 0.5076 Rec: 0.5072 F1: 0.5010\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Val Loss: 1.4619 Acc: 0.4950 Prec: 0.4075 Rec: 0.4934 F1: 0.3409\nSaved best model at epoch 1 (F1 0.3409)\n","output_type":"stream"},{"name":"stderr","text":"Train 2/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 2/30: 100%|██████████| 1769/1769 [12:48<00:00,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Train Loss: 2.9875 Acc: 0.6396 Prec: 0.6396 Rec: 0.6395 F1: 0.6395\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Val Loss: 1.0289 Acc: 0.8819 Prec: 0.9031 Rec: 0.8823 F1: 0.8804\nSaved best model at epoch 2 (F1 0.8804)\n","output_type":"stream"},{"name":"stderr","text":"Train 3/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 3/30: 100%|██████████| 1769/1769 [12:49<00:00,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Train Loss: 2.8154 Acc: 0.6946 Prec: 0.6949 Rec: 0.6944 F1: 0.6944\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Val Loss: 1.0395 Acc: 0.8662 Prec: 0.8885 Rec: 0.8666 F1: 0.8643\n","output_type":"stream"},{"name":"stderr","text":"Train 4/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 4/30: 100%|██████████| 1769/1769 [12:50<00:00,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Train Loss: 2.8325 Acc: 0.6880 Prec: 0.6882 Rec: 0.6881 F1: 0.6880\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Val Loss: 0.9862 Acc: 0.9372 Prec: 0.9424 Rec: 0.9374 F1: 0.9370\nSaved best model at epoch 4 (F1 0.9370)\n","output_type":"stream"},{"name":"stderr","text":"Train 5/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 5/30: 100%|██████████| 1769/1769 [12:50<00:00,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Train Loss: 2.7910 Acc: 0.7001 Prec: 0.7001 Rec: 0.7000 F1: 0.7000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Val Loss: 0.9959 Acc: 0.9202 Prec: 0.9296 Rec: 0.9205 F1: 0.9198\n--- Unfreezing all backbone layers at epoch 6 ---\n","output_type":"stream"},{"name":"stderr","text":"Train 6/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 6/30: 100%|██████████| 1769/1769 [21:14<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Train Loss: 2.5126 Acc: 0.7468 Prec: 0.7469 Rec: 0.7466 F1: 0.7467\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Val Loss: 0.9803 Acc: 0.9074 Prec: 0.9198 Rec: 0.9076 F1: 0.9067\n","output_type":"stream"},{"name":"stderr","text":"Train 7/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 8/30: 100%|██████████| 1769/1769 [21:13<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Train Loss: 2.1744 Acc: 0.8066 Prec: 0.8066 Rec: 0.8066 F1: 0.8066\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Val Loss: 0.9490 Acc: 0.9587 Prec: 0.9614 Rec: 0.9588 F1: 0.9586\nSaved best model at epoch 8 (F1 0.9586)\n","output_type":"stream"},{"name":"stderr","text":"Train 9/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 9/30: 100%|██████████| 1769/1769 [21:12<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Train Loss: 2.0845 Acc: 0.8188 Prec: 0.8188 Rec: 0.8188 F1: 0.8188\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Val Loss: 0.9238 Acc: 0.9705 Prec: 0.9712 Rec: 0.9705 F1: 0.9705\nSaved best model at epoch 9 (F1 0.9705)\n","output_type":"stream"},{"name":"stderr","text":"Train 10/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 10/30: 100%|██████████| 1769/1769 [21:12<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Train Loss: 2.0616 Acc: 0.8225 Prec: 0.8225 Rec: 0.8225 F1: 0.8225\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Val Loss: 0.9664 Acc: 0.9523 Prec: 0.9559 Rec: 0.9525 F1: 0.9522\n","output_type":"stream"},{"name":"stderr","text":"Train 11/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 11/30: 100%|██████████| 1769/1769 [21:13<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 11] Train Loss: 1.9738 Acc: 0.8382 Prec: 0.8383 Rec: 0.8380 F1: 0.8381\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 11] Val Loss: 0.9582 Acc: 0.9342 Prec: 0.9404 Rec: 0.9344 F1: 0.9339\n","output_type":"stream"},{"name":"stderr","text":"Train 12/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 12/30: 100%|██████████| 1769/1769 [21:14<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 12] Train Loss: 1.9468 Acc: 0.8383 Prec: 0.8386 Rec: 0.8382 F1: 0.8382\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 12] Val Loss: 0.9643 Acc: 0.9414 Prec: 0.9471 Rec: 0.9416 F1: 0.9412\n","output_type":"stream"},{"name":"stderr","text":"Train 13/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 13/30: 100%|██████████| 1769/1769 [21:11<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 13] Train Loss: 1.9487 Acc: 0.8361 Prec: 0.8362 Rec: 0.8360 F1: 0.8360\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 13] Val Loss: 0.9522 Acc: 0.9513 Prec: 0.9550 Rec: 0.9514 F1: 0.9512\n","output_type":"stream"},{"name":"stderr","text":"Train 14/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 14/30: 100%|██████████| 1769/1769 [21:10<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 14] Train Loss: 1.8722 Acc: 0.8422 Prec: 0.8422 Rec: 0.8422 F1: 0.8422\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 14] Val Loss: 0.9098 Acc: 0.9612 Prec: 0.9630 Rec: 0.9614 F1: 0.9612\n","output_type":"stream"},{"name":"stderr","text":"Train 15/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 15/30: 100%|██████████| 1769/1769 [21:19<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 15] Train Loss: 1.8418 Acc: 0.8452 Prec: 0.8453 Rec: 0.8451 F1: 0.8452\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 15] Val Loss: 0.9586 Acc: 0.9584 Prec: 0.9608 Rec: 0.9585 F1: 0.9583\n","output_type":"stream"},{"name":"stderr","text":"Train 16/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 16/30: 100%|██████████| 1769/1769 [21:22<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 16] Train Loss: 1.7852 Acc: 0.8570 Prec: 0.8571 Rec: 0.8570 F1: 0.8570\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 16] Val Loss: 0.9684 Acc: 0.9534 Prec: 0.9560 Rec: 0.9535 F1: 0.9533\n","output_type":"stream"},{"name":"stderr","text":"Train 17/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 17/30: 100%|██████████| 1769/1769 [21:17<00:00,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 17] Train Loss: 1.7315 Acc: 0.8589 Prec: 0.8589 Rec: 0.8589 F1: 0.8589\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 17] Val Loss: 0.9310 Acc: 0.9771 Prec: 0.9778 Rec: 0.9772 F1: 0.9771\nSaved best model at epoch 17 (F1 0.9771)\n","output_type":"stream"},{"name":"stderr","text":"Train 18/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 18/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 18] Train Loss: 1.7217 Acc: 0.8620 Prec: 0.8620 Rec: 0.8620 F1: 0.8620\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 18] Val Loss: 0.9210 Acc: 0.9731 Prec: 0.9740 Rec: 0.9731 F1: 0.9730\n","output_type":"stream"},{"name":"stderr","text":"Train 19/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 19/30: 100%|██████████| 1769/1769 [21:20<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 19] Train Loss: 1.7117 Acc: 0.8628 Prec: 0.8627 Rec: 0.8628 F1: 0.8627\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 19] Val Loss: 0.9193 Acc: 0.9829 Prec: 0.9832 Rec: 0.9829 F1: 0.9829\nSaved best model at epoch 19 (F1 0.9829)\n","output_type":"stream"},{"name":"stderr","text":"Train 20/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 20/30: 100%|██████████| 1769/1769 [21:23<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 20] Train Loss: 1.6407 Acc: 0.8675 Prec: 0.8675 Rec: 0.8674 F1: 0.8674\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 20] Val Loss: 0.9353 Acc: 0.9681 Prec: 0.9697 Rec: 0.9682 F1: 0.9680\n","output_type":"stream"},{"name":"stderr","text":"Train 21/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 21/30: 100%|██████████| 1769/1769 [21:22<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 21] Train Loss: 1.6266 Acc: 0.8678 Prec: 0.8678 Rec: 0.8678 F1: 0.8678\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 21] Val Loss: 0.9344 Acc: 0.9619 Prec: 0.9639 Rec: 0.9620 F1: 0.9618\n","output_type":"stream"},{"name":"stderr","text":"Train 22/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 22/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 22] Train Loss: 1.6040 Acc: 0.8728 Prec: 0.8728 Rec: 0.8726 F1: 0.8727\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 22] Val Loss: 0.9024 Acc: 0.9773 Prec: 0.9781 Rec: 0.9774 F1: 0.9773\n","output_type":"stream"},{"name":"stderr","text":"Train 23/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 23/30: 100%|██████████| 1769/1769 [21:23<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 23] Train Loss: 1.5959 Acc: 0.8664 Prec: 0.8664 Rec: 0.8664 F1: 0.8664\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 23] Val Loss: 0.9369 Acc: 0.9631 Prec: 0.9654 Rec: 0.9632 F1: 0.9630\n","output_type":"stream"},{"name":"stderr","text":"Train 24/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 24/30: 100%|██████████| 1769/1769 [21:22<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 24] Train Loss: 1.5912 Acc: 0.8688 Prec: 0.8688 Rec: 0.8688 F1: 0.8688\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 24] Val Loss: 0.9216 Acc: 0.9761 Prec: 0.9770 Rec: 0.9762 F1: 0.9761\n","output_type":"stream"},{"name":"stderr","text":"Train 25/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 25/30: 100%|██████████| 1769/1769 [21:22<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 25] Train Loss: 1.5579 Acc: 0.8727 Prec: 0.8727 Rec: 0.8726 F1: 0.8726\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 25] Val Loss: 0.9332 Acc: 0.9721 Prec: 0.9733 Rec: 0.9722 F1: 0.9721\n","output_type":"stream"},{"name":"stderr","text":"Train 26/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 26/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 26] Train Loss: 1.5140 Acc: 0.8808 Prec: 0.8808 Rec: 0.8808 F1: 0.8808\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 26] Val Loss: 0.9332 Acc: 0.9688 Prec: 0.9704 Rec: 0.9689 F1: 0.9688\n","output_type":"stream"},{"name":"stderr","text":"Train 27/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 27/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 27] Train Loss: 1.4931 Acc: 0.8787 Prec: 0.8787 Rec: 0.8787 F1: 0.8787\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 27] Val Loss: 0.9168 Acc: 0.9744 Prec: 0.9754 Rec: 0.9745 F1: 0.9744\n","output_type":"stream"},{"name":"stderr","text":"Train 28/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 28/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 28] Train Loss: 1.5101 Acc: 0.8797 Prec: 0.8797 Rec: 0.8797 F1: 0.8797\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 28] Val Loss: 0.9278 Acc: 0.9747 Prec: 0.9757 Rec: 0.9748 F1: 0.9747\n","output_type":"stream"},{"name":"stderr","text":"Train 29/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 29/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 29] Train Loss: 1.4786 Acc: 0.8789 Prec: 0.8789 Rec: 0.8789 F1: 0.8789\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 29] Val Loss: 0.9258 Acc: 0.9734 Prec: 0.9745 Rec: 0.9734 F1: 0.9733\n","output_type":"stream"},{"name":"stderr","text":"Train 30/30:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/3890398912.py:805: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 30/30: 100%|██████████| 1769/1769 [21:18<00:00,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 30] Train Loss: 1.4695 Acc: 0.8786 Prec: 0.8786 Rec: 0.8786 F1: 0.8786\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 30] Val Loss: 0.9229 Acc: 0.9747 Prec: 0.9757 Rec: 0.9748 F1: 0.9747\nTraining finished. Best val F1: 0.9828928798021519 at epoch 19\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm","metadata":{"_uuid":"d0b94887-4b0f-4317-84c8-1a7777d760d0","_cell_guid":"53c610f7-732b-435e-9459-de2f9e3d80c7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:00.849612Z","iopub.execute_input":"2025-11-01T10:14:00.849973Z","iopub.status.idle":"2025-11-01T10:14:10.94302Z","shell.execute_reply.started":"2025-11-01T10:14:00.849949Z","shell.execute_reply":"2025-11-01T10:14:10.942239Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom collections import Counter","metadata":{"_uuid":"23ff4136-a399-4879-b737-c64e8a7b9c6d","_cell_guid":"63fe8b36-a02c-4487-a769-3e87f2989073","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:10.943869Z","iopub.execute_input":"2025-11-01T10:14:10.944399Z","iopub.status.idle":"2025-11-01T10:14:10.947933Z","shell.execute_reply.started":"2025-11-01T10:14:10.944369Z","shell.execute_reply":"2025-11-01T10:14:10.947096Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)","metadata":{"_uuid":"2072ad41-1769-40e0-9dc2-ff2324551b11","_cell_guid":"8c96bf9f-8a4a-4b5b-ba3c-cc2904b1ab58","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:10.949645Z","iopub.execute_input":"2025-11-01T10:14:10.949856Z","iopub.status.idle":"2025-11-01T10:14:11.020476Z","shell.execute_reply.started":"2025-11-01T10:14:10.94984Z","shell.execute_reply":"2025-11-01T10:14:11.019866Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 8         # adjust if OOM\nEPOCHS = 30\nNUM_WORKERS = 4         # set 0 if worker issues on Kaggle\nLR = 3e-4\nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True\n\n# Loss weights from PDF suggestion\nALPHA_DOM = 0.5\nBETA_SUPCON = 0.3\nETA_CONS = 0.1\nGAMMA_SEG = 0.5\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# warmup epochs\nWARMUP_EPOCHS = 5\n\n# early stopping\nEARLY_STOPPING_PATIENCE = 8\nFREEZE_EPOCHS = 5\nACCUMULATION_STEPS = 4","metadata":{"_uuid":"b9c000da-bf1a-4523-bbb1-7b28ff015f10","_cell_guid":"1ca1b022-43ef-4a4f-9bc8-9502667f799e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.021303Z","iopub.execute_input":"2025-11-01T10:14:11.021554Z","iopub.status.idle":"2025-11-01T10:14:11.026259Z","shell.execute_reply.started":"2025-11-01T10:14:11.021537Z","shell.execute_reply":"2025-11-01T10:14:11.025615Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Noise + Transform","metadata":{"_uuid":"32b3ea07-c3c9-4592-81c8-7e7c5f53b060","_cell_guid":"6b93dda5-4bd4-4cc5-897f-2e3f7ce8d878","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])","metadata":{"_uuid":"8610d19e-aa2c-4c65-8dd3-63efff268002","_cell_guid":"a36a568a-587b-4e7d-95ea-3a45d5fa1652","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.026973Z","iopub.execute_input":"2025-11-01T10:14:11.027197Z","iopub.status.idle":"2025-11-01T10:14:11.039758Z","shell.execute_reply.started":"2025-11-01T10:14:11.027173Z","shell.execute_reply":"2025-11-01T10:14:11.039078Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"weak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\nstrong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\nval_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\n# reading a file with multiple encodings\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            print(f\"Failed to read {file_path} with encoding {encoding}\")\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n    image_paths = []\n    labels = []\n    \n    # List of encodings to try\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break  # If successful, break out of the loop\n        except UnicodeDecodeError:\n            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n        except Exception as e:\n            print(f\"Error reading {info_file}: {e}\")\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            # Ensure label is always 0 or 1\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue # Skip malformed lines\n                \n            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n\n# MultiDataset class with proper encoding handling\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        # Handle the testing dataset (testing-dataset images and labels)\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n                self.samples.append((img_path, label)) \n        \n        # Process other datasets (COD10K, CAMO, etc.)\n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}\")\n\n            # Use the read_file_with_encoding function to handle different encodings\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    # Fallback classification if label is not an integer\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            # Map labels to binary (CAM=1, Non-CAM=0)\n            lbl = 1 if lbl == 1 else 0\n\n            found = False\n            search_subs = [\n                \"\",  # If image is directly in root_dir (less common)\n                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n            ]\n            \n            base_fname = os.path.basename(fname)  \n\n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n            if not found:\n                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        \n        img_path = sample[0]\n        lbl = sample[1]\n        \n        # Define rdir for consistent mask lookup logic\n        if len(sample) == 3:\n            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n            rdir = sample[2]\n            # Root directory for testing-dataset (used as fallback for mask lookup)\n            testing_root = None \n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            # Set testing_root explicitly for clarity if needed later, but rdir is now defined.\n\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            \n            # Use the defined rdir for mask search\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    # Convert to binary mask: 0 or 1\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\ndef build_weighted_sampler(dataset):\n    \"\"\"\n    Builds a WeightedRandomSampler based on class imbalance.\n    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n    \"\"\"\n    \n    # Safely extract labels (always index 1, regardless of tuple length)\n    labels = [sample[1] for sample in dataset.samples]  \n    \n    counts = Counter(labels)\n    total = len(labels)\n    \n    # Ensure there are at least two classes to calculate class_weights\n    if len(counts) <= 1:\n        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n        weights = [1.0] * total\n    else:\n        # Calculate inverse frequency weights\n        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n        weights = [class_weights[lbl] for lbl in labels]\n        \n    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)","metadata":{"_uuid":"8a0980ba-9609-4279-9de2-cd19d8a166b9","_cell_guid":"0439529c-c10a-4627-b237-6d1ea7c53173","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.040538Z","iopub.execute_input":"2025-11-01T10:14:11.040739Z","iopub.status.idle":"2025-11-01T10:14:11.061111Z","shell.execute_reply.started":"2025-11-01T10:14:11.040716Z","shell.execute_reply":"2025-11-01T10:14:11.060504Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"0e0569d2-4f3f-47be-ba5a-80aac8370acb","_cell_guid":"b86311ae-c1bb-4f22-91c0-d3d43ff59d08","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \n# COD10K Info files\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\n# CAMO-COCO PATHS\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n\n# CAMO-COCO Info files\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\n# CAMO-COCO Root Directories\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\n# third dataset - NC4K + non-camouflage (places 365)\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\ntesting_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n\n# Split testing-dataset: 20% train (train_paths), 80% validation (val_paths)\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    testing_image_paths, testing_labels, test_size=0.8, random_state=42\n)\n\n# 1. All Root Directories\nALL_ROOT_DIRS = [\n    train_dir_cod,       \n    test_dir_cod,       \n    train_dir_camo_cam,  \n    train_dir_camo_noncam\n]\n\n# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\nALL_TRAIN_TXTS = [\n    train_cam_txt, train_noncam_txt, test_cam_txt, test_noncam_txt,\n    train_cam_txt2, train_noncam_txt2, test_cam_txt2, test_noncam_txt2,\n]\n\n# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\nALL_VAL_TXTS = []\n\n# --- Create the final unified datasets ---\n\n# Training Dataset: All external data (via ALL_TRAIN_TXTS) + 20% testing-dataset split (via train_paths)\ntrain_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS, \n    txt_files=ALL_TRAIN_TXTS,               \n    testing_image_paths=train_paths,        \n    testing_labels=train_labels,            \n    weak_transform=weak_tf, \n    strong_transform=strong_tf, \n    use_masks=USE_SEGMENTATION\n)\n\n# Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\nval_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS,  \n    txt_files=ALL_VAL_TXTS,                 \n    testing_image_paths=val_paths,          \n    testing_labels=val_labels,              \n    weak_transform=val_tf, \n    strong_transform=None, \n    use_masks=USE_SEGMENTATION\n)\n\n# Build Sampler and DataLoader\ntrain_sampler = build_weighted_sampler(train_ds)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nprint(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))","metadata":{"_uuid":"2977418b-5a9b-4b00-bd2b-6e6d1380c03a","_cell_guid":"8612016c-9564-42f6-be27-48e77e56c6eb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:14:11.061891Z","iopub.execute_input":"2025-11-01T10:14:11.062075Z","iopub.status.idle":"2025-11-01T10:15:18.966571Z","shell.execute_reply.started":"2025-11-01T10:14:11.062059Z","shell.execute_reply":"2025-11-01T10:15:18.965997Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"✅ Loaded 14150 samples from 4 root directories.\n✅ Loaded 6606 samples from 4 root directories.\nTotal Train samples: 14150 Total Val samples: 6606\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Backbones","metadata":{"_uuid":"9147384c-1b50-4cf1-93a9-da0d89a0615f","_cell_guid":"27410294-010a-4911-a7fe-8047a41f96b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats","metadata":{"_uuid":"0562f881-a9a2-425b-84f7-b87f9f1de827","_cell_guid":"7c590a73-9329-416b-b0e9-3cc63b6c61f7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:18.967314Z","iopub.execute_input":"2025-11-01T10:15:18.967535Z","iopub.status.idle":"2025-11-01T10:15:18.973608Z","shell.execute_reply.started":"2025-11-01T10:15:18.967518Z","shell.execute_reply":"2025-11-01T10:15:18.973058Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SwinExtractor(nn.Module):\n    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n    def forward(self, x):\n        return self.model(x)","metadata":{"_uuid":"4b565d5e-4ecb-457c-af1a-b89084719789","_cell_guid":"602ab64f-0bcd-48fd-9c92-789c331c1ced","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:18.975859Z","iopub.execute_input":"2025-11-01T10:15:18.976061Z","iopub.status.idle":"2025-11-01T10:15:18.98853Z","shell.execute_reply.started":"2025-11-01T10:15:18.976045Z","shell.execute_reply":"2025-11-01T10:15:18.987998Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction,4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)","metadata":{"_uuid":"77d2613c-fd8c-4bc9-a03a-3fbfa9258379","_cell_guid":"595c7ef4-f65e-4c97-818d-db3dea350d48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:18.989136Z","iopub.execute_input":"2025-11-01T10:15:18.989338Z","iopub.status.idle":"2025-11-01T10:15:19.000335Z","shell.execute_reply.started":"2025-11-01T10:15:18.989324Z","shell.execute_reply":"2025-11-01T10:15:18.999706Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        if H.shape[2:] != X.shape[2:]:\n            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n        g = self.g_fc(H)\n        return g * H + (1 - g) * X","metadata":{"_uuid":"a7ab254f-2a14-444a-bd57-e45aa8620035","_cell_guid":"3691f4eb-fe5e-4a90-8355-281bfb975eee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.001047Z","iopub.execute_input":"2025-11-01T10:15:19.00129Z","iopub.status.idle":"2025-11-01T10:15:19.013597Z","shell.execute_reply.started":"2025-11-01T10:15:19.001265Z","shell.execute_reply":"2025-11-01T10:15:19.013006Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, d_cnn, d_swin, d_out):\n        super().__init__()\n        self.q = nn.Linear(d_cnn, d_out)\n        self.k = nn.Linear(d_swin, d_out)\n        self.v = nn.Linear(d_swin, d_out)\n        self.scale = d_out ** -0.5\n    def forward(self, feat_cnn, feat_swin):\n        B, Cc, H, W = feat_cnn.shape\n        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n        if feat_swin.dim() == 4:\n            Bs, Cs, Hs, Ws = feat_swin.shape\n            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n        else:\n            kv = feat_swin\n        K = self.k(kv)\n        V = self.v(kv)\n        Q = self.q(q)\n        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, V)\n        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n        return out","metadata":{"_uuid":"95e9bd01-a25d-40bd-95d3-0d52401e3fd5","_cell_guid":"8e151f36-8629-46d4-8978-9dc6f44a874d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.01421Z","iopub.execute_input":"2025-11-01T10:15:19.014652Z","iopub.status.idle":"2025-11-01T10:15:19.026114Z","shell.execute_reply.started":"2025-11-01T10:15:19.014635Z","shell.execute_reply":"2025-11-01T10:15:19.025414Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Segmentation Decoder","metadata":{"_uuid":"fdfd48ad-a120-426f-812d-eab414adf245","_cell_guid":"8d1a2d8c-d55a-4a1c-8242-9619a3169e93","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class SegDecoder(nn.Module):\n    def __init__(self, in_channels_list, mid_channels=128):\n        super().__init__()\n        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n        self.out = nn.Conv2d(mid_channels, 1, 1)\n    def forward(self, feat_list):\n        target_size = feat_list[0].shape[2:]\n        ups = []\n        for f, p in zip(feat_list, self.projs):\n            x = p(f)\n            if x.shape[2:] != target_size:\n                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n            ups.append(x)\n        x = torch.cat(ups, dim=1)\n        x = self.conv(x)\n        x = self.out(x)\n        return x","metadata":{"_uuid":"59784957-96f9-43f9-ab4c-ba4c5069c266","_cell_guid":"5ea62e96-bc4e-4218-a0ba-7009414c6204","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.026836Z","iopub.execute_input":"2025-11-01T10:15:19.027135Z","iopub.status.idle":"2025-11-01T10:15:19.04381Z","shell.execute_reply.started":"2025-11-01T10:15:19.027107Z","shell.execute_reply":"2025-11-01T10:15:19.043241Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Probing Backbones","metadata":{"_uuid":"f9354086-efdb-49de-9956-72bdc5c1c804","_cell_guid":"226cd599-783c-4f41-8d35-ebcf350381b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"dnet = DenseNetExtractor().to(device).eval()\nmnet = MobileNetExtractor().to(device).eval()\nsnet = SwinExtractor().to(device).eval()\nwith torch.no_grad():\n    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n    featsA = dnet(dummy)\n    featsB = mnet(dummy)\n    featsS = snet(dummy)\nchA = [f.shape[1] for f in featsA]\nchB = [f.shape[1] for f in featsB]\nchS = [f.shape[1] for f in featsS]\nprint(\"DenseNet channels:\", chA)\nprint(\"MobileNet channels:\", chB)\nprint(\"Swin channels:\", chS)","metadata":{"_uuid":"c92e84d5-b62f-4c5c-baff-5897fd5bc9fe","_cell_guid":"2a3f1c47-b68b-4563-becc-b0d6171967fa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:19.044574Z","iopub.execute_input":"2025-11-01T10:15:19.044837Z","iopub.status.idle":"2025-11-01T10:15:23.668509Z","shell.execute_reply.started":"2025-11-01T10:15:19.044816Z","shell.execute_reply":"2025-11-01T10:15:23.667892Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n100%|██████████| 77.4M/77.4M [00:00<00:00, 222MB/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 174MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9607b0f6354a679919dcde914cdc59"}},"metadata":{}},{"name":"stdout","text":"DenseNet channels: [256, 512, 1792, 1920]\nMobileNet channels: [24, 40, 80, 112]\nSwin channels: [56, 28, 14, 7]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Fusion Model (DenseNet + MobileNet + Swin cross attention)","metadata":{"_uuid":"42955826-da73-49eb-a2b6-1eb868ac3be0","_cell_guid":"93c9df34-5655-4f06-8252-ebbc742d4db7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class FusionWithSwin(nn.Module):\n    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n        super().__init__()\n        self.backA = DenseNetExtractor()\n        self.backB = MobileNetExtractor()\n        self.backS = SwinExtractor()\n        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n        self.L = L\n        self.d = d\n        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n        self.reduce = nn.Conv2d(d * L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n        self.use_seg = use_seg\n        if self.use_seg:\n            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n\n        # Domain head for DANN (simple MLP)\n        self.domain_head = nn.Sequential(\n            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, x, grl_lambda=0.0):\n        fa = self.backA(x)\n        fb = self.backB(x)\n        fs = self.backS(x)\n        fused_feats = []\n        aligned_for_dec = []\n        for i in range(self.L):\n            a = self.alignA[i](fa[i])\n            a = self.cbamA[i](a)\n            b = self.alignB[i](fb[i])\n            b = self.cbamB[i](b)\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused = self.gates[i](a, b)\n            swin_feat = fs[i]\n            swin_att = self.cross_atts[i](fused, swin_feat)\n            if swin_att.shape[2:] != fused.shape[2:]:\n                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n            fused = fused + swin_att\n            fused_feats.append(fused)\n            aligned_for_dec.append(fused)\n        target = fused_feats[-1]\n        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.reduce(concat)\n        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n        logits = self.classifier(z)\n        out = {\"logits\": logits, \"feat\": z}\n        if self.use_seg:\n            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n\n        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n        if grl_lambda > 0.0:\n            # GRL implemented outside (we'll pass z through GRL function)\n            pass\n        out[\"domain_logits\"] = self.domain_head(z)\n        return out\n\n# instantiate model\nmodel = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\nprint(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)","metadata":{"_uuid":"71df8e36-6644-423d-86c1-0b9cc366e4b4","_cell_guid":"e1370fdf-9622-4230-b0ee-8c6cfe884584","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:23.669252Z","iopub.execute_input":"2025-11-01T10:15:23.669567Z","iopub.status.idle":"2025-11-01T10:15:24.915804Z","shell.execute_reply.started":"2025-11-01T10:15:23.669548Z","shell.execute_reply":"2025-11-01T10:15:24.915019Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Model parameters (M): 51.586615\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss_logits(pred_logits, target):\n    pred = torch.sigmoid(pred_logits)\n    target = target.float()\n    inter = (pred * target).sum(dim=(1,2,3))\n    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n    return 1.0 - dice.mean()\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nseg_bce = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n","metadata":{"_uuid":"66ed0d4e-2188-4630-b4b8-da16939ceaf1","_cell_guid":"8981b77b-e686-4024-a7ef-106fc71c5573","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.916629Z","iopub.execute_input":"2025-11-01T10:15:24.917161Z","iopub.status.idle":"2025-11-01T10:15:24.926959Z","shell.execute_reply.started":"2025-11-01T10:15:24.917136Z","shell.execute_reply":"2025-11-01T10:15:24.926108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#Supervised contrastive Loss\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        # features: [N, D], labels: [N]\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        # remove diagonal\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        # for each i, positive samples are where mask==1 (excluding self)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        # avoid divide by zero\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        # average only across anchors that have positives\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\nsupcon_loss_fn = SupConLoss(temperature=0.07)","metadata":{"_uuid":"27aad2d9-4477-4430-b87c-2805045736df","_cell_guid":"6ab89b5b-ccc8-4af3-9590-bc566204b65a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.927665Z","iopub.execute_input":"2025-11-01T10:15:24.927896Z","iopub.status.idle":"2025-11-01T10:15:24.944316Z","shell.execute_reply.started":"2025-11-01T10:15:24.927871Z","shell.execute_reply":"2025-11-01T10:15:24.943656Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Domain Adversarial: Gradient Reversal Layer (GRL)\n\nfrom torch.autograd import Function\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)","metadata":{"_uuid":"ffb8ae99-636a-43c6-9797-f85ea97f4b17","_cell_guid":"937a9a70-b252-4638-91b4-0acbcd8214f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.945141Z","iopub.execute_input":"2025-11-01T10:15:24.945299Z","iopub.status.idle":"2025-11-01T10:15:24.959356Z","shell.execute_reply.started":"2025-11-01T10:15:24.945285Z","shell.execute_reply":"2025-11-01T10:15:24.958815Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Optimizer + scheduler + mixed precision + clipping\n# -----------------------------\n# param groups: smaller LR for backbones, larger for heads\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2},\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# warmup + cosine schedule\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        # cosine from warmup -> total\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# -----------------------------\n# Mixup & CutMix helpers\n# -----------------------------\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)   # use builtin int\n    cut_h = int(H * cut_rat)   # use builtin int\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted","metadata":{"_uuid":"73e24310-551f-4554-bb03-48f305bb7ae3","_cell_guid":"5d0a6d0b-5ee0-459f-a9c4-f361ce6591ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.960063Z","iopub.execute_input":"2025-11-01T10:15:24.960255Z","iopub.status.idle":"2025-11-01T10:15:24.978605Z","shell.execute_reply.started":"2025-11-01T10:15:24.960237Z","shell.execute_reply":"2025-11-01T10:15:24.977996Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/4242938264.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Training","metadata":{"_uuid":"98de1d78-23d6-4f6e-8856-1c2bf4801903","_cell_guid":"8cefc78d-dfc4-4a80-853c-de8cdbaa0092","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"best_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    # mix_info: (mode, y_a, y_b, lam) or None\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        # mixup/cutmix: soft labels\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            # focal is not designed for soft labels; approximate by weighted CE\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\n\n\nfor epoch in range(1, EPOCHS+1):\n    # --- CHANGED: Improved freeze/unfreeze strategy ---\n    # We set FREEZE_EPOCHS = 5 in cell 5 (to match WARMUP_EPOCHS)\n    if epoch <= FREEZE_EPOCHS:\n        # Freeze ALL backbone parameters\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['backA', 'backB', 'backS']):\n                p.requires_grad = False\n    elif epoch == FREEZE_EPOCHS + 1:\n        # Unfreeze all parameters *once* after freeze period\n        print(f\"--- Unfreezing all backbone layers at epoch {epoch} ---\")\n        for p in model.parameters():\n            p.requires_grad = True\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    opt.zero_grad() \n    \n    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n        imgs = weak_imgs\n\n        # optionally apply mixup/cutmix on imgs (on weak view)\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n            out = model(imgs)  # returns logits, feat, seg, domain_logits\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            domain_logits = out.get(\"domain_logits\", None)\n\n            # classification loss (label-smoothing or focal)\n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # segmentation loss if available & mask present\n            seg_loss = 0.0\n            if USE_SEGMENTATION and (masks is not None):\n                seg_pred = out[\"seg\"]\n                seg_loss = seg_loss_fn(seg_pred, masks)\n            # supcon loss on features (use features from weak)\n            supcon_loss = supcon_loss_fn(feat, labels)\n\n            # consistency: forward strong view and compare predictions\n            out_strong = model(strong_imgs)\n            logits_strong = out_strong[\"logits\"]\n            probs_weak = F.softmax(logits.detach(), dim=1)\n            probs_strong = F.softmax(logits_strong, dim=1)\n            # L2 between probability vectors (could be KL)\n            cons_loss = F.mse_loss(probs_weak, probs_strong)\n            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n            dom_loss = 0.0\n\n            total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n\n            # 2. Scale the loss by accumulation steps to average the gradients\n            total_loss = total_loss / ACCUMULATION_STEPS \n\n        # Perform backward pass (gradients are accumulated until step is called)\n        scaler.scale(total_loss).backward()\n\n        # 3. Optimizer step only every ACCUMULATION_STEPS batches\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            # gradient clipping before step\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad() # Prepare for next accumulation cycle\n\n        running_loss += total_loss.item() * ACCUMULATION_STEPS # Re-scale back for correct loss tracking\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    # 4. Take a final step if there are remaining gradients (i.e., last batch was not a multiple of ACCUMULATION_STEPS)\n    if n_batches % ACCUMULATION_STEPS != 0:\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad()\n\n    scheduler.step()\n\n    # metrics (rest of the code remains the same)\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += seg_loss_fn(seg_out, masks)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    # early stopping & save best\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)","metadata":{"_uuid":"8f0812c9-bc6d-452f-b8dd-b4983872595a","_cell_guid":"4e9e207b-133f-40e0-8941-e99fb09ea18c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-01T10:15:24.979402Z","iopub.execute_input":"2025-11-01T10:15:24.979616Z","execution_failed":"2025-11-01T12:02:10.303Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"Train 1/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/1043995754.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 1/20: 100%|██████████| 1769/1769 [20:54<00:00,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 3.8569 Acc: 0.5082 Prec: 0.5076 Rec: 0.5072 F1: 0.5010\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Val Loss: 2.2284 Acc: 0.4950 Prec: 0.4075 Rec: 0.4934 F1: 0.3409\nSaved best model at epoch 1 (F1 0.3409)\n","output_type":"stream"},{"name":"stderr","text":"Train 2/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/1043995754.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 2/20: 100%|██████████| 1769/1769 [20:50<00:00,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Train Loss: 2.5716 Acc: 0.7430 Prec: 0.7430 Rec: 0.7430 F1: 0.7430\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Val Loss: 1.5401 Acc: 0.9295 Prec: 0.9378 Rec: 0.9297 F1: 0.9291\nSaved best model at epoch 2 (F1 0.9291)\n","output_type":"stream"},{"name":"stderr","text":"Train 3/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/1043995754.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 3/20: 100%|██████████| 1769/1769 [20:51<00:00,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Train Loss: 2.2894 Acc: 0.8025 Prec: 0.8029 Rec: 0.8024 F1: 0.8024\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Val Loss: 1.5061 Acc: 0.9393 Prec: 0.9445 Rec: 0.9395 F1: 0.9391\nSaved best model at epoch 3 (F1 0.9391)\n","output_type":"stream"},{"name":"stderr","text":"Train 4/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/1043995754.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 4/20: 100%|██████████| 1769/1769 [20:50<00:00,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Train Loss: 2.2285 Acc: 0.8080 Prec: 0.8082 Rec: 0.8081 F1: 0.8080\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Val Loss: 1.5217 Acc: 0.9473 Prec: 0.9500 Rec: 0.9475 F1: 0.9473\nSaved best model at epoch 4 (F1 0.9473)\n","output_type":"stream"},{"name":"stderr","text":"Train 5/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_36/1043995754.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 5/20:  81%|████████  | 1426/1769 [16:48<04:03,  1.41it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Test-time augmentation (TTA) helper\n# -----------------------------\ndef tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n    model.eval()\n    logits_accum = None\n    with torch.no_grad():\n        for s in scales:\n            tf = transforms.Compose([\n                transforms.Resize((s, s)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n            x = tf(img_pil).unsqueeze(0).to(device)\n            out = model(x)\n            logits = out[\"logits\"]\n            if flip:\n                x_f = torch.flip(x, dims=[3])\n                logits_f = model(x_f)[\"logits\"]\n                logits = (logits + logits_f) / 2.0\n            if logits_accum is None:\n                logits_accum = logits\n            else:\n                logits_accum += logits\n    logits_accum /= len(scales)\n    return logits_accum","metadata":{"_uuid":"651a579b-350f-4e7b-8a86-656d77f18664","_cell_guid":"69b67bd8-3130-4ad2-a32d-5c37cc0d2cd0","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-11-01T12:02:10.304Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n    \"\"\"\n    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n    Returns upsampled heatmap (H,W) normalized in [0,1].\n    \"\"\"\n    model.eval()\n    # find layer\n    target_module = None\n    for name, module in model.named_modules():\n        if name == layer_name:\n            target_module = module\n            break\n    if target_module is None:\n        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, input, output):\n        activations.append(output.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    h1 = target_module.register_forward_hook(forward_hook)\n    h2 = target_module.register_full_backward_hook(backward_hook)\n\n    out = model(input_tensor)\n    logits = out[\"logits\"]\n    if target_class is None:\n        target_class = logits.argmax(1).item()\n    loss = logits[:, target_class].sum()\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    act = activations[0]  # [B,C,H,W]\n    grad = gradients[0]   # [B,C,H,W]\n    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n    cam = F.relu(cam)\n    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n    cam = cam.squeeze().cpu().numpy()\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h1.remove(); h2.remove()\n    return cam","metadata":{"_uuid":"5e4aa318-69be-4863-bbf3-ff3c2c037fbd","_cell_guid":"83d30302-59ed-41f9-8b5d-eb7dae9b98b0","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-11-01T12:02:10.304Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}