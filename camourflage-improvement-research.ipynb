{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:32:23.422190Z","iopub.execute_input":"2025-09-16T15:32:23.422437Z","iopub.status.idle":"2025-09-16T15:32:23.433437Z","shell.execute_reply.started":"2025-09-16T15:32:23.422413Z","shell.execute_reply":"2025-09-16T15:32:23.432816Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:33:22.622679Z","iopub.execute_input":"2025-09-16T15:33:22.623372Z","iopub.status.idle":"2025-09-16T15:33:27.286872Z","shell.execute_reply.started":"2025-09-16T15:33:22.623347Z","shell.execute_reply":"2025-09-16T15:33:27.286301Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:36:08.955873Z","iopub.execute_input":"2025-09-16T15:36:08.956732Z","iopub.status.idle":"2025-09-16T15:36:09.186198Z","shell.execute_reply.started":"2025-09-16T15:36:08.956699Z","shell.execute_reply":"2025-09-16T15:36:09.185631Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:36:37.579887Z","iopub.execute_input":"2025-09-16T15:36:37.580163Z","iopub.status.idle":"2025-09-16T15:36:37.687096Z","shell.execute_reply.started":"2025-09-16T15:36:37.580143Z","shell.execute_reply":"2025-09-16T15:36:37.686469Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 8          \nEPOCHS = 10\nNUM_WORKERS = 0     \nLR = 3e-4\nLABEL_SMOOTH = 0.1\nUSE_SEGMENTATION = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:37:09.178971Z","iopub.execute_input":"2025-09-16T15:37:09.179218Z","iopub.status.idle":"2025-09-16T15:37:09.183032Z","shell.execute_reply.started":"2025-09-16T15:37:09.179200Z","shell.execute_reply":"2025-09-16T15:37:09.182347Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"info_dir  = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir = \"/kaggle/input/cod10k/COD10K-v3/Train\"\ntest_dir  = \"/kaggle/input/cod10k/COD10K-v3/Test\"\n\n# these exist in Info/\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:37:51.621653Z","iopub.execute_input":"2025-09-16T15:37:51.622321Z","iopub.status.idle":"2025-09-16T15:37:51.626750Z","shell.execute_reply.started":"2025-09-16T15:37:51.622289Z","shell.execute_reply":"2025-09-16T15:37:51.626086Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def merge_txts(txt_list, root_dir, transform, use_masks=True):\n    samples = []\n    for txt_file, label in txt_list:\n        with open(txt_file, \"r\") as f:\n            for line in f:\n                fname = line.strip().split()[0]\n                samples.append((fname, label))\n    return COD10KDataset(root_dir, txt_file=None, transform=transform, use_masks=use_masks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:39:07.799349Z","iopub.execute_input":"2025-09-16T15:39:07.800029Z","iopub.status.idle":"2025-09-16T15:39:07.804495Z","shell.execute_reply.started":"2025-09-16T15:39:07.800005Z","shell.execute_reply":"2025-09-16T15:39:07.803785Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Noise + Transform","metadata":{}},{"cell_type":"code","source":"class AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\ntrain_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),         # noise added after converting to tensor\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:39:57.435022Z","iopub.execute_input":"2025-09-16T15:39:57.435749Z","iopub.status.idle":"2025-09-16T15:39:57.443073Z","shell.execute_reply.started":"2025-09-16T15:39:57.435711Z","shell.execute_reply":"2025-09-16T15:39:57.442296Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class COD10KDataset(Dataset):\n    \"\"\"\n    Parses lines of the form:\n      <filename> <label>\n    or where label missing, infers from filename containing 'CAM' or 'NonCAM'.\n    Loads optional masks from GT_Object/<basename>.png if present.\n    \"\"\"\n    def __init__(self, root_dir, txt_file, transform=None, use_masks=True):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if not os.path.exists(txt_file):\n            raise RuntimeError(f\"TXT file not found: {txt_file}\")\n\n        with open(txt_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 0:\n                    continue\n                if len(parts) >= 2:\n                    fname = parts[0]\n                    try:\n                        lbl = int(parts[1])\n                    except:\n                        # fallback: try to parse last token as int\n                        lbl = 1 if \"CAM\" in fname else 0\n                else:\n                    fname = parts[0]\n                    # CORRECTED: use parts[0] to decide label, don't refer to undefined variable\n                    lbl = 1 if \"CAM\" in parts[0] else 0\n                img_path = os.path.join(self.root_dir, \"Image\", fname)\n                if os.path.exists(img_path):\n                    self.samples.append((fname, lbl))\n                else:\n                    # Skip missing images (print once or collect for user)\n                    pass\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No samples found. Check {txt_file} and the Image folder under {root_dir}.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        fname, lbl = self.samples[idx]\n        img_path = os.path.join(self.root_dir, \"Image\", fname)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            x = self.transform(img)\n        else:\n            x = transforms.ToTensor()(img)\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(fname)[0] + \".png\"\n            mask_path = os.path.join(self.root_dir, \"GT_Object\", mask_name)\n            if os.path.exists(mask_path):\n                m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                m = np.array(m).astype(np.float32) / 255.0\n                mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n\n        return x, lbl, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:41:05.662016Z","iopub.execute_input":"2025-09-16T15:41:05.662318Z","iopub.status.idle":"2025-09-16T15:41:05.671875Z","shell.execute_reply.started":"2025-09-16T15:41:05.662296Z","shell.execute_reply":"2025-09-16T15:41:05.671058Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_ds = COD10KDataset(train_dir, train_cam_txt, transform=train_tf, use_masks=USE_SEGMENTATION)\nval_ds   = COD10KDataset(test_dir,  test_cam_txt,  transform=val_tf,   use_masks=USE_SEGMENTATION)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\nprint(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:41:12.906144Z","iopub.execute_input":"2025-09-16T15:41:12.906620Z","iopub.status.idle":"2025-09-16T15:41:28.404420Z","shell.execute_reply.started":"2025-09-16T15:41:12.906596Z","shell.execute_reply":"2025-09-16T15:41:28.403685Z"}},"outputs":[{"name":"stdout","text":"Train samples: 3038 Val samples: 2026\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Backbones","metadata":{}},{"cell_type":"code","source":"class DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:42:23.543206Z","iopub.execute_input":"2025-09-16T15:42:23.543739Z","iopub.status.idle":"2025-09-16T15:42:23.549655Z","shell.execute_reply.started":"2025-09-16T15:42:23.543718Z","shell.execute_reply":"2025-09-16T15:42:23.548904Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class SwinExtractor(nn.Module):\n    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n        super().__init__()\n        # timm features_only model returns list of feature maps\n        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:42:30.134281Z","iopub.execute_input":"2025-09-16T15:42:30.134539Z","iopub.status.idle":"2025-09-16T15:42:30.139231Z","shell.execute_reply.started":"2025-09-16T15:42:30.134520Z","shell.execute_reply":"2025-09-16T15:42:30.138687Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction,4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:42:38.360143Z","iopub.execute_input":"2025-09-16T15:42:38.360449Z","iopub.status.idle":"2025-09-16T15:42:38.365830Z","shell.execute_reply.started":"2025-09-16T15:42:38.360427Z","shell.execute_reply":"2025-09-16T15:42:38.365069Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        # ensure same spatial\n        if H.shape[2:] != X.shape[2:]:\n            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n        g = self.g_fc(H)\n        return g * H + (1 - g) * X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:42:46.892370Z","iopub.execute_input":"2025-09-16T15:42:46.892964Z","iopub.status.idle":"2025-09-16T15:42:46.898008Z","shell.execute_reply.started":"2025-09-16T15:42:46.892940Z","shell.execute_reply":"2025-09-16T15:42:46.897312Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, d_cnn, d_swin, d_out):\n        super().__init__()\n        self.q = nn.Linear(d_cnn, d_out)\n        self.k = nn.Linear(d_swin, d_out)\n        self.v = nn.Linear(d_swin, d_out)\n        self.scale = d_out ** -0.5\n    def forward(self, feat_cnn, feat_swin):\n        # feat_cnn: [B, Cc, H, W] -> [B, Nq, Cc]\n        # feat_swin: [B, Cs, Hs, Ws] -> [B, Ns, Cs]\n        B, Cc, H, W = feat_cnn.shape\n        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)  # [B, Nq, Cc]\n        if feat_swin.dim() == 4:\n            Bs, Cs, Hs, Ws = feat_swin.shape\n            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)  # [B, Ns, Cs]\n        else:\n            kv = feat_swin\n        K = self.k(kv)    # [B, Ns, d_out]\n        V = self.v(kv)\n        Q = self.q(q)     # [B, Nq, d_out]\n        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # [B, Nq, Ns]\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, V)  # [B, Nq, d_out]\n        out = out.reshape(B, H, W, -1).permute(0,3,1,2)  # [B, d_out, H, W]\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:42:58.924737Z","iopub.execute_input":"2025-09-16T15:42:58.925032Z","iopub.status.idle":"2025-09-16T15:42:58.931366Z","shell.execute_reply.started":"2025-09-16T15:42:58.925012Z","shell.execute_reply":"2025-09-16T15:42:58.930816Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Segmentation Decoder","metadata":{}},{"cell_type":"code","source":"class SegDecoder(nn.Module):\n    def __init__(self, in_channels_list, mid_channels=128):\n        super().__init__()\n        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n        self.out = nn.Conv2d(mid_channels, 1, 1)\n    def forward(self, feat_list):\n        target_size = feat_list[0].shape[2:]\n        ups = []\n        for f, p in zip(feat_list, self.projs):\n            x = p(f)\n            if x.shape[2:] != target_size:\n                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n            ups.append(x)\n        x = torch.cat(ups, dim=1)\n        x = self.conv(x)\n        x = self.out(x)\n        return x  # [B,1,H,W]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:43:24.328664Z","iopub.execute_input":"2025-09-16T15:43:24.329478Z","iopub.status.idle":"2025-09-16T15:43:24.335544Z","shell.execute_reply.started":"2025-09-16T15:43:24.329437Z","shell.execute_reply":"2025-09-16T15:43:24.334843Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Probing Backbones","metadata":{}},{"cell_type":"code","source":"dnet = DenseNetExtractor().to(device).eval()\nmnet = MobileNetExtractor().to(device).eval()\nsnet = SwinExtractor().to(device).eval()\nwith torch.no_grad():\n    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n    featsA = dnet(dummy)\n    featsB = mnet(dummy)\n    featsS = snet(dummy)\nchA = [f.shape[1] for f in featsA]\nchB = [f.shape[1] for f in featsB]\nchS = [f.shape[1] for f in featsS]\nprint(\"DenseNet channels:\", chA)\nprint(\"MobileNet channels:\", chB)\nprint(\"Swin channels:\", chS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:43:49.191287Z","iopub.execute_input":"2025-09-16T15:43:49.191882Z","iopub.status.idle":"2025-09-16T15:43:55.048017Z","shell.execute_reply.started":"2025-09-16T15:43:49.191860Z","shell.execute_reply":"2025-09-16T15:43:55.047333Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n100%|██████████| 77.4M/77.4M [00:00<00:00, 207MB/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 133MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8769e826d8ab499da21275314f94d639"}},"metadata":{}},{"name":"stdout","text":"DenseNet channels: [256, 512, 1792, 1920]\nMobileNet channels: [24, 40, 80, 112]\nSwin channels: [56, 28, 14, 7]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Fusion Model (DenseNet + MobileNet + Swin cross attention)","metadata":{}},{"cell_type":"code","source":"class FusionWithSwin(nn.Module):\n    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True):\n        super().__init__()\n        self.backA = DenseNetExtractor()\n        self.backB = MobileNetExtractor()\n        self.backS = SwinExtractor()\n        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n        self.L = L\n        self.d = d\n        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n        self.reduce = nn.Conv2d(d * L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128, 2)\n        )\n        self.use_seg = use_seg\n        if self.use_seg:\n            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n    def forward(self, x):\n        fa = self.backA(x)\n        fb = self.backB(x)\n        fs = self.backS(x)  # timm returns list [B, C, H, W]\n        fused_feats = []\n        aligned_for_dec = []\n        for i in range(self.L):\n            a = self.alignA[i](fa[i])\n            a = self.cbamA[i](a)\n            b = self.alignB[i](fb[i])\n            b = self.cbamB[i](b)\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused = self.gates[i](a, b)\n            swin_feat = fs[i]\n            swin_att = self.cross_atts[i](fused, swin_feat)\n            if swin_att.shape[2:] != fused.shape[2:]:\n                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n            fused = fused + swin_att\n            fused_feats.append(fused)\n            aligned_for_dec.append(fused)\n        # fuse multi-scale\n        target = fused_feats[-1]\n        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.reduce(concat)\n        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n        logits = self.classifier(z)\n        out = {\"logits\": logits, \"feat\": z}\n        if self.use_seg:\n            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n        return out\n\n# instantiate model\nmodel = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION).to(device)\nprint(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:45:11.996633Z","iopub.execute_input":"2025-09-16T15:45:11.996951Z","iopub.status.idle":"2025-09-16T15:45:13.301656Z","shell.execute_reply.started":"2025-09-16T15:45:11.996930Z","shell.execute_reply":"2025-09-16T15:45:13.300894Z"}},"outputs":[{"name":"stdout","text":"Model parameters (M): 51.520309\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\ndef dice_loss_logits(pred_logits, target):\n    pred = torch.sigmoid(pred_logits)\n    target = target.float()\n    inter = (pred * target).sum(dim=(1,2,3))\n    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n    return 1.0 - dice.mean()\n\nclf_loss_fn = LabelSmoothingCE(LABEL_SMOOTH)\nseg_bce = nn.BCEWithLogitsLoss()\n\n# optimizer (single group for simplicity)\nopt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    # Upsample predictions to GT mask size (224×224)\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:46:25.525427Z","iopub.execute_input":"2025-09-16T15:46:25.525953Z","iopub.status.idle":"2025-09-16T15:46:25.539657Z","shell.execute_reply.started":"2025-09-16T15:46:25.525927Z","shell.execute_reply":"2025-09-16T15:46:25.539105Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"best_f1 = 0.0\nfor epoch in range(1, EPOCHS+1):\n    # -------------------\n    # TRAIN\n    # -------------------\n    model.train()\n    running_loss = 0\n    y_true, y_pred = [], []\n\n    for imgs, labels, masks in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        out = model(imgs)\n        loss = clf_loss_fn(out[\"logits\"], labels)\n        if masks is not None:\n            masks = masks.to(device)\n            loss += seg_loss_fn(out[\"seg\"], masks)\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        running_loss += loss.item()\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(out[\"logits\"].argmax(1).cpu().numpy())\n     # metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average=\"macro\", zero_division=0\n    )\n\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/len(train_loader):.4f} \"\n      f\"Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # VALIDATION\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0\n    with torch.no_grad():\n        for imgs, labels, masks in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            out = model(imgs)\n\n            loss = clf_loss_fn(out[\"logits\"], labels)\n            if masks is not None:\n                masks = masks.to(device)\n                loss += seg_loss_fn(out[\"seg\"], masks)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(out[\"logits\"].argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/len(val_loader):.4f} \"\n          f\"Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T15:47:33.070629Z","iopub.execute_input":"2025-09-16T15:47:33.070922Z"}},"outputs":[{"name":"stderr","text":"Train 1/10: 100%|██████████| 380/380 [05:58<00:00,  1.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 1.3524 Acc: 0.9984 Prec: 0.5000 Rec: 0.4992 F1: 0.4996\n[Epoch 1] Val Loss: 1.2867 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 2/10: 100%|██████████| 380/380 [05:04<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Train Loss: 1.2338 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 2] Val Loss: 1.2286 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 3/10: 100%|██████████| 380/380 [05:04<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Train Loss: 1.2070 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 3] Val Loss: 1.1754 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 4/10: 100%|██████████| 380/380 [05:02<00:00,  1.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Train Loss: 1.1912 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 4] Val Loss: 1.2142 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 5/10: 100%|██████████| 380/380 [05:02<00:00,  1.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Train Loss: 1.1860 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 5] Val Loss: 1.1775 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 6/10: 100%|██████████| 380/380 [06:03<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Train Loss: 1.1725 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 6] Val Loss: 1.2277 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 7/10: 100%|██████████| 380/380 [05:03<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Train Loss: 1.1597 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}