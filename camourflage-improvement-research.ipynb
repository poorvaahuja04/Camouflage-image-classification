{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anuhskaa/camouflage-improvement-research-2?scriptVersionId=272564021\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"9d417f65","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-11-01T06:39:02.113708Z","iopub.status.busy":"2025-11-01T06:39:02.112959Z","iopub.status.idle":"2025-11-01T06:39:02.145445Z","shell.execute_reply":"2025-11-01T06:39:02.144881Z"},"papermill":{"duration":0.041188,"end_time":"2025-11-01T06:39:02.146596","exception":false,"start_time":"2025-11-01T06:39:02.105408","status":"completed"},"tags":[]},"outputs":[],"source":["import os, random, math, time\n","from pathlib import Path\n","from tqdm import tqdm\n","import numpy as np\n","from PIL import Image"]},{"cell_type":"code","execution_count":2,"id":"502bf599","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:02.158932Z","iopub.status.busy":"2025-11-01T06:39:02.158655Z","iopub.status.idle":"2025-11-01T06:39:15.277593Z","shell.execute_reply":"2025-11-01T06:39:15.277015Z"},"papermill":{"duration":13.126294,"end_time":"2025-11-01T06:39:15.278896","exception":false,"start_time":"2025-11-01T06:39:02.152602","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torchvision import transforms, models\n","from torchvision.transforms import RandAugment\n","import timm"]},{"cell_type":"code","execution_count":3,"id":"d31e1b89","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:15.291058Z","iopub.status.busy":"2025-11-01T06:39:15.290825Z","iopub.status.idle":"2025-11-01T06:39:16.539204Z","shell.execute_reply":"2025-11-01T06:39:16.538543Z"},"papermill":{"duration":1.255681,"end_time":"2025-11-01T06:39:16.540603","exception":false,"start_time":"2025-11-01T06:39:15.284922","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from collections import Counter"]},{"cell_type":"code","execution_count":4,"id":"77a96fd1","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.553096Z","iopub.status.busy":"2025-11-01T06:39:16.552731Z","iopub.status.idle":"2025-11-01T06:39:16.645323Z","shell.execute_reply":"2025-11-01T06:39:16.64442Z"},"papermill":{"duration":0.099888,"end_time":"2025-11-01T06:39:16.64648","exception":false,"start_time":"2025-11-01T06:39:16.546592","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if device == \"cuda\": torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"code","execution_count":5,"id":"16095c88","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.658353Z","iopub.status.busy":"2025-11-01T06:39:16.658105Z","iopub.status.idle":"2025-11-01T06:39:16.663919Z","shell.execute_reply":"2025-11-01T06:39:16.663392Z"},"papermill":{"duration":0.012652,"end_time":"2025-11-01T06:39:16.664893","exception":false,"start_time":"2025-11-01T06:39:16.652241","status":"completed"},"tags":[]},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 8          # adjust if OOM\n","EPOCHS = 20\n","NUM_WORKERS = 4         # set 0 if worker issues on Kaggle\n","LR = 3e-4\n","LABEL_SMOOTH = 0.1\n","SAVE_PATH = \"best_model.pth\"\n","USE_SEGMENTATION = True\n","\n","# Loss weights from PDF suggestion\n","ALPHA_DOM = 0.5\n","BETA_SUPCON = 0.2\n","ETA_CONS = 0.1\n","\n","# Mixup/CutMix probabilities and alphas\n","PROB_MIXUP = 0.5\n","PROB_CUTMIX = 0.5\n","MIXUP_ALPHA = 0.2\n","CUTMIX_ALPHA = 1.0\n","\n","# warmup epochs\n","WARMUP_EPOCHS = 5\n","\n","# early stopping\n","EARLY_STOPPING_PATIENCE = 8\n","FREEZE_EPOCHS = 10\n","ACCUMULATION_STEPS = 2"]},{"cell_type":"code","execution_count":6,"id":"b992d91f","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.676306Z","iopub.status.busy":"2025-11-01T06:39:16.676095Z","iopub.status.idle":"2025-11-01T06:39:16.679994Z","shell.execute_reply":"2025-11-01T06:39:16.679321Z"},"papermill":{"duration":0.010704,"end_time":"2025-11-01T06:39:16.680995","exception":false,"start_time":"2025-11-01T06:39:16.670291","status":"completed"},"tags":[]},"outputs":[],"source":["info_dir  = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n","train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train/Image\"\n","test_dir_cod  = \"/kaggle/input/cod10k/COD10K-v3/Test/Image\"\n","\n","# these exist in Info/\n","combined_train_cam  = os.path.join(info_dir, \"CAM_train.txt\")\n","combined_train_noncam  = os.path.join(info_dir, \"NonCAM_train.txt\")\n","combined_test_cam = os.path.join(info_dir, \"CAM_test.txt\")\n","combined_test_noncam = os.path.join(info_dir, \"NonCAM_test.txt\")"]},{"cell_type":"code","execution_count":7,"id":"2d93f0b2","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.693841Z","iopub.status.busy":"2025-11-01T06:39:16.693218Z","iopub.status.idle":"2025-11-01T06:39:16.696887Z","shell.execute_reply":"2025-11-01T06:39:16.696368Z"},"papermill":{"duration":0.010769,"end_time":"2025-11-01T06:39:16.697941","exception":false,"start_time":"2025-11-01T06:39:16.687172","status":"completed"},"tags":[]},"outputs":[],"source":["info_dir2 =\"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n","train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n","train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n","test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n","test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")"]},{"cell_type":"code","execution_count":8,"id":"a04395bf","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.709319Z","iopub.status.busy":"2025-11-01T06:39:16.709097Z","iopub.status.idle":"2025-11-01T06:39:16.712633Z","shell.execute_reply":"2025-11-01T06:39:16.711876Z"},"papermill":{"duration":0.010369,"end_time":"2025-11-01T06:39:16.713682","exception":false,"start_time":"2025-11-01T06:39:16.703313","status":"completed"},"tags":[]},"outputs":[],"source":["train_dir_camo = {\n","    \"cam\": \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage/Images/Train\",\n","    \"noncam\": \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage/Images/Train\"\n","}\n","test_dir_camo = {\n","    \"cam\": \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage/Images/Test\",\n","    \"noncam\": \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage/Images/Test\"\n","}"]},{"cell_type":"code","execution_count":9,"id":"c060a334","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.725438Z","iopub.status.busy":"2025-11-01T06:39:16.725227Z","iopub.status.idle":"2025-11-01T06:39:16.728631Z","shell.execute_reply":"2025-11-01T06:39:16.727902Z"},"papermill":{"duration":0.010707,"end_time":"2025-11-01T06:39:16.729654","exception":false,"start_time":"2025-11-01T06:39:16.718947","status":"completed"},"tags":[]},"outputs":[],"source":["train_dir = [train_dir_cod, train_dir_camo[\"cam\"], train_dir_camo[\"noncam\"]]\n","test_dir  = [test_dir_cod, test_dir_camo[\"cam\"], test_dir_camo[\"noncam\"]]\n"]},{"cell_type":"code","execution_count":10,"id":"f8f0766a","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.741243Z","iopub.status.busy":"2025-11-01T06:39:16.741028Z","iopub.status.idle":"2025-11-01T06:39:16.815688Z","shell.execute_reply":"2025-11-01T06:39:16.814991Z"},"papermill":{"duration":0.081741,"end_time":"2025-11-01T06:39:16.816845","exception":false,"start_time":"2025-11-01T06:39:16.735104","status":"completed"},"tags":[]},"outputs":[],"source":["# Combine train files\n","with open(combined_train_cam) as f1, open(train_cam_txt2) as f2:\n","    train_cam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","with open(combined_train_noncam) as f1, open(train_noncam_txt2) as f2:\n","    train_noncam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","# Combine test files\n","with open(combined_test_cam) as f1, open(test_cam_txt2) as f2:\n","    test_cam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","with open(combined_test_noncam) as f1, open(test_noncam_txt2) as f2:\n","    test_noncam_txt= f1.read().splitlines() + f2.read().splitlines()\n"]},{"cell_type":"markdown","id":"f513894b","metadata":{"papermill":{"duration":0.005271,"end_time":"2025-11-01T06:39:16.827722","exception":false,"start_time":"2025-11-01T06:39:16.822451","status":"completed"},"tags":[]},"source":["Noise + Transform"]},{"cell_type":"code","execution_count":11,"id":"43487f2b","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.83961Z","iopub.status.busy":"2025-11-01T06:39:16.839392Z","iopub.status.idle":"2025-11-01T06:39:16.847352Z","shell.execute_reply":"2025-11-01T06:39:16.846752Z"},"papermill":{"duration":0.015238,"end_time":"2025-11-01T06:39:16.848393","exception":false,"start_time":"2025-11-01T06:39:16.833155","status":"completed"},"tags":[]},"outputs":[],"source":["class AddGaussianNoise(object):\n","    def __init__(self, mean=0., std=0.05):\n","        self.mean = mean\n","        self.std = std\n","    def __call__(self, tensor):\n","        noise = torch.randn(tensor.size()) * self.std + self.mean\n","        noisy_tensor = tensor + noise\n","        return torch.clamp(noisy_tensor, 0., 1.)\n","    def __repr__(self):\n","        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n","\n","weak_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(0., 0.02),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","strong_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n","    RandAugment(num_ops=2, magnitude=9),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(30),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(0., 0.05),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","val_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n"]},{"cell_type":"code","execution_count":12,"id":"45fee3fb","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.860148Z","iopub.status.busy":"2025-11-01T06:39:16.859922Z","iopub.status.idle":"2025-11-01T06:39:16.893315Z","shell.execute_reply":"2025-11-01T06:39:16.892812Z"},"papermill":{"duration":0.040536,"end_time":"2025-11-01T06:39:16.894356","exception":false,"start_time":"2025-11-01T06:39:16.85382","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","from collections import Counter\n","from torch.utils.data import Dataset, WeightedRandomSampler, DataLoader\n","from PIL import Image\n","import torch\n","import numpy as np\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","\n","# NOTE: Placeholder definitions for missing global variables for completeness.\n","# Please ensure these are correctly defined in your main script.\n","IMG_SIZE = 224  # Common size for COD/SOD tasks\n","weak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n","strong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()]) # Placeholder for actual strong transform\n","val_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n","USE_SEGMENTATION = True\n","BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","\n","ACCUMULATION_STEPS = 2\n","# Function to try reading a file with multiple encodings\n","def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n","    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n","    for encoding in encodings:\n","        try:\n","            with open(file_path, 'r', encoding=encoding) as f:\n","                return f.readlines()\n","        except UnicodeDecodeError:\n","            print(f\"Failed to read {file_path} with encoding {encoding}\")\n","        except Exception as e:\n","            print(f\"Error reading {file_path}: {e}\")\n","    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n","\n","def load_testing_dataset_info(info_file, image_dir):\n","    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n","    image_paths = []\n","    labels = []\n","    \n","    # List of encodings to try\n","    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n","    lines = []\n","    \n","    for encoding in encodings:\n","        try:\n","            with open(info_file, 'r', encoding=encoding) as f:\n","                lines = f.readlines()\n","            break  # If successful, break out of the loop\n","        except UnicodeDecodeError:\n","            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n","        except Exception as e:\n","            print(f\"Error reading {info_file}: {e}\")\n","            raise  # Raise the error if it's something unexpected\n","    \n","    # Process the lines if file was successfully read\n","    for line in lines:\n","        parts = line.strip().split()\n","        if len(parts) == 2:\n","            image_filename = parts[0]\n","            # Ensure label is always 0 or 1\n","            try:\n","                label = int(parts[1])\n","            except ValueError:\n","                continue # Skip malformed lines\n","                \n","            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n","            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n","            image_paths.append(image_full_path)\n","            labels.append(label)\n","    \n","    return image_paths, labels\n","\n","\n","# MultiDataset class with proper encoding handling\n","class MultiDataset(Dataset):\n","    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n","        self.root_dirs = root_dirs\n","        self.weak_transform = weak_transform\n","        self.strong_transform = strong_transform\n","        self.use_masks = use_masks\n","        self.samples = []\n","\n","        # Handle the testing dataset (testing-dataset images and labels)\n","        if testing_image_paths is not None and testing_labels is not None:\n","            for img_path, label in zip(testing_image_paths, testing_labels):\n","                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n","                self.samples.append((img_path, label)) \n","        \n","        # Process other datasets (COD10K, CAMO, etc.)\n","        if isinstance(txt_files, str):\n","            txt_files = [txt_files]\n","\n","        all_lines = []\n","        for t in txt_files:\n","            if not os.path.exists(t):\n","                raise RuntimeError(f\"TXT file not found: {t}\")\n","\n","            # Use the read_file_with_encoding function to handle different encodings\n","            lines = read_file_with_encoding(t)\n","            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n","\n","        for line, src_txt in all_lines:\n","            parts = line.split()\n","            if len(parts) == 0:\n","                continue\n","\n","            fname = parts[0]\n","            if len(parts) >= 2:\n","                try:\n","                    lbl = int(parts[1])\n","                except:\n","                    # Fallback classification if label is not an integer\n","                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n","            else:\n","                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n","            \n","            # Map labels to binary (CAM=1, Non-CAM=0)\n","            lbl = 1 if lbl == 1 else 0\n","\n","            found = False\n","            search_subs = [\n","                \"\",  # If image is directly in root_dir (less common)\n","                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n","                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n","            ]\n","            \n","            base_fname = os.path.basename(fname)  \n","\n","            for rdir in self.root_dirs:\n","                for sub in search_subs:\n","                    img_path = os.path.join(rdir, sub, base_fname)\n","                    if os.path.exists(img_path):\n","                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n","                        self.samples.append((img_path, lbl, rdir))\n","                        found = True\n","                        break\n","                if found:\n","                    break\n","\n","            if not found:\n","                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n","\n","        if len(self.samples) == 0:\n","            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n","\n","        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        global IMG_SIZE \n","        \n","        sample = self.samples[idx]\n","        \n","        # 1. SAFELY UNPACK sample tuple (length 2 or 3)\n","        img_path = sample[0]\n","        lbl = sample[1]\n","        \n","        # Define rdir for consistent mask lookup logic\n","        if len(sample) == 3:\n","            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n","            rdir = sample[2]\n","            # Root directory for testing-dataset (used as fallback for mask lookup)\n","            testing_root = None \n","        else:\n","            # If it's a 2-element tuple (testing-dataset split), rdir is undefined.\n","            # We set rdir to a base directory that will be searched for masks.\n","            # Assuming testing_images_dir is defined globally or passed, \n","            # we infer the testing-dataset root from the image path (two levels up)\n","            # This is a guess but required to prevent NameError in mask search.\n","            rdir = os.path.dirname(os.path.dirname(img_path))\n","            # Set testing_root explicitly for clarity if needed later, but rdir is now defined.\n","\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.weak_transform:\n","            weak = self.weak_transform(img)\n","        else:\n","            weak = transforms.ToTensor()(img)\n","            \n","        if self.strong_transform:\n","            strong = self.strong_transform(img)\n","        else:\n","            strong = weak.clone()\n","\n","        mask = None\n","        if self.use_masks:\n","            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n","            \n","            # Use the defined rdir for mask search\n","            found_mask = False\n","            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n","                mask_path = os.path.join(rdir, mask_dir, mask_name)\n","                \n","                if os.path.exists(mask_path):\n","                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n","                    m = np.array(m).astype(np.float32) / 255.0\n","                    # Convert to binary mask: 0 or 1\n","                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n","                    found_mask = True\n","                    break\n","\n","            if mask is None:\n","                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n","                \n","        return weak, strong, lbl, mask\n","\n","def build_weighted_sampler(dataset):\n","    \"\"\"\n","    Builds a WeightedRandomSampler based on class imbalance.\n","    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n","    \"\"\"\n","    \n","    # Safely extract labels (always index 1, regardless of tuple length)\n","    labels = [sample[1] for sample in dataset.samples]  \n","    \n","    counts = Counter(labels)\n","    total = len(labels)\n","    \n","    # Ensure there are at least two classes to calculate class_weights\n","    if len(counts) <= 1:\n","        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n","        weights = [1.0] * total\n","    else:\n","        # Calculate inverse frequency weights\n","        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n","        weights = [class_weights[lbl] for lbl in labels]\n","        \n","    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n","\n","# --- Path and Configuration Setup (as provided by user) ---\n","\n"]},{"cell_type":"code","execution_count":13,"id":"d7a7ee16","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:39:16.906507Z","iopub.status.busy":"2025-11-01T06:39:16.906307Z","iopub.status.idle":"2025-11-01T06:40:16.246269Z","shell.execute_reply":"2025-11-01T06:40:16.245503Z"},"papermill":{"duration":59.353447,"end_time":"2025-11-01T06:40:16.253351","exception":false,"start_time":"2025-11-01T06:39:16.899904","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Loaded 14150 samples from 4 root directories.\n","✅ Loaded 6606 samples from 4 root directories.\n","Total Train samples: 14150 Total Val samples: 6606\n"]}],"source":["info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n","train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n","test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n","    \n","# COD10K Info files\n","train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n","train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n","test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n","test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n","\n","# CAMO-COCO PATHS\n","info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n","\n","# CAMO-COCO Info files\n","train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n","train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n","test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n","test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n","\n","# CAMO-COCO Root Directories\n","train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n","train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n","\n","testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n","testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n","testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n","\n","# Split testing-dataset: 20% train (train_paths), 80% validation (val_paths)\n","train_paths, val_paths, train_labels, val_labels = train_test_split(\n","    testing_image_paths, testing_labels, test_size=0.8, random_state=42\n",")\n","\n","# 1. All Root Directories\n","ALL_ROOT_DIRS = [\n","    train_dir_cod,       \n","    test_dir_cod,       \n","    train_dir_camo_cam,  \n","    train_dir_camo_noncam\n","]\n","\n","# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\n","ALL_TRAIN_TXTS = [\n","    train_cam_txt, train_noncam_txt, test_cam_txt, test_noncam_txt,\n","    train_cam_txt2, train_noncam_txt2, test_cam_txt2, test_noncam_txt2,\n","]\n","\n","# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\n","ALL_VAL_TXTS = []\n","\n","# --- Create the final unified datasets ---\n","\n","# Training Dataset: All external data (via ALL_TRAIN_TXTS) + 20% testing-dataset split (via train_paths)\n","train_ds = MultiDataset(\n","    root_dirs=ALL_ROOT_DIRS, \n","    txt_files=ALL_TRAIN_TXTS,               \n","    testing_image_paths=train_paths,        \n","    testing_labels=train_labels,            \n","    weak_transform=weak_tf, \n","    strong_transform=strong_tf, \n","    use_masks=USE_SEGMENTATION\n",")\n","\n","# Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\n","val_ds = MultiDataset(\n","    root_dirs=ALL_ROOT_DIRS,  \n","    txt_files=ALL_VAL_TXTS,                 \n","    testing_image_paths=val_paths,          \n","    testing_labels=val_labels,              \n","    weak_transform=val_tf, \n","    strong_transform=None, \n","    use_masks=USE_SEGMENTATION\n",")\n","\n","# Build Sampler and DataLoader\n","train_sampler = build_weighted_sampler(train_ds)\n","\n","train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n","val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n","\n","print(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))\n"]},{"cell_type":"markdown","id":"468d6f31","metadata":{"papermill":{"duration":0.005298,"end_time":"2025-11-01T06:40:16.264146","exception":false,"start_time":"2025-11-01T06:40:16.258848","status":"completed"},"tags":[]},"source":["## Backbones"]},{"cell_type":"code","execution_count":14,"id":"047ea141","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.276251Z","iopub.status.busy":"2025-11-01T06:40:16.27561Z","iopub.status.idle":"2025-11-01T06:40:16.282107Z","shell.execute_reply":"2025-11-01T06:40:16.281468Z"},"papermill":{"duration":0.013612,"end_time":"2025-11-01T06:40:16.283311","exception":false,"start_time":"2025-11-01T06:40:16.269699","status":"completed"},"tags":[]},"outputs":[],"source":["class DenseNetExtractor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.densenet201(pretrained=pretrained).features\n","    def forward(self, x):\n","        feats = []\n","        for name, layer in self.features._modules.items():\n","            x = layer(x)\n","            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n","                feats.append(x)\n","        return feats\n","\n","\n","class MobileNetExtractor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n","    def forward(self, x):\n","        feats = []\n","        out = x\n","        for i, layer in enumerate(self.features):\n","            out = layer(out)\n","            if i in (2,5,9,12):\n","                feats.append(out)\n","        if len(feats) < 4:\n","            feats.append(out)\n","        return feats"]},{"cell_type":"code","execution_count":15,"id":"207d4c6b","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.295508Z","iopub.status.busy":"2025-11-01T06:40:16.295319Z","iopub.status.idle":"2025-11-01T06:40:16.299284Z","shell.execute_reply":"2025-11-01T06:40:16.298871Z"},"papermill":{"duration":0.011154,"end_time":"2025-11-01T06:40:16.300275","exception":false,"start_time":"2025-11-01T06:40:16.289121","status":"completed"},"tags":[]},"outputs":[],"source":["class SwinExtractor(nn.Module):\n","    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":16,"id":"f997a512","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.312759Z","iopub.status.busy":"2025-11-01T06:40:16.312209Z","iopub.status.idle":"2025-11-01T06:40:16.317729Z","shell.execute_reply":"2025-11-01T06:40:16.316937Z"},"papermill":{"duration":0.012938,"end_time":"2025-11-01T06:40:16.318805","exception":false,"start_time":"2025-11-01T06:40:16.305867","status":"completed"},"tags":[]},"outputs":[],"source":["class CBAMlite(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super().__init__()\n","        self.se = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(channels, max(channels//reduction,4), 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(max(channels//reduction,4), channels, 1),\n","            nn.Sigmoid()\n","        )\n","        self.spatial = nn.Sequential(\n","            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels, 1, 1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        return x * self.se(x) * self.spatial(x)\n"]},{"cell_type":"code","execution_count":17,"id":"2eabd1d9","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.331274Z","iopub.status.busy":"2025-11-01T06:40:16.331049Z","iopub.status.idle":"2025-11-01T06:40:16.335671Z","shell.execute_reply":"2025-11-01T06:40:16.335132Z"},"papermill":{"duration":0.012109,"end_time":"2025-11-01T06:40:16.336636","exception":false,"start_time":"2025-11-01T06:40:16.324527","status":"completed"},"tags":[]},"outputs":[],"source":["class GatedFusion(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.g_fc = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(dim, max(dim//4, 4), 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(max(dim//4,4), dim, 1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, H, X):\n","        if H.shape[2:] != X.shape[2:]:\n","            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n","        g = self.g_fc(H)\n","        return g * H + (1 - g) * X"]},{"cell_type":"code","execution_count":18,"id":"6df18c90","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.349298Z","iopub.status.busy":"2025-11-01T06:40:16.349084Z","iopub.status.idle":"2025-11-01T06:40:16.355719Z","shell.execute_reply":"2025-11-01T06:40:16.354897Z"},"papermill":{"duration":0.014407,"end_time":"2025-11-01T06:40:16.356856","exception":false,"start_time":"2025-11-01T06:40:16.342449","status":"completed"},"tags":[]},"outputs":[],"source":["class CrossAttention(nn.Module):\n","    def __init__(self, d_cnn, d_swin, d_out):\n","        super().__init__()\n","        self.q = nn.Linear(d_cnn, d_out)\n","        self.k = nn.Linear(d_swin, d_out)\n","        self.v = nn.Linear(d_swin, d_out)\n","        self.scale = d_out ** -0.5\n","    def forward(self, feat_cnn, feat_swin):\n","        B, Cc, H, W = feat_cnn.shape\n","        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n","        if feat_swin.dim() == 4:\n","            Bs, Cs, Hs, Ws = feat_swin.shape\n","            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n","        else:\n","            kv = feat_swin\n","        K = self.k(kv)\n","        V = self.v(kv)\n","        Q = self.q(q)\n","        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        out = torch.matmul(attn, V)\n","        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n","        return out\n"]},{"cell_type":"markdown","id":"c75f12ec","metadata":{"papermill":{"duration":0.005906,"end_time":"2025-11-01T06:40:16.368823","exception":false,"start_time":"2025-11-01T06:40:16.362917","status":"completed"},"tags":[]},"source":["## Segmentation Decoder"]},{"cell_type":"code","execution_count":19,"id":"565d1291","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.38176Z","iopub.status.busy":"2025-11-01T06:40:16.381531Z","iopub.status.idle":"2025-11-01T06:40:16.38739Z","shell.execute_reply":"2025-11-01T06:40:16.386799Z"},"papermill":{"duration":0.013559,"end_time":"2025-11-01T06:40:16.38845","exception":false,"start_time":"2025-11-01T06:40:16.374891","status":"completed"},"tags":[]},"outputs":[],"source":["class SegDecoder(nn.Module):\n","    def __init__(self, in_channels_list, mid_channels=128):\n","        super().__init__()\n","        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n","        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n","        self.out = nn.Conv2d(mid_channels, 1, 1)\n","    def forward(self, feat_list):\n","        target_size = feat_list[0].shape[2:]\n","        ups = []\n","        for f, p in zip(feat_list, self.projs):\n","            x = p(f)\n","            if x.shape[2:] != target_size:\n","                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n","            ups.append(x)\n","        x = torch.cat(ups, dim=1)\n","        x = self.conv(x)\n","        x = self.out(x)\n","        return x"]},{"cell_type":"markdown","id":"7886bb05","metadata":{"papermill":{"duration":0.005974,"end_time":"2025-11-01T06:40:16.400471","exception":false,"start_time":"2025-11-01T06:40:16.394497","status":"completed"},"tags":[]},"source":["## Probing Backbones"]},{"cell_type":"code","execution_count":20,"id":"7150a4ae","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:16.413383Z","iopub.status.busy":"2025-11-01T06:40:16.413154Z","iopub.status.idle":"2025-11-01T06:40:21.419606Z","shell.execute_reply":"2025-11-01T06:40:21.418622Z"},"papermill":{"duration":5.014706,"end_time":"2025-11-01T06:40:21.421185","exception":false,"start_time":"2025-11-01T06:40:16.406479","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n","100%|██████████| 77.4M/77.4M [00:00<00:00, 196MB/s]\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n","100%|██████████| 21.1M/21.1M [00:00<00:00, 125MB/s] \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"091187b7110943b8b818fd55c389f321","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DenseNet channels: [256, 512, 1792, 1920]\n","MobileNet channels: [24, 40, 80, 112]\n","Swin channels: [56, 28, 14, 7]\n"]}],"source":["dnet = DenseNetExtractor().to(device).eval()\n","mnet = MobileNetExtractor().to(device).eval()\n","snet = SwinExtractor().to(device).eval()\n","with torch.no_grad():\n","    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n","    featsA = dnet(dummy)\n","    featsB = mnet(dummy)\n","    featsS = snet(dummy)\n","chA = [f.shape[1] for f in featsA]\n","chB = [f.shape[1] for f in featsB]\n","chS = [f.shape[1] for f in featsS]\n","print(\"DenseNet channels:\", chA)\n","print(\"MobileNet channels:\", chB)\n","print(\"Swin channels:\", chS)"]},{"cell_type":"markdown","id":"638fc7a4","metadata":{"papermill":{"duration":0.006283,"end_time":"2025-11-01T06:40:21.434812","exception":false,"start_time":"2025-11-01T06:40:21.428529","status":"completed"},"tags":[]},"source":["# Fusion Model (DenseNet + MobileNet + Swin cross attention)"]},{"cell_type":"code","execution_count":21,"id":"509bb897","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:21.448696Z","iopub.status.busy":"2025-11-01T06:40:21.448118Z","iopub.status.idle":"2025-11-01T06:40:22.755838Z","shell.execute_reply":"2025-11-01T06:40:22.755003Z"},"papermill":{"duration":1.316117,"end_time":"2025-11-01T06:40:22.757121","exception":false,"start_time":"2025-11-01T06:40:21.441004","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model parameters (M): 51.586615\n"]}],"source":["class FusionWithSwin(nn.Module):\n","    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n","        super().__init__()\n","        self.backA = DenseNetExtractor()\n","        self.backB = MobileNetExtractor()\n","        self.backS = SwinExtractor()\n","        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n","        self.L = L\n","        self.d = d\n","        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n","        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n","        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n","        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n","        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n","        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n","        self.reduce = nn.Conv2d(d * L, d, 1)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(128, num_classes)\n","        )\n","        self.use_seg = use_seg\n","        if self.use_seg:\n","            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n","\n","        # Domain head for DANN (simple MLP)\n","        self.domain_head = nn.Sequential(\n","            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(256, 2)\n","        )\n","\n","    def forward(self, x, grl_lambda=0.0):\n","        fa = self.backA(x)\n","        fb = self.backB(x)\n","        fs = self.backS(x)\n","        fused_feats = []\n","        aligned_for_dec = []\n","        for i in range(self.L):\n","            a = self.alignA[i](fa[i])\n","            a = self.cbamA[i](a)\n","            b = self.alignB[i](fb[i])\n","            b = self.cbamB[i](b)\n","            if b.shape[2:] != a.shape[2:]:\n","                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n","            fused = self.gates[i](a, b)\n","            swin_feat = fs[i]\n","            swin_att = self.cross_atts[i](fused, swin_feat)\n","            if swin_att.shape[2:] != fused.shape[2:]:\n","                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n","            fused = fused + swin_att\n","            fused_feats.append(fused)\n","            aligned_for_dec.append(fused)\n","        target = fused_feats[-1]\n","        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n","        concat = torch.cat(upsampled, dim=1)\n","        fused = self.reduce(concat)\n","        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n","        logits = self.classifier(z)\n","        out = {\"logits\": logits, \"feat\": z}\n","        if self.use_seg:\n","            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n","\n","        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n","        if grl_lambda > 0.0:\n","            # GRL implemented outside (we'll pass z through GRL function)\n","            pass\n","        out[\"domain_logits\"] = self.domain_head(z)\n","        return out\n","\n","# instantiate model\n","model = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\n","print(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)"]},{"cell_type":"code","execution_count":22,"id":"5fb2f462","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:22.771606Z","iopub.status.busy":"2025-11-01T06:40:22.771346Z","iopub.status.idle":"2025-11-01T06:40:22.781418Z","shell.execute_reply":"2025-11-01T06:40:22.78085Z"},"papermill":{"duration":0.018299,"end_time":"2025-11-01T06:40:22.782474","exception":false,"start_time":"2025-11-01T06:40:22.764175","status":"completed"},"tags":[]},"outputs":[],"source":["class LabelSmoothingCE(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        self.s = smoothing\n","    def forward(self, logits, target):\n","        c = logits.size(-1)\n","        logp = F.log_softmax(logits, dim=-1)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(logp)\n","            true_dist.fill_(self.s / (c - 1))\n","            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n","        return (-true_dist * logp).sum(dim=-1).mean()\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=1.5):\n","        super().__init__()\n","        self.gamma = gamma\n","    def forward(self, logits, target):\n","        prob = F.softmax(logits, dim=1)\n","        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n","        ce = F.cross_entropy(logits, target, reduction='none')\n","        loss = ((1 - pt) ** self.gamma) * ce\n","        return loss.mean()\n","\n","def dice_loss_logits(pred_logits, target):\n","    pred = torch.sigmoid(pred_logits)\n","    target = target.float()\n","    inter = (pred * target).sum(dim=(1,2,3))\n","    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n","    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n","    return 1.0 - dice.mean()\n","\n","clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n","clf_loss_focal = FocalLoss(gamma=1.5)\n","seg_bce = nn.BCEWithLogitsLoss()\n","\n","def dice_loss(pred, target, smooth=1.0):\n","    pred = torch.sigmoid(pred)\n","    num = 2 * (pred * target).sum() + smooth\n","    den = pred.sum() + target.sum() + smooth\n","    return 1 - (num / den)\n","\n","def seg_loss_fn(pred, mask):\n","    if pred.shape[-2:] != mask.shape[-2:]:\n","        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n","    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n"]},{"cell_type":"code","execution_count":23,"id":"dca62f2f","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:22.795876Z","iopub.status.busy":"2025-11-01T06:40:22.795689Z","iopub.status.idle":"2025-11-01T06:40:22.801974Z","shell.execute_reply":"2025-11-01T06:40:22.801446Z"},"papermill":{"duration":0.014044,"end_time":"2025-11-01T06:40:22.803046","exception":false,"start_time":"2025-11-01T06:40:22.789002","status":"completed"},"tags":[]},"outputs":[],"source":["#Supervised contrastive Loss\n","class SupConLoss(nn.Module):\n","    def __init__(self, temperature=0.07):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.cos = nn.CosineSimilarity(dim=-1)\n","    def forward(self, features, labels):\n","        # features: [N, D], labels: [N]\n","        device = features.device\n","        f = F.normalize(features, dim=1)\n","        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n","        labels = labels.contiguous().view(-1,1)\n","        mask = torch.eq(labels, labels.T).float().to(device)\n","        # remove diagonal\n","        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n","        logits = sim - logits_max.detach()\n","        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n","        denom = exp_logits.sum(1, keepdim=True)\n","        # for each i, positive samples are where mask==1 (excluding self)\n","        pos_mask = mask - torch.eye(len(features), device=device)\n","        pos_exp = (exp_logits * pos_mask).sum(1)\n","        # avoid divide by zero\n","        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n","        # average only across anchors that have positives\n","        valid = (pos_mask.sum(1) > 0).float()\n","        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n","        return loss\n","supcon_loss_fn = SupConLoss(temperature=0.07)"]},{"cell_type":"code","execution_count":24,"id":"1a63b46c","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:22.816082Z","iopub.status.busy":"2025-11-01T06:40:22.815863Z","iopub.status.idle":"2025-11-01T06:40:22.820175Z","shell.execute_reply":"2025-11-01T06:40:22.819494Z"},"papermill":{"duration":0.01186,"end_time":"2025-11-01T06:40:22.821307","exception":false,"start_time":"2025-11-01T06:40:22.809447","status":"completed"},"tags":[]},"outputs":[],"source":["# Domain Adversarial: Gradient Reversal Layer (GRL)\n","\n","from torch.autograd import Function\n","class GradReverse(Function):\n","    @staticmethod\n","    def forward(ctx, x, l):\n","        ctx.l = l\n","        return x.view_as(x)\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return grad_output.neg() * ctx.l, None\n","\n","def grad_reverse(x, l=1.0):\n","    return GradReverse.apply(x, l)"]},{"cell_type":"code","execution_count":25,"id":"b0968fac","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:22.834878Z","iopub.status.busy":"2025-11-01T06:40:22.834207Z","iopub.status.idle":"2025-11-01T06:40:22.849477Z","shell.execute_reply":"2025-11-01T06:40:22.848621Z"},"papermill":{"duration":0.022968,"end_time":"2025-11-01T06:40:22.850579","exception":false,"start_time":"2025-11-01T06:40:22.827611","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_19/876045050.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"]}],"source":["# Optimizer + scheduler + mixed precision + clipping\n","# -----------------------------\n","# param groups: smaller LR for backbones, larger for heads\n","backbone_params = []\n","head_params = []\n","for name, param in model.named_parameters():\n","    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n","        backbone_params.append(param)\n","    else:\n","        head_params.append(param)\n","\n","opt = torch.optim.AdamW([\n","    {'params': backbone_params, 'lr': LR * 0.2},\n","    {'params': head_params, 'lr': LR}\n","], lr=LR, weight_decay=1e-4)\n","\n","# warmup + cosine schedule\n","def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n","    def lr_lambda(epoch):\n","        if epoch < warmup_epochs:\n","            return float(epoch) / float(max(1.0, warmup_epochs))\n","        # cosine from warmup -> total\n","        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n","        return 0.5 * (1.0 + math.cos(math.pi * t))\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n","\n","scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","\n","# -----------------------------\n","# Mixup & CutMix helpers\n","# -----------------------------\n","def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)   # use builtin int\n","    cut_h = int(H * cut_rat)   # use builtin int\n","\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n","    lam = np.random.beta(alpha, alpha)\n","    idx = torch.randperm(x.size(0))\n","    mixed_x = lam * x + (1 - lam) * x[idx]\n","    y_a, y_b = y, y[idx]\n","    return mixed_x, y_a, y_b, lam\n","\n","def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n","    lam = np.random.beta(alpha, alpha)\n","    idx = torch.randperm(x.size(0))\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n","    new_x = x.clone()\n","    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n","    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n","    return new_x, y, y[idx], lam_adjusted\n"]},{"cell_type":"markdown","id":"7d673d82","metadata":{"papermill":{"duration":0.005858,"end_time":"2025-11-01T06:40:22.862552","exception":false,"start_time":"2025-11-01T06:40:22.856694","status":"completed"},"tags":[]},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"144e73db","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:22.876041Z","iopub.status.busy":"2025-11-01T06:40:22.875806Z","iopub.status.idle":"2025-11-01T06:40:22.88159Z","shell.execute_reply":"2025-11-01T06:40:22.880868Z"},"papermill":{"duration":0.013913,"end_time":"2025-11-01T06:40:22.882696","exception":false,"start_time":"2025-11-01T06:40:22.868783","status":"completed"},"tags":[]},"outputs":[],"source":["# best_vf1 = 0.0\n","# best_epoch = 0\n","# patience_count = 0\n","\n","# def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n","#     # mix_info: (mode, y_a, y_b, lam) or None\n","#     if mix_info is None:\n","#         if use_focal:\n","#             return clf_loss_focal(logits, targets)\n","#         else:\n","#             return clf_loss_ce(logits, targets)\n","#     else:\n","#         # mixup/cutmix: soft labels\n","#         y_a, y_b, lam = mix_info\n","#         if use_focal:\n","#             # focal is not designed for soft labels; approximate by weighted CE\n","#             loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n","#         else:\n","#             loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n","#         return loss\n","# for epoch in range(1, EPOCHS+1):\n","#     # freeze/unfreeze strategy\n","#     if epoch <= FREEZE_EPOCHS:\n","#         # freeze early layers of backbones\n","#         for name, p in model.named_parameters():\n","#             if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n","#                 p.requires_grad = False\n","#     else:\n","#         for p in model.parameters():\n","#             p.requires_grad = True\n","\n","\n","#     model.train()\n","#     running_loss = 0.0\n","#     y_true, y_pred = [], []\n","#     n_batches = 0\n","\n","#     for weak_imgs, strong_imgs, labels, masks in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n","#         weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n","#         labels = labels.to(device)\n","#         if masks is not None:\n","#             masks = masks.to(device)\n","\n","#         # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n","#         imgs = weak_imgs\n","\n","#         # optionally apply mixup/cutmix on imgs (on weak view)\n","#         mix_info = None\n","#         rand = random.random()\n","#         if rand < PROB_MIXUP:\n","#             imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n","#             mix_info = (y_a.to(device), y_b.to(device), lam)\n","#         elif rand < PROB_MIXUP + PROB_CUTMIX:\n","#             imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n","#             mix_info = (y_a.to(device), y_b.to(device), lam)\n","\n","#         with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","#             out = model(imgs)  # returns logits, feat, seg, domain_logits\n","#             logits = out[\"logits\"]\n","#             feat = out[\"feat\"]\n","#             seg_out = out.get(\"seg\", None)\n","#             domain_logits = out.get(\"domain_logits\", None)\n","\n","#             # classification loss (label-smoothing or focal)\n","#             clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n","\n","#             # segmentation loss if available & mask present\n","#             seg_loss = 0.0\n","#             if USE_SEGMENTATION and (masks is not None):\n","#                 seg_pred = out[\"seg\"]\n","#                 seg_loss = seg_loss_fn(seg_pred, masks)\n","#             # supcon loss on features (use features from weak)\n","#             supcon_loss = supcon_loss_fn(feat, labels)\n","\n","#             # consistency: forward strong view and compare predictions\n","#             out_strong = model(strong_imgs)\n","#             logits_strong = out_strong[\"logits\"]\n","#             probs_weak = F.softmax(logits.detach(), dim=1)\n","#             probs_strong = F.softmax(logits_strong, dim=1)\n","#             # L2 between probability vectors (could be KL)\n","#             cons_loss = F.mse_loss(probs_weak, probs_strong)\n","#             # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n","#             # To support domain adaptation, user should provide target dataloader and stack batches with domain labels\n","#             dom_loss = 0.0\n","#             # (If domain labels are provided, compute dom logits after GRL: domain_logits_grl = domain_head(grad_reverse(feat, l)))\n","#             # then dom_loss = criterion(domain_logits_grl, domain_labels)\n","\n","#             total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n","\n","#         opt.zero_grad()\n","#         scaler.scale(total_loss).backward()\n","#         # gradient clipping\n","#         scaler.unscale_(opt)\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","#         scaler.step(opt)\n","#         scaler.update()\n","\n","#         running_loss += total_loss.item()\n","#         y_true.extend(labels.cpu().numpy())\n","#         y_pred.extend(logits.argmax(1).cpu().numpy())\n","#         n_batches += 1\n","\n","#     scheduler.step()\n","\n","#     # metrics\n","#     acc = accuracy_score(y_true, y_pred)\n","#     prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n","#     print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n","\n","#     # -------------------\n","#     # VALIDATION\n","#     # -------------------\n","#     model.eval()\n","#     val_y_true, val_y_pred = [], []\n","#     val_loss = 0.0\n","#     with torch.no_grad():\n","#         for weak_imgs, _, labels, masks in val_loader:\n","#             imgs = weak_imgs.to(device)\n","#             labels = labels.to(device)\n","#             if masks is not None:\n","#                 masks = masks.to(device)\n","\n","#             out = model(imgs)\n","#             logits = out[\"logits\"]\n","#             feat = out[\"feat\"]\n","#             seg_out = out.get(\"seg\", None)\n","#             loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n","#             if USE_SEGMENTATION and (masks is not None):\n","#                 loss += seg_loss_fn(seg_out, masks)\n","#             val_loss += loss.item()\n","\n","#             val_y_true.extend(labels.cpu().numpy())\n","#             val_y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","#     vacc = accuracy_score(val_y_true, val_y_pred)\n","#     vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n","#     print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n","\n","#     # early stopping & save best\n","#     if vf1 > best_vf1:\n","#         best_vf1 = vf1\n","#         best_epoch = epoch\n","#         torch.save({\n","#             \"epoch\": epoch,\n","#             \"model_state\": model.state_dict(),\n","#             \"opt_state\": opt.state_dict(),\n","#             \"best_vf1\": best_vf1\n","#         }, SAVE_PATH)\n","#         patience_count = 0\n","#         print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n","#     else:\n","#         patience_count += 1\n","#         if patience_count >= EARLY_STOPPING_PATIENCE:\n","#             print(\"Early stopping triggered.\")\n","#             break\n","\n","# print(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)\n"]},{"cell_type":"code","execution_count":27,"id":"1fe872e9","metadata":{"execution":{"iopub.execute_input":"2025-11-01T06:40:22.895946Z","iopub.status.busy":"2025-11-01T06:40:22.89572Z","iopub.status.idle":"2025-11-01T10:27:54.77904Z","shell.execute_reply":"2025-11-01T10:27:54.777858Z"},"papermill":{"duration":13652.818285,"end_time":"2025-11-01T10:27:55.707325","exception":false,"start_time":"2025-11-01T06:40:22.88904","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Train 1/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 1/20: 100%|██████████| 1769/1769 [15:53<00:00,  1.86it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Train Loss: 3.8568 Acc: 0.5082 Prec: 0.5076 Rec: 0.5072 F1: 0.5010\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Val Loss: 2.2284 Acc: 0.4950 Prec: 0.4075 Rec: 0.4934 F1: 0.3409\n","Saved best model at epoch 1 (F1 0.3409)\n"]},{"name":"stderr","output_type":"stream","text":["Train 2/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 2/20: 100%|██████████| 1769/1769 [15:52<00:00,  1.86it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 2] Train Loss: 2.6431 Acc: 0.7221 Prec: 0.7222 Rec: 0.7221 F1: 0.7221\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 2] Val Loss: 1.5021 Acc: 0.9720 Prec: 0.9729 Rec: 0.9721 F1: 0.9720\n","Saved best model at epoch 2 (F1 0.9720)\n"]},{"name":"stderr","output_type":"stream","text":["Train 3/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 3/20: 100%|██████████| 1769/1769 [15:58<00:00,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 3] Train Loss: 2.2958 Acc: 0.7974 Prec: 0.7975 Rec: 0.7973 F1: 0.7973\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 3] Val Loss: 1.4733 Acc: 0.9538 Prec: 0.9559 Rec: 0.9539 F1: 0.9538\n"]},{"name":"stderr","output_type":"stream","text":["Train 4/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 4/20: 100%|██████████| 1769/1769 [15:59<00:00,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 4] Train Loss: 2.1992 Acc: 0.8083 Prec: 0.8084 Rec: 0.8084 F1: 0.8083\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 4] Val Loss: 1.5495 Acc: 0.9720 Prec: 0.9726 Rec: 0.9721 F1: 0.9720\n","Saved best model at epoch 4 (F1 0.9720)\n"]},{"name":"stderr","output_type":"stream","text":["Train 5/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 5/20: 100%|██████████| 1769/1769 [15:57<00:00,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 5] Train Loss: 2.1352 Acc: 0.8212 Prec: 0.8213 Rec: 0.8212 F1: 0.8212\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 5] Val Loss: 1.4716 Acc: 0.9658 Prec: 0.9677 Rec: 0.9659 F1: 0.9658\n"]},{"name":"stderr","output_type":"stream","text":["Train 6/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 6/20: 100%|██████████| 1769/1769 [15:58<00:00,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 6] Train Loss: 2.1409 Acc: 0.8144 Prec: 0.8150 Rec: 0.8142 F1: 0.8142\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 6] Val Loss: 1.4948 Acc: 0.9319 Prec: 0.9394 Rec: 0.9321 F1: 0.9316\n"]},{"name":"stderr","output_type":"stream","text":["Train 7/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 7/20: 100%|██████████| 1769/1769 [16:15<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 7] Train Loss: 2.0790 Acc: 0.8216 Prec: 0.8216 Rec: 0.8215 F1: 0.8215\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 7] Val Loss: 1.4580 Acc: 0.9105 Prec: 0.9213 Rec: 0.9108 F1: 0.9100\n"]},{"name":"stderr","output_type":"stream","text":["Train 8/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 8/20: 100%|██████████| 1769/1769 [16:17<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 8] Train Loss: 2.0169 Acc: 0.8247 Prec: 0.8247 Rec: 0.8247 F1: 0.8247\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 8] Val Loss: 1.5120 Acc: 0.9540 Prec: 0.9574 Rec: 0.9541 F1: 0.9539\n"]},{"name":"stderr","output_type":"stream","text":["Train 9/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 9/20: 100%|██████████| 1769/1769 [16:18<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 9] Train Loss: 1.9449 Acc: 0.8318 Prec: 0.8318 Rec: 0.8318 F1: 0.8318\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 9] Val Loss: 1.4846 Acc: 0.9617 Prec: 0.9636 Rec: 0.9618 F1: 0.9617\n"]},{"name":"stderr","output_type":"stream","text":["Train 10/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 10/20: 100%|██████████| 1769/1769 [16:18<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 10] Train Loss: 1.9343 Acc: 0.8238 Prec: 0.8238 Rec: 0.8238 F1: 0.8238\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 10] Val Loss: 1.5462 Acc: 0.9391 Prec: 0.9452 Rec: 0.9393 F1: 0.9390\n"]},{"name":"stderr","output_type":"stream","text":["Train 11/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 11/20: 100%|██████████| 1769/1769 [16:47<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 11] Train Loss: 1.8545 Acc: 0.8381 Prec: 0.8381 Rec: 0.8381 F1: 0.8381\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 11] Val Loss: 1.5157 Acc: 0.9500 Prec: 0.9524 Rec: 0.9502 F1: 0.9500\n"]},{"name":"stderr","output_type":"stream","text":["Train 12/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 12/20: 100%|██████████| 1769/1769 [16:45<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 12] Train Loss: 1.8313 Acc: 0.8351 Prec: 0.8351 Rec: 0.8351 F1: 0.8351\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 12] Val Loss: 1.4764 Acc: 0.9646 Prec: 0.9668 Rec: 0.9647 F1: 0.9645\n","Early stopping triggered.\n","Training finished. Best val F1: 0.9719888648569479 at epoch 4\n"]}],"source":["best_vf1 = 0.0\n","best_epoch = 0\n","patience_count = 0\n","\n","def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n","    # mix_info: (mode, y_a, y_b, lam) or None\n","    if mix_info is None:\n","        if use_focal:\n","            return clf_loss_focal(logits, targets)\n","        else:\n","            return clf_loss_ce(logits, targets)\n","    else:\n","        # mixup/cutmix: soft labels\n","        y_a, y_b, lam = mix_info\n","        if use_focal:\n","            # focal is not designed for soft labels; approximate by weighted CE\n","            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n","        else:\n","            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n","        return loss\n","\n","# --- Ensure ACCUMULATION_STEPS is defined (e.g., in Code Cell 5) ---\n","# For this code to run, you MUST have ACCUMULATION_STEPS defined globally (e.g., set to 4 in Cell 5)\n","# -------------------------------------------------------------------\n","\n","for epoch in range(1, EPOCHS+1):\n","    # freeze/unfreeze strategy\n","    if epoch <= FREEZE_EPOCHS:\n","        # freeze early layers of backbones\n","        for name, p in model.named_parameters():\n","            if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n","                p.requires_grad = False\n","    else:\n","        for p in model.parameters():\n","            p.requires_grad = True\n","\n","    model.train()\n","    running_loss = 0.0\n","    y_true, y_pred = [], []\n","    n_batches = 0\n","\n","    # 1. Initialize zero_grad at the start of the epoch\n","    opt.zero_grad() \n","    \n","    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n","        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n","        labels = labels.to(device)\n","        if masks is not None:\n","            masks = masks.to(device)\n","\n","        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n","        imgs = weak_imgs\n","\n","        # optionally apply mixup/cutmix on imgs (on weak view)\n","        mix_info = None\n","        rand = random.random()\n","        if rand < PROB_MIXUP:\n","            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n","            mix_info = (y_a.to(device), y_b.to(device), lam)\n","        elif rand < PROB_MIXUP + PROB_CUTMIX:\n","            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n","            mix_info = (y_a.to(device), y_b.to(device), lam)\n","\n","        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","            out = model(imgs)  # returns logits, feat, seg, domain_logits\n","            logits = out[\"logits\"]\n","            feat = out[\"feat\"]\n","            seg_out = out.get(\"seg\", None)\n","            domain_logits = out.get(\"domain_logits\", None)\n","\n","            # classification loss (label-smoothing or focal)\n","            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n","\n","            # segmentation loss if available & mask present\n","            seg_loss = 0.0\n","            if USE_SEGMENTATION and (masks is not None):\n","                seg_pred = out[\"seg\"]\n","                seg_loss = seg_loss_fn(seg_pred, masks)\n","            # supcon loss on features (use features from weak)\n","            supcon_loss = supcon_loss_fn(feat, labels)\n","\n","            # consistency: forward strong view and compare predictions\n","            out_strong = model(strong_imgs)\n","            logits_strong = out_strong[\"logits\"]\n","            probs_weak = F.softmax(logits.detach(), dim=1)\n","            probs_strong = F.softmax(logits_strong, dim=1)\n","            # L2 between probability vectors (could be KL)\n","            cons_loss = F.mse_loss(probs_weak, probs_strong)\n","            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n","            dom_loss = 0.0\n","\n","            total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n","\n","            # 2. Scale the loss by accumulation steps to average the gradients\n","            total_loss = total_loss / ACCUMULATION_STEPS \n","\n","        # Perform backward pass (gradients are accumulated until step is called)\n","        scaler.scale(total_loss).backward()\n","\n","        # 3. Optimizer step only every ACCUMULATION_STEPS batches\n","        if (i + 1) % ACCUMULATION_STEPS == 0:\n","            # gradient clipping before step\n","            scaler.unscale_(opt)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            scaler.step(opt)\n","            scaler.update()\n","            opt.zero_grad() # Prepare for next accumulation cycle\n","\n","        running_loss += total_loss.item() * ACCUMULATION_STEPS # Re-scale back for correct loss tracking\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(logits.argmax(1).cpu().numpy())\n","        n_batches += 1\n","\n","    # 4. Take a final step if there are remaining gradients (i.e., last batch was not a multiple of ACCUMULATION_STEPS)\n","    if n_batches % ACCUMULATION_STEPS != 0:\n","        scaler.unscale_(opt)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        scaler.step(opt)\n","        scaler.update()\n","        opt.zero_grad()\n","\n","    scheduler.step()\n","\n","    # metrics (rest of the code remains the same)\n","    acc = accuracy_score(y_true, y_pred)\n","    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n","    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n","\n","    # -------------------\n","    # VALIDATION\n","    # -------------------\n","    model.eval()\n","    val_y_true, val_y_pred = [], []\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for weak_imgs, _, labels, masks in val_loader:\n","            imgs = weak_imgs.to(device)\n","            labels = labels.to(device)\n","            if masks is not None:\n","                masks = masks.to(device)\n","\n","            out = model(imgs)\n","            logits = out[\"logits\"]\n","            feat = out[\"feat\"]\n","            seg_out = out.get(\"seg\", None)\n","            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n","            if USE_SEGMENTATION and (masks is not None):\n","                loss += seg_loss_fn(seg_out, masks)\n","            val_loss += loss.item()\n","\n","            val_y_true.extend(labels.cpu().numpy())\n","            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","    vacc = accuracy_score(val_y_true, val_y_pred)\n","    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n","    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n","\n","    # early stopping & save best\n","    if vf1 > best_vf1:\n","        best_vf1 = vf1\n","        best_epoch = epoch\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"model_state\": model.state_dict(),\n","            \"opt_state\": opt.state_dict(),\n","            \"best_vf1\": best_vf1\n","        }, SAVE_PATH)\n","        patience_count = 0\n","        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n","    else:\n","        patience_count += 1\n","        if patience_count >= EARLY_STOPPING_PATIENCE:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","print(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"]},{"cell_type":"code","execution_count":28,"id":"4bf81672","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:27:57.592307Z","iopub.status.busy":"2025-11-01T10:27:57.592012Z","iopub.status.idle":"2025-11-01T10:27:57.598377Z","shell.execute_reply":"2025-11-01T10:27:57.597773Z"},"papermill":{"duration":0.944902,"end_time":"2025-11-01T10:27:57.599441","exception":false,"start_time":"2025-11-01T10:27:56.654539","status":"completed"},"tags":[]},"outputs":[],"source":["# Test-time augmentation (TTA) helper\n","# -----------------------------\n","def tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n","    model.eval()\n","    logits_accum = None\n","    with torch.no_grad():\n","        for s in scales:\n","            tf = transforms.Compose([\n","                transforms.Resize((s, s)),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ])\n","            x = tf(img_pil).unsqueeze(0).to(device)\n","            out = model(x)\n","            logits = out[\"logits\"]\n","            if flip:\n","                x_f = torch.flip(x, dims=[3])\n","                logits_f = model(x_f)[\"logits\"]\n","                logits = (logits + logits_f) / 2.0\n","            if logits_accum is None:\n","                logits_accum = logits\n","            else:\n","                logits_accum += logits\n","    logits_accum /= len(scales)\n","    return logits_accum"]},{"cell_type":"code","execution_count":29,"id":"29d19c80","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:27:59.499377Z","iopub.status.busy":"2025-11-01T10:27:59.499061Z","iopub.status.idle":"2025-11-01T10:27:59.508373Z","shell.execute_reply":"2025-11-01T10:27:59.507539Z"},"papermill":{"duration":1.063046,"end_time":"2025-11-01T10:27:59.509716","exception":false,"start_time":"2025-11-01T10:27:58.44667","status":"completed"},"tags":[]},"outputs":[],"source":["# Grad-CAM helper (very simple)\n","# -----------------------------\n","def get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n","    \"\"\"\n","    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n","    Returns upsampled heatmap (H,W) normalized in [0,1].\n","    \"\"\"\n","    model.eval()\n","    # find layer\n","    target_module = None\n","    for name, module in model.named_modules():\n","        if name == layer_name:\n","            target_module = module\n","            break\n","    if target_module is None:\n","        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n","\n","    activations = []\n","    gradients = []\n","\n","    def forward_hook(module, input, output):\n","        activations.append(output.detach())\n","    def backward_hook(module, grad_in, grad_out):\n","        gradients.append(grad_out[0].detach())\n","\n","    h1 = target_module.register_forward_hook(forward_hook)\n","    h2 = target_module.register_full_backward_hook(backward_hook)\n","\n","    out = model(input_tensor)\n","    logits = out[\"logits\"]\n","    if target_class is None:\n","        target_class = logits.argmax(1).item()\n","    loss = logits[:, target_class].sum()\n","    model.zero_grad()\n","    loss.backward(retain_graph=True)\n","\n","    act = activations[0]  # [B,C,H,W]\n","    grad = gradients[0]   # [B,C,H,W]\n","    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n","    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n","    cam = F.relu(cam)\n","    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n","    cam = cam.squeeze().cpu().numpy()\n","    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n","    h1.remove(); h2.remove()\n","    return cam"]},{"cell_type":"code","execution_count":null,"id":"315dfb8f","metadata":{"papermill":{"duration":0.950856,"end_time":"2025-11-01T10:28:01.358189","exception":false,"start_time":"2025-11-01T10:28:00.407333","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2932761,"sourceId":5051281,"sourceType":"datasetVersion"},{"datasetId":8580489,"sourceId":13514489,"sourceType":"datasetVersion"},{"datasetId":8582404,"sourceId":13517101,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":13747.151727,"end_time":"2025-11-01T10:28:04.97079","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-01T06:38:57.819063","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"03b07d23e4ad45bfbc20d5e39cb450e9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"091187b7110943b8b818fd55c389f321":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6734ef6c8ae4aa5b051151b3bfcabd4","IPY_MODEL_a21e98e612b74022b280c52174f2f37d","IPY_MODEL_20e209c346fb44b392a2c825fc3ad709"],"layout":"IPY_MODEL_94043ab0db23404db2dc37afbe6d97cd","tabbable":null,"tooltip":null}},"0c398a0690ee4c979be3df97c4494df5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20e209c346fb44b392a2c825fc3ad709":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f7c305e454e34b52825ac690d04d4b1a","placeholder":"​","style":"IPY_MODEL_2450a34834b647959a98f650442475ef","tabbable":null,"tooltip":null,"value":" 114M/114M [00:01&lt;00:00, 106MB/s]"}},"2450a34834b647959a98f650442475ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"560e91d011f042628e904b767c9ee604":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6bb756829d9640ba81dc5f8ef44c329e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"94043ab0db23404db2dc37afbe6d97cd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a21e98e612b74022b280c52174f2f37d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_0c398a0690ee4c979be3df97c4494df5","max":114286722.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_6bb756829d9640ba81dc5f8ef44c329e","tabbable":null,"tooltip":null,"value":114286722.0}},"c6734ef6c8ae4aa5b051151b3bfcabd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_03b07d23e4ad45bfbc20d5e39cb450e9","placeholder":"​","style":"IPY_MODEL_560e91d011f042628e904b767c9ee604","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"f7c305e454e34b52825ac690d04d4b1a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}