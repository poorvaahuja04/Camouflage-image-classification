{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761},{"sourceId":13517101,"sourceType":"datasetVersion","datasetId":8582404},{"sourceId":13514489,"sourceType":"datasetVersion","datasetId":8580489}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:36.860758Z","iopub.execute_input":"2025-10-30T06:15:36.861049Z","iopub.status.idle":"2025-10-30T06:15:37.382384Z","shell.execute_reply.started":"2025-10-30T06:15:36.860995Z","shell.execute_reply":"2025-10-30T06:15:37.381614Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:37.383575Z","iopub.execute_input":"2025-10-30T06:15:37.384214Z","iopub.status.idle":"2025-10-30T06:15:48.652584Z","shell.execute_reply.started":"2025-10-30T06:15:37.384188Z","shell.execute_reply":"2025-10-30T06:15:48.651837Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:48.653467Z","iopub.execute_input":"2025-10-30T06:15:48.653684Z","iopub.status.idle":"2025-10-30T06:15:48.657315Z","shell.execute_reply.started":"2025-10-30T06:15:48.653668Z","shell.execute_reply":"2025-10-30T06:15:48.656716Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:48.658716Z","iopub.execute_input":"2025-10-30T06:15:48.658916Z","iopub.status.idle":"2025-10-30T06:15:48.763664Z","shell.execute_reply.started":"2025-10-30T06:15:48.658901Z","shell.execute_reply":"2025-10-30T06:15:48.762962Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 16          # adjust if OOM\nEPOCHS = 12\nNUM_WORKERS = 4         # set 0 if worker issues on Kaggle\nLR = 3e-4\nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True\n\n# Loss weights from PDF suggestion\nALPHA_DOM = 0.5\nBETA_SUPCON = 0.2\nETA_CONS = 0.1\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# warmup epochs\nWARMUP_EPOCHS = 5\n\n# early stopping\nEARLY_STOPPING_PATIENCE = 8\nFREEZE_EPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:48.764474Z","iopub.execute_input":"2025-10-30T06:15:48.764947Z","iopub.status.idle":"2025-10-30T06:15:48.769408Z","shell.execute_reply.started":"2025-10-30T06:15:48.764919Z","shell.execute_reply":"2025-10-30T06:15:48.768706Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Noise + Transform","metadata":{}},{"cell_type":"code","source":"class AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:48.770120Z","iopub.execute_input":"2025-10-30T06:15:48.770292Z","iopub.status.idle":"2025-10-30T06:15:48.784674Z","shell.execute_reply.started":"2025-10-30T06:15:48.770277Z","shell.execute_reply":"2025-10-30T06:15:48.784043Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"weak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\nstrong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\nval_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\n# reading a file with multiple encodings\ndef read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n    for encoding in encodings:\n        try:\n            with open(file_path, 'r', encoding=encoding) as f:\n                return f.readlines()\n        except UnicodeDecodeError:\n            print(f\"Failed to read {file_path} with encoding {encoding}\")\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n\ndef load_testing_dataset_info(info_file, image_dir):\n    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n    image_paths = []\n    labels = []\n    \n    # List of encodings to try\n    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n    lines = []\n    \n    for encoding in encodings:\n        try:\n            with open(info_file, 'r', encoding=encoding) as f:\n                lines = f.readlines()\n            break  # If successful, break out of the loop\n        except UnicodeDecodeError:\n            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n        except Exception as e:\n            print(f\"Error reading {info_file}: {e}\")\n            raise \n\n    for line in lines:\n        parts = line.strip().split()\n        if len(parts) == 2:\n            image_filename = parts[0]\n            # Ensure label is always 0 or 1\n            try:\n                label = int(parts[1])\n            except ValueError:\n                continue # Skip malformed lines\n                \n            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n            image_paths.append(image_full_path)\n            labels.append(label)\n    \n    return image_paths, labels\n\n\n# MultiDataset class with proper encoding handling\nclass MultiDataset(Dataset):\n    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dirs = root_dirs\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        # Handle the testing dataset (testing-dataset images and labels)\n        if testing_image_paths is not None and testing_labels is not None:\n            for img_path, label in zip(testing_image_paths, testing_labels):\n                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n                self.samples.append((img_path, label)) \n        \n        # Process other datasets (COD10K, CAMO, etc.)\n        if isinstance(txt_files, str):\n            txt_files = [txt_files]\n\n        all_lines = []\n        for t in txt_files:\n            if not os.path.exists(t):\n                raise RuntimeError(f\"TXT file not found: {t}\")\n\n            # Use the read_file_with_encoding function to handle different encodings\n            lines = read_file_with_encoding(t)\n            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n\n        for line, src_txt in all_lines:\n            parts = line.split()\n            if len(parts) == 0:\n                continue\n\n            fname = parts[0]\n            if len(parts) >= 2:\n                try:\n                    lbl = int(parts[1])\n                except:\n                    # Fallback classification if label is not an integer\n                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            else:\n                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n            \n            # Map labels to binary (CAM=1, Non-CAM=0)\n            lbl = 1 if lbl == 1 else 0\n\n            found = False\n            search_subs = [\n                \"\",  # If image is directly in root_dir (less common)\n                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n            ]\n            \n            base_fname = os.path.basename(fname)  \n\n            for rdir in self.root_dirs:\n                for sub in search_subs:\n                    img_path = os.path.join(rdir, sub, base_fname)\n                    if os.path.exists(img_path):\n                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n                        self.samples.append((img_path, lbl, rdir))\n                        found = True\n                        break\n                if found:\n                    break\n\n            if not found:\n                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n\n        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        global IMG_SIZE \n        \n        sample = self.samples[idx]\n        \n        img_path = sample[0]\n        lbl = sample[1]\n        \n        # Define rdir for consistent mask lookup logic\n        if len(sample) == 3:\n            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n            rdir = sample[2]\n            # Root directory for testing-dataset (used as fallback for mask lookup)\n            testing_root = None \n        else:\n            rdir = os.path.dirname(os.path.dirname(img_path))\n            # Set testing_root explicitly for clarity if needed later, but rdir is now defined.\n\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n            \n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n            \n            # Use the defined rdir for mask search\n            found_mask = False\n            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n                mask_path = os.path.join(rdir, mask_dir, mask_name)\n                \n                if os.path.exists(mask_path):\n                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                    m = np.array(m).astype(np.float32) / 255.0\n                    # Convert to binary mask: 0 or 1\n                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n                    found_mask = True\n                    break\n\n            if mask is None:\n                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n                \n        return weak, strong, lbl, mask\n\ndef build_weighted_sampler(dataset):\n    \"\"\"\n    Builds a WeightedRandomSampler based on class imbalance.\n    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n    \"\"\"\n    \n    # Safely extract labels (always index 1, regardless of tuple length)\n    labels = [sample[1] for sample in dataset.samples]  \n    \n    counts = Counter(labels)\n    total = len(labels)\n    \n    # Ensure there are at least two classes to calculate class_weights\n    if len(counts) <= 1:\n        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n        weights = [1.0] * total\n    else:\n        # Calculate inverse frequency weights\n        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n        weights = [class_weights[lbl] for lbl in labels]\n        \n    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:48.785491Z","iopub.execute_input":"2025-10-30T06:15:48.785732Z","iopub.status.idle":"2025-10-30T06:15:48.806621Z","shell.execute_reply.started":"2025-10-30T06:15:48.785709Z","shell.execute_reply":"2025-10-30T06:15:48.806059Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \ntest_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n    \n# COD10K Info files\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\n# CAMO-COCO PATHS\ninfo_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n\n# CAMO-COCO Info files\ntrain_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\ntrain_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\ntest_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\ntest_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n\n# CAMO-COCO Root Directories\ntrain_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\ntrain_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n\n# third dataset - NC4K + non-camouflage (places 365)\ntesting_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\ntesting_images_dir = \"/kaggle/input/testing-dataset/Images\"\ntesting_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n\n# Split testing-dataset: 20% train (train_paths), 80% validation (val_paths)\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    testing_image_paths, testing_labels, test_size=0.8, random_state=42\n)\n\n# 1. All Root Directories\nALL_ROOT_DIRS = [\n    train_dir_cod,       \n    test_dir_cod,       \n    train_dir_camo_cam,  \n    train_dir_camo_noncam\n]\n\n# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\nALL_TRAIN_TXTS = [\n    train_cam_txt, train_noncam_txt, test_cam_txt, test_noncam_txt,\n    train_cam_txt2, train_noncam_txt2, test_cam_txt2, test_noncam_txt2,\n]\n\n# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\nALL_VAL_TXTS = []\n\n# --- Create the final unified datasets ---\n\n# Training Dataset: All external data (via ALL_TRAIN_TXTS) + 20% testing-dataset split (via train_paths)\ntrain_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS, \n    txt_files=ALL_TRAIN_TXTS,               \n    testing_image_paths=train_paths,        \n    testing_labels=train_labels,            \n    weak_transform=weak_tf, \n    strong_transform=strong_tf, \n    use_masks=USE_SEGMENTATION\n)\n\n# Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\nval_ds = MultiDataset(\n    root_dirs=ALL_ROOT_DIRS,  \n    txt_files=ALL_VAL_TXTS,                 \n    testing_image_paths=val_paths,          \n    testing_labels=val_labels,              \n    weak_transform=val_tf, \n    strong_transform=None, \n    use_masks=USE_SEGMENTATION\n)\n\n# Build Sampler and DataLoader\ntrain_sampler = build_weighted_sampler(train_ds)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nprint(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:15:48.807258Z","iopub.execute_input":"2025-10-30T06:15:48.807506Z","iopub.status.idle":"2025-10-30T06:16:34.721890Z","shell.execute_reply.started":"2025-10-30T06:15:48.807482Z","shell.execute_reply":"2025-10-30T06:16:34.721264Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded 14150 samples from 4 root directories.\n✅ Loaded 6606 samples from 4 root directories.\nTotal Train samples: 14150 Total Val samples: 6606\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Backbones","metadata":{}},{"cell_type":"code","source":"class DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.722589Z","iopub.execute_input":"2025-10-30T06:16:34.722848Z","iopub.status.idle":"2025-10-30T06:16:34.728769Z","shell.execute_reply.started":"2025-10-30T06:16:34.722830Z","shell.execute_reply":"2025-10-30T06:16:34.728157Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class SwinExtractor(nn.Module):\n    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.730870Z","iopub.execute_input":"2025-10-30T06:16:34.731107Z","iopub.status.idle":"2025-10-30T06:16:34.742416Z","shell.execute_reply.started":"2025-10-30T06:16:34.731085Z","shell.execute_reply":"2025-10-30T06:16:34.741659Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction,4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.743020Z","iopub.execute_input":"2025-10-30T06:16:34.743253Z","iopub.status.idle":"2025-10-30T06:16:34.758738Z","shell.execute_reply.started":"2025-10-30T06:16:34.743238Z","shell.execute_reply":"2025-10-30T06:16:34.758044Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        if H.shape[2:] != X.shape[2:]:\n            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n        g = self.g_fc(H)\n        return g * H + (1 - g) * X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.759466Z","iopub.execute_input":"2025-10-30T06:16:34.759686Z","iopub.status.idle":"2025-10-30T06:16:34.772482Z","shell.execute_reply.started":"2025-10-30T06:16:34.759671Z","shell.execute_reply":"2025-10-30T06:16:34.771727Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, d_cnn, d_swin, d_out):\n        super().__init__()\n        self.q = nn.Linear(d_cnn, d_out)\n        self.k = nn.Linear(d_swin, d_out)\n        self.v = nn.Linear(d_swin, d_out)\n        self.scale = d_out ** -0.5\n    def forward(self, feat_cnn, feat_swin):\n        B, Cc, H, W = feat_cnn.shape\n        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n        if feat_swin.dim() == 4:\n            Bs, Cs, Hs, Ws = feat_swin.shape\n            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n        else:\n            kv = feat_swin\n        K = self.k(kv)\n        V = self.v(kv)\n        Q = self.q(q)\n        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, V)\n        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.773058Z","iopub.execute_input":"2025-10-30T06:16:34.773279Z","iopub.status.idle":"2025-10-30T06:16:34.788225Z","shell.execute_reply.started":"2025-10-30T06:16:34.773264Z","shell.execute_reply":"2025-10-30T06:16:34.787612Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Segmentation Decoder","metadata":{}},{"cell_type":"code","source":"class SegDecoder(nn.Module):\n    def __init__(self, in_channels_list, mid_channels=128):\n        super().__init__()\n        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n        self.out = nn.Conv2d(mid_channels, 1, 1)\n    def forward(self, feat_list):\n        target_size = feat_list[0].shape[2:]\n        ups = []\n        for f, p in zip(feat_list, self.projs):\n            x = p(f)\n            if x.shape[2:] != target_size:\n                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n            ups.append(x)\n        x = torch.cat(ups, dim=1)\n        x = self.conv(x)\n        x = self.out(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.788797Z","iopub.execute_input":"2025-10-30T06:16:34.788954Z","iopub.status.idle":"2025-10-30T06:16:34.807244Z","shell.execute_reply.started":"2025-10-30T06:16:34.788941Z","shell.execute_reply":"2025-10-30T06:16:34.806704Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Probing Backbones","metadata":{}},{"cell_type":"code","source":"dnet = DenseNetExtractor().to(device).eval()\nmnet = MobileNetExtractor().to(device).eval()\nsnet = SwinExtractor().to(device).eval()\nwith torch.no_grad():\n    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n    featsA = dnet(dummy)\n    featsB = mnet(dummy)\n    featsS = snet(dummy)\nchA = [f.shape[1] for f in featsA]\nchB = [f.shape[1] for f in featsB]\nchS = [f.shape[1] for f in featsS]\nprint(\"DenseNet channels:\", chA)\nprint(\"MobileNet channels:\", chB)\nprint(\"Swin channels:\", chS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:34.807863Z","iopub.execute_input":"2025-10-30T06:16:34.808157Z","iopub.status.idle":"2025-10-30T06:16:39.533177Z","shell.execute_reply.started":"2025-10-30T06:16:34.808137Z","shell.execute_reply":"2025-10-30T06:16:39.532407Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n100%|██████████| 77.4M/77.4M [00:00<00:00, 201MB/s] \n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 138MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed6b98c2d7a4b83ad5e151d9d6e0a17"}},"metadata":{}},{"name":"stdout","text":"DenseNet channels: [256, 512, 1792, 1920]\nMobileNet channels: [24, 40, 80, 112]\nSwin channels: [56, 28, 14, 7]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Fusion Model (DenseNet + MobileNet + Swin cross attention)","metadata":{}},{"cell_type":"code","source":"class FusionWithSwin(nn.Module):\n    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n        super().__init__()\n        self.backA = DenseNetExtractor()\n        self.backB = MobileNetExtractor()\n        self.backS = SwinExtractor()\n        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n        self.L = L\n        self.d = d\n        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n        self.reduce = nn.Conv2d(d * L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n        self.use_seg = use_seg\n        if self.use_seg:\n            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n\n        # Domain head for DANN (simple MLP)\n        self.domain_head = nn.Sequential(\n            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, x, grl_lambda=0.0):\n        fa = self.backA(x)\n        fb = self.backB(x)\n        fs = self.backS(x)\n        fused_feats = []\n        aligned_for_dec = []\n        for i in range(self.L):\n            a = self.alignA[i](fa[i])\n            a = self.cbamA[i](a)\n            b = self.alignB[i](fb[i])\n            b = self.cbamB[i](b)\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused = self.gates[i](a, b)\n            swin_feat = fs[i]\n            swin_att = self.cross_atts[i](fused, swin_feat)\n            if swin_att.shape[2:] != fused.shape[2:]:\n                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n            fused = fused + swin_att\n            fused_feats.append(fused)\n            aligned_for_dec.append(fused)\n        target = fused_feats[-1]\n        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.reduce(concat)\n        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n        logits = self.classifier(z)\n        out = {\"logits\": logits, \"feat\": z}\n        if self.use_seg:\n            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n\n        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n        if grl_lambda > 0.0:\n            # GRL implemented outside (we'll pass z through GRL function)\n            pass\n        out[\"domain_logits\"] = self.domain_head(z)\n        return out\n\n# instantiate model\nmodel = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\nprint(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:39.534025Z","iopub.execute_input":"2025-10-30T06:16:39.534311Z","iopub.status.idle":"2025-10-30T06:16:40.786375Z","shell.execute_reply.started":"2025-10-30T06:16:39.534286Z","shell.execute_reply":"2025-10-30T06:16:40.785469Z"}},"outputs":[{"name":"stdout","text":"Model parameters (M): 51.586615\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss_logits(pred_logits, target):\n    pred = torch.sigmoid(pred_logits)\n    target = target.float()\n    inter = (pred * target).sum(dim=(1,2,3))\n    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n    return 1.0 - dice.mean()\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nseg_bce = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:40.787434Z","iopub.execute_input":"2025-10-30T06:16:40.787784Z","iopub.status.idle":"2025-10-30T06:16:40.800132Z","shell.execute_reply.started":"2025-10-30T06:16:40.787753Z","shell.execute_reply":"2025-10-30T06:16:40.799528Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#Supervised contrastive Loss\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        # features: [N, D], labels: [N]\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        # remove diagonal\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        # for each i, positive samples are where mask==1 (excluding self)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        # avoid divide by zero\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        # average only across anchors that have positives\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\nsupcon_loss_fn = SupConLoss(temperature=0.07)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:40.800988Z","iopub.execute_input":"2025-10-30T06:16:40.801478Z","iopub.status.idle":"2025-10-30T06:16:40.820496Z","shell.execute_reply.started":"2025-10-30T06:16:40.801459Z","shell.execute_reply":"2025-10-30T06:16:40.819944Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Domain Adversarial: Gradient Reversal Layer (GRL)\n\nfrom torch.autograd import Function\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:40.821198Z","iopub.execute_input":"2025-10-30T06:16:40.821490Z","iopub.status.idle":"2025-10-30T06:16:40.838796Z","shell.execute_reply.started":"2025-10-30T06:16:40.821473Z","shell.execute_reply":"2025-10-30T06:16:40.838115Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Optimizer + scheduler + mixed precision + clipping\n# -----------------------------\n# param groups: smaller LR for backbones, larger for heads\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2},\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# warmup + cosine schedule\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        # cosine from warmup -> total\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# -----------------------------\n# Mixup & CutMix helpers\n# -----------------------------\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)   # use builtin int\n    cut_h = int(H * cut_rat)   # use builtin int\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:40.839444Z","iopub.execute_input":"2025-10-30T06:16:40.839679Z","iopub.status.idle":"2025-10-30T06:16:40.859965Z","shell.execute_reply.started":"2025-10-30T06:16:40.839659Z","shell.execute_reply":"2025-10-30T06:16:40.859269Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/876045050.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"best_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    # mix_info: (mode, y_a, y_b, lam) or None\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        # mixup/cutmix: soft labels\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            # focal is not designed for soft labels; approximate by weighted CE\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\nfor epoch in range(1, EPOCHS+1):\n    # freeze/unfreeze strategy\n    if epoch <= FREEZE_EPOCHS:\n        # freeze early layers of backbones\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n                p.requires_grad = False\n    else:\n        for p in model.parameters():\n            p.requires_grad = True\n\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    for weak_imgs, strong_imgs, labels, masks in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n        imgs = weak_imgs\n\n        # optionally apply mixup/cutmix on imgs (on weak view)\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n            out = model(imgs)  # returns logits, feat, seg, domain_logits\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            domain_logits = out.get(\"domain_logits\", None)\n\n            # classification loss (label-smoothing or focal)\n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # segmentation loss if available & mask present\n            seg_loss = 0.0\n            if USE_SEGMENTATION and (masks is not None):\n                seg_pred = out[\"seg\"]\n                seg_loss = seg_loss_fn(seg_pred, masks)\n            # supcon loss on features (use features from weak)\n            supcon_loss = supcon_loss_fn(feat, labels)\n\n            # consistency: forward strong view and compare predictions\n            out_strong = model(strong_imgs)\n            logits_strong = out_strong[\"logits\"]\n            probs_weak = F.softmax(logits.detach(), dim=1)\n            probs_strong = F.softmax(logits_strong, dim=1)\n            # L2 between probability vectors (could be KL)\n            cons_loss = F.mse_loss(probs_weak, probs_strong)\n            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n            # To support domain adaptation, user should provide target dataloader and stack batches with domain labels\n            dom_loss = 0.0\n            # (If domain labels are provided, compute dom logits after GRL: domain_logits_grl = domain_head(grad_reverse(feat, l)))\n            # then dom_loss = criterion(domain_logits_grl, domain_labels)\n\n            total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n\n        opt.zero_grad()\n        scaler.scale(total_loss).backward()\n        # gradient clipping\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n\n        running_loss += total_loss.item()\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    scheduler.step()\n\n    # metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += seg_loss_fn(seg_out, masks)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    # early stopping & save best\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T06:16:40.860719Z","iopub.execute_input":"2025-10-30T06:16:40.860897Z","iopub.status.idle":"2025-10-30T09:06:47.480363Z","shell.execute_reply.started":"2025-10-30T06:16:40.860883Z","shell.execute_reply":"2025-10-30T09:06:47.478917Z"}},"outputs":[{"name":"stderr","text":"Train 1/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 1/12: 100%|██████████| 885/885 [13:41<00:00,  1.08it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 4.8975 Acc: 0.4931 Prec: 0.4917 Rec: 0.4921 F1: 0.4856\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Val Loss: 2.2240 Acc: 0.4958 Prec: 0.4131 Rec: 0.4942 F1: 0.3410\nSaved best model at epoch 1 (F1 0.3410)\n","output_type":"stream"},{"name":"stderr","text":"Train 2/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 2/12: 100%|██████████| 885/885 [13:55<00:00,  1.06it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Train Loss: 3.5350 Acc: 0.7080 Prec: 0.7084 Rec: 0.7081 F1: 0.7079\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Val Loss: 1.5001 Acc: 0.9458 Prec: 0.9503 Rec: 0.9460 F1: 0.9457\nSaved best model at epoch 2 (F1 0.9457)\n","output_type":"stream"},{"name":"stderr","text":"Train 3/12: 100%|██████████| 885/885 [14:25<00:00,  1.02it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Train Loss: 2.9845 Acc: 0.7946 Prec: 0.7947 Rec: 0.7946 F1: 0.7946\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Val Loss: 1.5077 Acc: 0.9072 Prec: 0.9212 Rec: 0.9075 F1: 0.9065\n","output_type":"stream"},{"name":"stderr","text":"Train 4/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 4/12: 100%|██████████| 885/885 [13:57<00:00,  1.06it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Train Loss: 2.8554 Acc: 0.7948 Prec: 0.7949 Rec: 0.7948 F1: 0.7948\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Val Loss: 1.5347 Acc: 0.9479 Prec: 0.9524 Rec: 0.9481 F1: 0.9478\nSaved best model at epoch 4 (F1 0.9478)\n","output_type":"stream"},{"name":"stderr","text":"Train 5/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 5/12: 100%|██████████| 885/885 [13:56<00:00,  1.06it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Train Loss: 2.6893 Acc: 0.8073 Prec: 0.8074 Rec: 0.8073 F1: 0.8073\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Val Loss: 1.4682 Acc: 0.9619 Prec: 0.9642 Rec: 0.9620 F1: 0.9618\nSaved best model at epoch 5 (F1 0.9618)\n","output_type":"stream"},{"name":"stderr","text":"Train 6/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 6/12: 100%|██████████| 885/885 [14:07<00:00,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Train Loss: 2.6857 Acc: 0.8053 Prec: 0.8055 Rec: 0.8052 F1: 0.8052\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Val Loss: 1.4881 Acc: 0.9575 Prec: 0.9604 Rec: 0.9576 F1: 0.9574\n","output_type":"stream"},{"name":"stderr","text":"Train 7/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 7/12: 100%|██████████| 885/885 [14:10<00:00,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Train Loss: 2.6007 Acc: 0.8110 Prec: 0.8112 Rec: 0.8110 F1: 0.8110\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Val Loss: 1.4956 Acc: 0.9463 Prec: 0.9511 Rec: 0.9464 F1: 0.9461\n","output_type":"stream"},{"name":"stderr","text":"Train 8/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 8/12: 100%|██████████| 885/885 [13:57<00:00,  1.06it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Train Loss: 2.4487 Acc: 0.8203 Prec: 0.8203 Rec: 0.8203 F1: 0.8203\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Val Loss: 1.4863 Acc: 0.9631 Prec: 0.9648 Rec: 0.9632 F1: 0.9630\nSaved best model at epoch 8 (F1 0.9630)\n","output_type":"stream"},{"name":"stderr","text":"Train 9/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 9/12: 100%|██████████| 885/885 [13:57<00:00,  1.06it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Train Loss: 2.3642 Acc: 0.8253 Prec: 0.8253 Rec: 0.8253 F1: 0.8253\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Val Loss: 1.4390 Acc: 0.9728 Prec: 0.9735 Rec: 0.9728 F1: 0.9727\nSaved best model at epoch 9 (F1 0.9727)\n","output_type":"stream"},{"name":"stderr","text":"Train 10/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 10/12: 100%|██████████| 885/885 [14:28<00:00,  1.02it/s]","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Train Loss: 2.2097 Acc: 0.8344 Prec: 0.8344 Rec: 0.8343 F1: 0.8344\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Val Loss: 1.5017 Acc: 0.9605 Prec: 0.9631 Rec: 0.9606 F1: 0.9604\n","output_type":"stream"},{"name":"stderr","text":"Train 11/12:   0%|          | 0/885 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 11/12:   0%|          | 0/885 [00:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1965969712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m# gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.01 GiB. GPU 0 has a total capacity of 14.74 GiB of which 122.12 MiB is free. Process 2848 has 14.62 GiB memory in use. Of the allocated memory 12.45 GiB is allocated by PyTorch, and 2.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.01 GiB. GPU 0 has a total capacity of 14.74 GiB of which 122.12 MiB is free. Process 2848 has 14.62 GiB memory in use. Of the allocated memory 12.45 GiB is allocated by PyTorch, and 2.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"# Test-time augmentation (TTA) helper\n# -----------------------------\ndef tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n    model.eval()\n    logits_accum = None\n    with torch.no_grad():\n        for s in scales:\n            tf = transforms.Compose([\n                transforms.Resize((s, s)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n            x = tf(img_pil).unsqueeze(0).to(device)\n            out = model(x)\n            logits = out[\"logits\"]\n            if flip:\n                x_f = torch.flip(x, dims=[3])\n                logits_f = model(x_f)[\"logits\"]\n                logits = (logits + logits_f) / 2.0\n            if logits_accum is None:\n                logits_accum = logits\n            else:\n                logits_accum += logits\n    logits_accum /= len(scales)\n    return logits_accum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:06:47.480947Z","iopub.status.idle":"2025-10-30T09:06:47.481169Z","shell.execute_reply.started":"2025-10-30T09:06:47.481066Z","shell.execute_reply":"2025-10-30T09:06:47.481077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grad-CAM helper (very simple)\n# -----------------------------\ndef get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n    \"\"\"\n    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n    Returns upsampled heatmap (H,W) normalized in [0,1].\n    \"\"\"\n    model.eval()\n    # find layer\n    target_module = None\n    for name, module in model.named_modules():\n        if name == layer_name:\n            target_module = module\n            break\n    if target_module is None:\n        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, input, output):\n        activations.append(output.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    h1 = target_module.register_forward_hook(forward_hook)\n    h2 = target_module.register_full_backward_hook(backward_hook)\n\n    out = model(input_tensor)\n    logits = out[\"logits\"]\n    if target_class is None:\n        target_class = logits.argmax(1).item()\n    loss = logits[:, target_class].sum()\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    act = activations[0]  # [B,C,H,W]\n    grad = gradients[0]   # [B,C,H,W]\n    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n    cam = F.relu(cam)\n    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n    cam = cam.squeeze().cpu().numpy()\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h1.remove(); h2.remove()\n    return cam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:06:47.482312Z","iopub.status.idle":"2025-10-30T09:06:47.482518Z","shell.execute_reply.started":"2025-10-30T09:06:47.482422Z","shell.execute_reply":"2025-10-30T09:06:47.482431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}