{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/poorvaahuja/camourflage-improvement-research?scriptVersionId=270513988\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os, random, math, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:22:57.987444Z","iopub.execute_input":"2025-10-24T10:22:57.987975Z","iopub.status.idle":"2025-10-24T10:22:57.997434Z","shell.execute_reply.started":"2025-10-24T10:22:57.987947Z","shell.execute_reply":"2025-10-24T10:22:57.996916Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom torchvision.transforms import RandAugment\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:22:57.998624Z","iopub.execute_input":"2025-10-24T10:22:57.998892Z","iopub.status.idle":"2025-10-24T10:23:13.056566Z","shell.execute_reply.started":"2025-10-24T10:22:57.998871Z","shell.execute_reply":"2025-10-24T10:23:13.056014Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:51.887901Z","iopub.execute_input":"2025-10-24T10:24:51.888595Z","iopub.status.idle":"2025-10-24T10:24:51.891906Z","shell.execute_reply.started":"2025-10-24T10:24:51.888568Z","shell.execute_reply":"2025-10-24T10:24:51.891313Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == \"cuda\": torch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:52.960939Z","iopub.execute_input":"2025-10-24T10:24:52.961756Z","iopub.status.idle":"2025-10-24T10:24:53.075292Z","shell.execute_reply.started":"2025-10-24T10:24:52.961703Z","shell.execute_reply":"2025-10-24T10:24:53.074585Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 8          # adjust if OOM\nEPOCHS = 20\nNUM_WORKERS = 0         # set 0 if worker issues on Kaggle\nLR = 3e-4\nLABEL_SMOOTH = 0.1\nSAVE_PATH = \"best_model.pth\"\nUSE_SEGMENTATION = True\n\n# Loss weights from PDF suggestion\nALPHA_DOM = 0.5\nBETA_SUPCON = 0.2\nETA_CONS = 0.1\n\n# Mixup/CutMix probabilities and alphas\nPROB_MIXUP = 0.5\nPROB_CUTMIX = 0.5\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 1.0\n\n# warmup epochs\nWARMUP_EPOCHS = 5\n\n# early stopping\nEARLY_STOPPING_PATIENCE = 8\nFREEZE_EPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:53.500822Z","iopub.execute_input":"2025-10-24T10:24:53.501118Z","iopub.status.idle":"2025-10-24T10:24:53.506032Z","shell.execute_reply.started":"2025-10-24T10:24:53.501097Z","shell.execute_reply":"2025-10-24T10:24:53.505309Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"info_dir  = \"/kaggle/input/cod10k/COD10K-v3/Info\"\ntrain_dir = \"/kaggle/input/cod10k/COD10K-v3/Train\"\ntest_dir  = \"/kaggle/input/cod10k/COD10K-v3/Test\"\n\n# these exist in Info/\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:54.175366Z","iopub.execute_input":"2025-10-24T10:24:54.175925Z","iopub.status.idle":"2025-10-24T10:24:54.181009Z","shell.execute_reply.started":"2025-10-24T10:24:54.175892Z","shell.execute_reply":"2025-10-24T10:24:54.180202Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# def merge_txts(txt_list, root_dir, transform, use_masks=True):\n#     samples = []\n#     for txt_file, label in txt_list:\n#         with open(txt_file, \"r\") as f:\n#             for line in f:\n#                 fname = line.strip().split()[0]\n#                 samples.append((fname, label))\n#     return COD10KDataset(root_dir, txt_file=None, transform=transform, use_masks=use_masks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:54.594305Z","iopub.execute_input":"2025-10-24T10:24:54.59511Z","iopub.status.idle":"2025-10-24T10:24:54.599255Z","shell.execute_reply.started":"2025-10-24T10:24:54.595074Z","shell.execute_reply":"2025-10-24T10:24:54.598401Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Noise + Transform","metadata":{}},{"cell_type":"code","source":"class AddGaussianNoise(object):\n    def __init__(self, mean=0., std=0.05):\n        self.mean = mean\n        self.std = std\n    def __call__(self, tensor):\n        noise = torch.randn(tensor.size()) * self.std + self.mean\n        noisy_tensor = tensor + noise\n        return torch.clamp(noisy_tensor, 0., 1.)\n    def __repr__(self):\n        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n\nweak_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.02),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    AddGaussianNoise(0., 0.05),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:54.93976Z","iopub.execute_input":"2025-10-24T10:24:54.94003Z","iopub.status.idle":"2025-10-24T10:24:54.948115Z","shell.execute_reply.started":"2025-10-24T10:24:54.94001Z","shell.execute_reply":"2025-10-24T10:24:54.947411Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class COD10KDataset(Dataset):\n    \"\"\"\n    Parses lines of the form:\n      <filename> <label>\n    Returns (weak_image, strong_image, label, mask_or_none)\n    \"\"\"\n    def __init__(self, root_dir, txt_file, weak_transform=None, strong_transform=None, use_masks=True):\n        self.root_dir = root_dir\n        self.weak_transform = weak_transform\n        self.strong_transform = strong_transform\n        self.use_masks = use_masks\n        self.samples = []\n\n        if not os.path.exists(txt_file):\n            raise RuntimeError(f\"TXT file not found: {txt_file}\")\n\n        with open(txt_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 0:\n                    continue\n                if len(parts) >= 2:\n                    fname = parts[0]\n                    try:\n                        lbl = int(parts[1])\n                    except:\n                        lbl = 1 if \"CAM\" in fname else 0\n                else:\n                    fname = parts[0]\n                    lbl = 1 if \"CAM\" in parts[0] else 0\n                img_path = os.path.join(self.root_dir, \"Image\", fname)\n                if os.path.exists(img_path):\n                    self.samples.append((fname, lbl))\n                else:\n                    pass\n\n        if len(self.samples) == 0:\n            raise RuntimeError(f\"No samples found. Check {txt_file} and the Image folder under {root_dir}.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        fname, lbl = self.samples[idx]\n        img_path = os.path.join(self.root_dir, \"Image\", fname)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.weak_transform:\n            weak = self.weak_transform(img)\n        else:\n            weak = transforms.ToTensor()(img)\n        if self.strong_transform:\n            strong = self.strong_transform(img)\n        else:\n            strong = weak.clone()\n\n        mask = None\n        if self.use_masks:\n            mask_name = os.path.splitext(fname)[0] + \".png\"\n            mask_path = os.path.join(self.root_dir, \"GT_Object\", mask_name)\n            if os.path.exists(mask_path):\n                m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n                m = np.array(m).astype(np.float32) / 255.0\n                mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n\n        return weak, strong, lbl, mask\n\n# helper to build weighted sampler\ndef build_weighted_sampler(dataset):\n    labels = [lbl for (_, lbl) in dataset.samples]\n    counts = Counter(labels)\n    total = len(labels)\n    class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n    weights = [class_weights[lbl] for lbl in labels]\n    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n    return sampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:55.075135Z","iopub.execute_input":"2025-10-24T10:24:55.075355Z","iopub.status.idle":"2025-10-24T10:24:55.085795Z","shell.execute_reply.started":"2025-10-24T10:24:55.075339Z","shell.execute_reply":"2025-10-24T10:24:55.0851Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_ds = COD10KDataset(train_dir, train_cam_txt, weak_transform=weak_tf, strong_transform=strong_tf, use_masks=USE_SEGMENTATION)\nval_ds   = COD10KDataset(test_dir,  test_cam_txt,  weak_transform=val_tf, strong_transform=None, use_masks=USE_SEGMENTATION)\n\ntrain_sampler = build_weighted_sampler(train_ds)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\nprint(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:24:55.387777Z","iopub.execute_input":"2025-10-24T10:24:55.388049Z","iopub.status.idle":"2025-10-24T10:25:04.812797Z","shell.execute_reply.started":"2025-10-24T10:24:55.388031Z","shell.execute_reply":"2025-10-24T10:25:04.812006Z"}},"outputs":[{"name":"stdout","text":"Train samples: 3038 Val samples: 2026\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Backbones","metadata":{}},{"cell_type":"code","source":"class DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.densenet201(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats\n\n\nclass MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):\n                feats.append(out)\n        if len(feats) < 4:\n            feats.append(out)\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.814245Z","iopub.execute_input":"2025-10-24T10:25:04.81478Z","iopub.status.idle":"2025-10-24T10:25:04.82073Z","shell.execute_reply.started":"2025-10-24T10:25:04.814749Z","shell.execute_reply":"2025-10-24T10:25:04.820041Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class SwinExtractor(nn.Module):\n    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.821475Z","iopub.execute_input":"2025-10-24T10:25:04.821738Z","iopub.status.idle":"2025-10-24T10:25:04.861038Z","shell.execute_reply.started":"2025-10-24T10:25:04.821695Z","shell.execute_reply":"2025-10-24T10:25:04.860519Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction,4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.862772Z","iopub.execute_input":"2025-10-24T10:25:04.862987Z","iopub.status.idle":"2025-10-24T10:25:04.878769Z","shell.execute_reply.started":"2025-10-24T10:25:04.862973Z","shell.execute_reply":"2025-10-24T10:25:04.878237Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        if H.shape[2:] != X.shape[2:]:\n            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n        g = self.g_fc(H)\n        return g * H + (1 - g) * X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.879296Z","iopub.execute_input":"2025-10-24T10:25:04.879498Z","iopub.status.idle":"2025-10-24T10:25:04.898089Z","shell.execute_reply.started":"2025-10-24T10:25:04.879472Z","shell.execute_reply":"2025-10-24T10:25:04.897551Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class CrossAttention(nn.Module):\n    def __init__(self, d_cnn, d_swin, d_out):\n        super().__init__()\n        self.q = nn.Linear(d_cnn, d_out)\n        self.k = nn.Linear(d_swin, d_out)\n        self.v = nn.Linear(d_swin, d_out)\n        self.scale = d_out ** -0.5\n    def forward(self, feat_cnn, feat_swin):\n        B, Cc, H, W = feat_cnn.shape\n        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n        if feat_swin.dim() == 4:\n            Bs, Cs, Hs, Ws = feat_swin.shape\n            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n        else:\n            kv = feat_swin\n        K = self.k(kv)\n        V = self.v(kv)\n        Q = self.q(q)\n        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, V)\n        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.898678Z","iopub.execute_input":"2025-10-24T10:25:04.898929Z","iopub.status.idle":"2025-10-24T10:25:04.923489Z","shell.execute_reply.started":"2025-10-24T10:25:04.898914Z","shell.execute_reply":"2025-10-24T10:25:04.922841Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Segmentation Decoder","metadata":{}},{"cell_type":"code","source":"class SegDecoder(nn.Module):\n    def __init__(self, in_channels_list, mid_channels=128):\n        super().__init__()\n        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n        self.out = nn.Conv2d(mid_channels, 1, 1)\n    def forward(self, feat_list):\n        target_size = feat_list[0].shape[2:]\n        ups = []\n        for f, p in zip(feat_list, self.projs):\n            x = p(f)\n            if x.shape[2:] != target_size:\n                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n            ups.append(x)\n        x = torch.cat(ups, dim=1)\n        x = self.conv(x)\n        x = self.out(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.92429Z","iopub.execute_input":"2025-10-24T10:25:04.924535Z","iopub.status.idle":"2025-10-24T10:25:04.941422Z","shell.execute_reply.started":"2025-10-24T10:25:04.924505Z","shell.execute_reply":"2025-10-24T10:25:04.940749Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Probing Backbones","metadata":{}},{"cell_type":"code","source":"dnet = DenseNetExtractor().to(device).eval()\nmnet = MobileNetExtractor().to(device).eval()\nsnet = SwinExtractor().to(device).eval()\nwith torch.no_grad():\n    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n    featsA = dnet(dummy)\n    featsB = mnet(dummy)\n    featsS = snet(dummy)\nchA = [f.shape[1] for f in featsA]\nchB = [f.shape[1] for f in featsB]\nchS = [f.shape[1] for f in featsS]\nprint(\"DenseNet channels:\", chA)\nprint(\"MobileNet channels:\", chB)\nprint(\"Swin channels:\", chS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:04.942218Z","iopub.execute_input":"2025-10-24T10:25:04.942836Z","iopub.status.idle":"2025-10-24T10:25:11.305273Z","shell.execute_reply.started":"2025-10-24T10:25:04.942814Z","shell.execute_reply":"2025-10-24T10:25:11.304648Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n100%|██████████| 77.4M/77.4M [00:00<00:00, 191MB/s] \n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 134MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a3b4add889c4ec6915334da7f3e7122"}},"metadata":{}},{"name":"stdout","text":"DenseNet channels: [256, 512, 1792, 1920]\nMobileNet channels: [24, 40, 80, 112]\nSwin channels: [56, 28, 14, 7]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Fusion Model (DenseNet + MobileNet + Swin cross attention)","metadata":{}},{"cell_type":"code","source":"class FusionWithSwin(nn.Module):\n    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n        super().__init__()\n        self.backA = DenseNetExtractor()\n        self.backB = MobileNetExtractor()\n        self.backS = SwinExtractor()\n        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n        self.L = L\n        self.d = d\n        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n        self.reduce = nn.Conv2d(d * L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n        self.use_seg = use_seg\n        if self.use_seg:\n            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n\n        # Domain head for DANN (simple MLP)\n        self.domain_head = nn.Sequential(\n            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, x, grl_lambda=0.0):\n        fa = self.backA(x)\n        fb = self.backB(x)\n        fs = self.backS(x)\n        fused_feats = []\n        aligned_for_dec = []\n        for i in range(self.L):\n            a = self.alignA[i](fa[i])\n            a = self.cbamA[i](a)\n            b = self.alignB[i](fb[i])\n            b = self.cbamB[i](b)\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused = self.gates[i](a, b)\n            swin_feat = fs[i]\n            swin_att = self.cross_atts[i](fused, swin_feat)\n            if swin_att.shape[2:] != fused.shape[2:]:\n                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n            fused = fused + swin_att\n            fused_feats.append(fused)\n            aligned_for_dec.append(fused)\n        target = fused_feats[-1]\n        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.reduce(concat)\n        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n        logits = self.classifier(z)\n        out = {\"logits\": logits, \"feat\": z}\n        if self.use_seg:\n            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n\n        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n        if grl_lambda > 0.0:\n            # GRL implemented outside (we'll pass z through GRL function)\n            pass\n        out[\"domain_logits\"] = self.domain_head(z)\n        return out\n\n# instantiate model\nmodel = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\nprint(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:11.306041Z","iopub.execute_input":"2025-10-24T10:25:11.306853Z","iopub.status.idle":"2025-10-24T10:25:12.548769Z","shell.execute_reply.started":"2025-10-24T10:25:11.306828Z","shell.execute_reply":"2025-10-24T10:25:12.548132Z"}},"outputs":[{"name":"stdout","text":"Model parameters (M): 51.586615\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class LabelSmoothingCE(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.s = smoothing\n    def forward(self, logits, target):\n        c = logits.size(-1)\n        logp = F.log_softmax(logits, dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(logp)\n            true_dist.fill_(self.s / (c - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n        return (-true_dist * logp).sum(dim=-1).mean()\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.5):\n        super().__init__()\n        self.gamma = gamma\n    def forward(self, logits, target):\n        prob = F.softmax(logits, dim=1)\n        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n        ce = F.cross_entropy(logits, target, reduction='none')\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n\ndef dice_loss_logits(pred_logits, target):\n    pred = torch.sigmoid(pred_logits)\n    target = target.float()\n    inter = (pred * target).sum(dim=(1,2,3))\n    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n    return 1.0 - dice.mean()\n\nclf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\nclf_loss_focal = FocalLoss(gamma=1.5)\nseg_bce = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = torch.sigmoid(pred)\n    num = 2 * (pred * target).sum() + smooth\n    den = pred.sum() + target.sum() + smooth\n    return 1 - (num / den)\n\ndef seg_loss_fn(pred, mask):\n    if pred.shape[-2:] != mask.shape[-2:]:\n        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:12.550808Z","iopub.execute_input":"2025-10-24T10:25:12.551002Z","iopub.status.idle":"2025-10-24T10:25:12.561192Z","shell.execute_reply.started":"2025-10-24T10:25:12.550987Z","shell.execute_reply":"2025-10-24T10:25:12.56068Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#Supervised contrastive Loss\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.cos = nn.CosineSimilarity(dim=-1)\n    def forward(self, features, labels):\n        # features: [N, D], labels: [N]\n        device = features.device\n        f = F.normalize(features, dim=1)\n        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n        labels = labels.contiguous().view(-1,1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        # remove diagonal\n        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n        logits = sim - logits_max.detach()\n        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n        denom = exp_logits.sum(1, keepdim=True)\n        # for each i, positive samples are where mask==1 (excluding self)\n        pos_mask = mask - torch.eye(len(features), device=device)\n        pos_exp = (exp_logits * pos_mask).sum(1)\n        # avoid divide by zero\n        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n        # average only across anchors that have positives\n        valid = (pos_mask.sum(1) > 0).float()\n        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n        return loss\nsupcon_loss_fn = SupConLoss(temperature=0.07)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:12.561967Z","iopub.execute_input":"2025-10-24T10:25:12.562208Z","iopub.status.idle":"2025-10-24T10:25:12.586008Z","shell.execute_reply.started":"2025-10-24T10:25:12.562189Z","shell.execute_reply":"2025-10-24T10:25:12.58542Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Domain Adversarial: Gradient Reversal Layer (GRL)\n\nfrom torch.autograd import Function\nclass GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, l):\n        ctx.l = l\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.l, None\n\ndef grad_reverse(x, l=1.0):\n    return GradReverse.apply(x, l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:12.586755Z","iopub.execute_input":"2025-10-24T10:25:12.586952Z","iopub.status.idle":"2025-10-24T10:25:12.607438Z","shell.execute_reply.started":"2025-10-24T10:25:12.586936Z","shell.execute_reply":"2025-10-24T10:25:12.606948Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Optimizer + scheduler + mixed precision + clipping\n# -----------------------------\n# param groups: smaller LR for backbones, larger for heads\nbackbone_params = []\nhead_params = []\nfor name, param in model.named_parameters():\n    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n        backbone_params.append(param)\n    else:\n        head_params.append(param)\n\nopt = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': LR * 0.2},\n    {'params': head_params, 'lr': LR}\n], lr=LR, weight_decay=1e-4)\n\n# warmup + cosine schedule\ndef get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1.0, warmup_epochs))\n        # cosine from warmup -> total\n        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n        return 0.5 * (1.0 + math.cos(math.pi * t))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n\nscheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n# -----------------------------\n# Mixup & CutMix helpers\n# -----------------------------\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)   # use builtin int\n    cut_h = int(H * cut_rat)   # use builtin int\n\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef apply_mixup(x, y, alpha=MIXUP_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\ndef apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0))\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    new_x = x.clone()\n    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n    return new_x, y, y[idx], lam_adjusted\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:12.608156Z","iopub.execute_input":"2025-10-24T10:25:12.608345Z","iopub.status.idle":"2025-10-24T10:25:12.630855Z","shell.execute_reply.started":"2025-10-24T10:25:12.608323Z","shell.execute_reply":"2025-10-24T10:25:12.630166Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/876045050.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"best_vf1 = 0.0\nbest_epoch = 0\npatience_count = 0\n\ndef compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n    # mix_info: (mode, y_a, y_b, lam) or None\n    if mix_info is None:\n        if use_focal:\n            return clf_loss_focal(logits, targets)\n        else:\n            return clf_loss_ce(logits, targets)\n    else:\n        # mixup/cutmix: soft labels\n        y_a, y_b, lam = mix_info\n        if use_focal:\n            # focal is not designed for soft labels; approximate by weighted CE\n            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n        else:\n            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n        return loss\nfor epoch in range(1, EPOCHS+1):\n    # freeze/unfreeze strategy\n    if epoch <= FREEZE_EPOCHS:\n        # freeze early layers of backbones\n        for name, p in model.named_parameters():\n            if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n                p.requires_grad = False\n    else:\n        for p in model.parameters():\n            p.requires_grad = True\n\n\n    model.train()\n    running_loss = 0.0\n    y_true, y_pred = [], []\n    n_batches = 0\n\n    for weak_imgs, strong_imgs, labels, masks in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n        labels = labels.to(device)\n        if masks is not None:\n            masks = masks.to(device)\n\n        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n        imgs = weak_imgs\n\n        # optionally apply mixup/cutmix on imgs (on weak view)\n        mix_info = None\n        rand = random.random()\n        if rand < PROB_MIXUP:\n            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n        elif rand < PROB_MIXUP + PROB_CUTMIX:\n            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n            mix_info = (y_a.to(device), y_b.to(device), lam)\n\n        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n            out = model(imgs)  # returns logits, feat, seg, domain_logits\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            domain_logits = out.get(\"domain_logits\", None)\n\n            # classification loss (label-smoothing or focal)\n            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n\n            # segmentation loss if available & mask present\n            seg_loss = 0.0\n            if USE_SEGMENTATION and (masks is not None):\n                seg_pred = out[\"seg\"]\n                seg_loss = seg_loss_fn(seg_pred, masks)\n            # supcon loss on features (use features from weak)\n            supcon_loss = supcon_loss_fn(feat, labels)\n\n            # consistency: forward strong view and compare predictions\n            out_strong = model(strong_imgs)\n            logits_strong = out_strong[\"logits\"]\n            probs_weak = F.softmax(logits.detach(), dim=1)\n            probs_strong = F.softmax(logits_strong, dim=1)\n            # L2 between probability vectors (could be KL)\n            cons_loss = F.mse_loss(probs_weak, probs_strong)\n            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n            # To support domain adaptation, user should provide target dataloader and stack batches with domain labels\n            dom_loss = 0.0\n            # (If domain labels are provided, compute dom logits after GRL: domain_logits_grl = domain_head(grad_reverse(feat, l)))\n            # then dom_loss = criterion(domain_logits_grl, domain_labels)\n\n            total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n\n        opt.zero_grad()\n        scaler.scale(total_loss).backward()\n        # gradient clipping\n        scaler.unscale_(opt)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(opt)\n        scaler.update()\n\n        running_loss += total_loss.item()\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(logits.argmax(1).cpu().numpy())\n        n_batches += 1\n\n    scheduler.step()\n\n    # metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n\n    # -------------------\n    # VALIDATION\n    # -------------------\n    model.eval()\n    val_y_true, val_y_pred = [], []\n    val_loss = 0.0\n    with torch.no_grad():\n        for weak_imgs, _, labels, masks in val_loader:\n            imgs = weak_imgs.to(device)\n            labels = labels.to(device)\n            if masks is not None:\n                masks = masks.to(device)\n\n            out = model(imgs)\n            logits = out[\"logits\"]\n            feat = out[\"feat\"]\n            seg_out = out.get(\"seg\", None)\n            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n            if USE_SEGMENTATION and (masks is not None):\n                loss += seg_loss_fn(seg_out, masks)\n            val_loss += loss.item()\n\n            val_y_true.extend(labels.cpu().numpy())\n            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n\n    vacc = accuracy_score(val_y_true, val_y_pred)\n    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n\n    # early stopping & save best\n    if vf1 > best_vf1:\n        best_vf1 = vf1\n        best_epoch = epoch\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"best_vf1\": best_vf1\n        }, SAVE_PATH)\n        patience_count = 0\n        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n    else:\n        patience_count += 1\n        if patience_count >= EARLY_STOPPING_PATIENCE:\n            print(\"Early stopping triggered.\")\n            break\n\nprint(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:25:12.631638Z","iopub.execute_input":"2025-10-24T10:25:12.632415Z","iopub.status.idle":"2025-10-24T10:56:57.217014Z","shell.execute_reply.started":"2025-10-24T10:25:12.632397Z","shell.execute_reply":"2025-10-24T10:56:57.215893Z"}},"outputs":[{"name":"stderr","text":"Train 1/20:   0%|          | 0/380 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 1/20: 100%|██████████| 380/380 [05:25<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 2.1491 Acc: 0.6359 Prec: 0.5000 Rec: 0.3180 F1: 0.3887\n[Epoch 1] Val Loss: 2.2695 Acc: 0.8850 Prec: 0.5000 Rec: 0.4425 F1: 0.4695\nSaved best model at epoch 1 (F1 0.4695)\n","output_type":"stream"},{"name":"stderr","text":"Train 2/20:   0%|          | 0/380 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 2/20: 100%|██████████| 380/380 [05:17<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Train Loss: 1.3793 Acc: 0.9967 Prec: 0.5000 Rec: 0.4984 F1: 0.4992\n[Epoch 2] Val Loss: 1.1828 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\nSaved best model at epoch 2 (F1 1.0000)\n","output_type":"stream"},{"name":"stderr","text":"Train 3/20:   0%|          | 0/380 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 3/20: 100%|██████████| 380/380 [05:11<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Train Loss: 1.2713 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 3] Val Loss: 1.1991 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 4/20:   0%|          | 0/380 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 4/20: 100%|██████████| 380/380 [05:09<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Train Loss: 1.2525 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n[Epoch 4] Val Loss: 1.1289 Acc: 1.0000 Prec: 1.0000 Rec: 1.0000 F1: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Train 5/20:   0%|          | 0/380 [00:00<?, ?it/s]/tmp/ipykernel_36/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\nTrain 5/20:  81%|████████  | 307/380 [04:11<00:59,  1.22it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1965969712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m# gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# Test-time augmentation (TTA) helper\n# -----------------------------\ndef tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n    model.eval()\n    logits_accum = None\n    with torch.no_grad():\n        for s in scales:\n            tf = transforms.Compose([\n                transforms.Resize((s, s)),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n            ])\n            x = tf(img_pil).unsqueeze(0).to(device)\n            out = model(x)\n            logits = out[\"logits\"]\n            if flip:\n                x_f = torch.flip(x, dims=[3])\n                logits_f = model(x_f)[\"logits\"]\n                logits = (logits + logits_f) / 2.0\n            if logits_accum is None:\n                logits_accum = logits\n            else:\n                logits_accum += logits\n    logits_accum /= len(scales)\n    return logits_accum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:57:05.367461Z","iopub.execute_input":"2025-10-24T10:57:05.368019Z","iopub.status.idle":"2025-10-24T10:57:05.373458Z","shell.execute_reply.started":"2025-10-24T10:57:05.367996Z","shell.execute_reply":"2025-10-24T10:57:05.372766Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Grad-CAM helper (very simple)\n# -----------------------------\ndef get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n    \"\"\"\n    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n    Returns upsampled heatmap (H,W) normalized in [0,1].\n    \"\"\"\n    model.eval()\n    # find layer\n    target_module = None\n    for name, module in model.named_modules():\n        if name == layer_name:\n            target_module = module\n            break\n    if target_module is None:\n        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n\n    activations = []\n    gradients = []\n\n    def forward_hook(module, input, output):\n        activations.append(output.detach())\n    def backward_hook(module, grad_in, grad_out):\n        gradients.append(grad_out[0].detach())\n\n    h1 = target_module.register_forward_hook(forward_hook)\n    h2 = target_module.register_full_backward_hook(backward_hook)\n\n    out = model(input_tensor)\n    logits = out[\"logits\"]\n    if target_class is None:\n        target_class = logits.argmax(1).item()\n    loss = logits[:, target_class].sum()\n    model.zero_grad()\n    loss.backward(retain_graph=True)\n\n    act = activations[0]  # [B,C,H,W]\n    grad = gradients[0]   # [B,C,H,W]\n    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n    cam = F.relu(cam)\n    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n    cam = cam.squeeze().cpu().numpy()\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    h1.remove(); h2.remove()\n    return cam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T10:56:57.218406Z","iopub.status.idle":"2025-10-24T10:56:57.218645Z","shell.execute_reply.started":"2025-10-24T10:56:57.218537Z","shell.execute_reply":"2025-10-24T10:56:57.218552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}