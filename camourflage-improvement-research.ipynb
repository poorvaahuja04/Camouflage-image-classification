{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anuhskaa/camouflage-improvement-research-2?scriptVersionId=272602025\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"b8e82228","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-11-01T10:38:16.027304Z","iopub.status.busy":"2025-11-01T10:38:16.027032Z","iopub.status.idle":"2025-11-01T10:38:16.070312Z","shell.execute_reply":"2025-11-01T10:38:16.069578Z"},"papermill":{"duration":0.052011,"end_time":"2025-11-01T10:38:16.071551","exception":false,"start_time":"2025-11-01T10:38:16.01954","status":"completed"},"tags":[]},"outputs":[],"source":["import os, random, math, time\n","from pathlib import Path\n","from tqdm import tqdm\n","import numpy as np\n","from PIL import Image"]},{"cell_type":"code","execution_count":2,"id":"e3444091","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:16.083445Z","iopub.status.busy":"2025-11-01T10:38:16.083266Z","iopub.status.idle":"2025-11-01T10:38:33.965195Z","shell.execute_reply":"2025-11-01T10:38:33.964615Z"},"papermill":{"duration":17.889122,"end_time":"2025-11-01T10:38:33.966499","exception":false,"start_time":"2025-11-01T10:38:16.077377","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torchvision import transforms, models\n","from torchvision.transforms import RandAugment\n","import timm"]},{"cell_type":"code","execution_count":3,"id":"2220c041","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:33.978375Z","iopub.status.busy":"2025-11-01T10:38:33.978146Z","iopub.status.idle":"2025-11-01T10:38:36.111501Z","shell.execute_reply":"2025-11-01T10:38:36.11088Z"},"papermill":{"duration":2.140671,"end_time":"2025-11-01T10:38:36.112905","exception":false,"start_time":"2025-11-01T10:38:33.972234","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from collections import Counter"]},{"cell_type":"code","execution_count":4,"id":"584a7f25","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.125366Z","iopub.status.busy":"2025-11-01T10:38:36.124578Z","iopub.status.idle":"2025-11-01T10:38:36.233148Z","shell.execute_reply":"2025-11-01T10:38:36.232241Z"},"papermill":{"duration":0.115782,"end_time":"2025-11-01T10:38:36.234452","exception":false,"start_time":"2025-11-01T10:38:36.11867","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if device == \"cuda\": torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"code","execution_count":5,"id":"ffa7989a","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.246285Z","iopub.status.busy":"2025-11-01T10:38:36.246046Z","iopub.status.idle":"2025-11-01T10:38:36.250467Z","shell.execute_reply":"2025-11-01T10:38:36.249858Z"},"papermill":{"duration":0.011324,"end_time":"2025-11-01T10:38:36.251561","exception":false,"start_time":"2025-11-01T10:38:36.240237","status":"completed"},"tags":[]},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 8          # adjust if OOM\n","EPOCHS = 20\n","NUM_WORKERS = 4         # set 0 if worker issues on Kaggle\n","LR = 3e-4\n","LABEL_SMOOTH = 0.1\n","SAVE_PATH = \"best_model.pth\"\n","USE_SEGMENTATION = True\n","\n","# Loss weights from PDF suggestion\n","ALPHA_DOM = 0.5\n","BETA_SUPCON = 0.2\n","ETA_CONS = 0.1\n","\n","# Mixup/CutMix probabilities and alphas\n","PROB_MIXUP = 0.5\n","PROB_CUTMIX = 0.5\n","MIXUP_ALPHA = 0.2\n","CUTMIX_ALPHA = 1.0\n","\n","# warmup epochs\n","WARMUP_EPOCHS = 5\n","\n","# early stopping\n","EARLY_STOPPING_PATIENCE = 15\n","FREEZE_EPOCHS = 10\n","ACCUMULATION_STEPS = 2"]},{"cell_type":"code","execution_count":6,"id":"3f0c162c","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.263275Z","iopub.status.busy":"2025-11-01T10:38:36.262859Z","iopub.status.idle":"2025-11-01T10:38:36.266652Z","shell.execute_reply":"2025-11-01T10:38:36.266158Z"},"papermill":{"duration":0.010609,"end_time":"2025-11-01T10:38:36.267592","exception":false,"start_time":"2025-11-01T10:38:36.256983","status":"completed"},"tags":[]},"outputs":[],"source":["info_dir  = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n","train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train/Image\"\n","test_dir_cod  = \"/kaggle/input/cod10k/COD10K-v3/Test/Image\"\n","\n","# these exist in Info/\n","combined_train_cam  = os.path.join(info_dir, \"CAM_train.txt\")\n","combined_train_noncam  = os.path.join(info_dir, \"NonCAM_train.txt\")\n","combined_test_cam = os.path.join(info_dir, \"CAM_test.txt\")\n","combined_test_noncam = os.path.join(info_dir, \"NonCAM_test.txt\")"]},{"cell_type":"code","execution_count":7,"id":"e39fd4fd","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.27959Z","iopub.status.busy":"2025-11-01T10:38:36.279389Z","iopub.status.idle":"2025-11-01T10:38:36.28299Z","shell.execute_reply":"2025-11-01T10:38:36.282434Z"},"papermill":{"duration":0.010432,"end_time":"2025-11-01T10:38:36.283973","exception":false,"start_time":"2025-11-01T10:38:36.273541","status":"completed"},"tags":[]},"outputs":[],"source":["info_dir2 =\"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n","train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n","train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n","test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n","test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")"]},{"cell_type":"code","execution_count":8,"id":"4103a3d6","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.29534Z","iopub.status.busy":"2025-11-01T10:38:36.2951Z","iopub.status.idle":"2025-11-01T10:38:36.298325Z","shell.execute_reply":"2025-11-01T10:38:36.297807Z"},"papermill":{"duration":0.010103,"end_time":"2025-11-01T10:38:36.299338","exception":false,"start_time":"2025-11-01T10:38:36.289235","status":"completed"},"tags":[]},"outputs":[],"source":["train_dir_camo = {\n","    \"cam\": \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage/Images/Train\",\n","    \"noncam\": \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage/Images/Train\"\n","}\n","test_dir_camo = {\n","    \"cam\": \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage/Images/Test\",\n","    \"noncam\": \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage/Images/Test\"\n","}"]},{"cell_type":"code","execution_count":9,"id":"ec8bc9aa","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.310939Z","iopub.status.busy":"2025-11-01T10:38:36.310747Z","iopub.status.idle":"2025-11-01T10:38:36.31422Z","shell.execute_reply":"2025-11-01T10:38:36.313648Z"},"papermill":{"duration":0.010652,"end_time":"2025-11-01T10:38:36.315309","exception":false,"start_time":"2025-11-01T10:38:36.304657","status":"completed"},"tags":[]},"outputs":[],"source":["train_dir = [train_dir_cod, train_dir_camo[\"cam\"], train_dir_camo[\"noncam\"]]\n","test_dir  = [test_dir_cod, test_dir_camo[\"cam\"], test_dir_camo[\"noncam\"]]\n"]},{"cell_type":"code","execution_count":10,"id":"74a929ce","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.326425Z","iopub.status.busy":"2025-11-01T10:38:36.326244Z","iopub.status.idle":"2025-11-01T10:38:36.444293Z","shell.execute_reply":"2025-11-01T10:38:36.443725Z"},"papermill":{"duration":0.125142,"end_time":"2025-11-01T10:38:36.445589","exception":false,"start_time":"2025-11-01T10:38:36.320447","status":"completed"},"tags":[]},"outputs":[],"source":["# Combine train files\n","with open(combined_train_cam) as f1, open(train_cam_txt2) as f2:\n","    train_cam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","with open(combined_train_noncam) as f1, open(train_noncam_txt2) as f2:\n","    train_noncam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","# Combine test files\n","with open(combined_test_cam) as f1, open(test_cam_txt2) as f2:\n","    test_cam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","with open(combined_test_noncam) as f1, open(test_noncam_txt2) as f2:\n","    test_noncam_txt= f1.read().splitlines() + f2.read().splitlines()\n"]},{"cell_type":"markdown","id":"7c288be1","metadata":{"papermill":{"duration":0.004994,"end_time":"2025-11-01T10:38:36.456203","exception":false,"start_time":"2025-11-01T10:38:36.451209","status":"completed"},"tags":[]},"source":["Noise + Transform"]},{"cell_type":"code","execution_count":11,"id":"a12eb344","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.467285Z","iopub.status.busy":"2025-11-01T10:38:36.467089Z","iopub.status.idle":"2025-11-01T10:38:36.474438Z","shell.execute_reply":"2025-11-01T10:38:36.473723Z"},"papermill":{"duration":0.01438,"end_time":"2025-11-01T10:38:36.475586","exception":false,"start_time":"2025-11-01T10:38:36.461206","status":"completed"},"tags":[]},"outputs":[],"source":["class AddGaussianNoise(object):\n","    def __init__(self, mean=0., std=0.05):\n","        self.mean = mean\n","        self.std = std\n","    def __call__(self, tensor):\n","        noise = torch.randn(tensor.size()) * self.std + self.mean\n","        noisy_tensor = tensor + noise\n","        return torch.clamp(noisy_tensor, 0., 1.)\n","    def __repr__(self):\n","        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n","\n","weak_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(0., 0.02),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","strong_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n","    RandAugment(num_ops=2, magnitude=9),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(30),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(0., 0.05),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","val_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n"]},{"cell_type":"code","execution_count":12,"id":"38d77e75","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.487307Z","iopub.status.busy":"2025-11-01T10:38:36.487094Z","iopub.status.idle":"2025-11-01T10:38:36.527286Z","shell.execute_reply":"2025-11-01T10:38:36.526742Z"},"papermill":{"duration":0.047423,"end_time":"2025-11-01T10:38:36.528268","exception":false,"start_time":"2025-11-01T10:38:36.480845","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","from collections import Counter\n","from torch.utils.data import Dataset, WeightedRandomSampler, DataLoader\n","from PIL import Image\n","import torch\n","import numpy as np\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","\n","# NOTE: Placeholder definitions for missing global variables for completeness.\n","# Please ensure these are correctly defined in your main script.\n","IMG_SIZE = 224  # Common size for COD/SOD tasks\n","weak_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n","strong_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()]) # Placeholder for actual strong transform\n","val_tf = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n","USE_SEGMENTATION = True\n","BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","\n","ACCUMULATION_STEPS = 2\n","# Function to try reading a file with multiple encodings\n","def read_file_with_encoding(file_path, encodings=['utf-8', 'utf-8-sig', 'ISO-8859-1']):\n","    \"\"\"Tries to read a file using a list of common encodings.\"\"\"\n","    for encoding in encodings:\n","        try:\n","            with open(file_path, 'r', encoding=encoding) as f:\n","                return f.readlines()\n","        except UnicodeDecodeError:\n","            print(f\"Failed to read {file_path} with encoding {encoding}\")\n","        except Exception as e:\n","            print(f\"Error reading {file_path}: {e}\")\n","    raise RuntimeError(f\"Unable to read {file_path} with any of the provided encodings.\")\n","\n","def load_testing_dataset_info(info_file, image_dir):\n","    \"\"\"Loads image paths and labels from the testing dataset info file.\"\"\"\n","    image_paths = []\n","    labels = []\n","    \n","    # List of encodings to try\n","    encodings = ['utf-8-sig', 'utf-8', 'ISO-8859-1', 'latin-1']\n","    lines = []\n","    \n","    for encoding in encodings:\n","        try:\n","            with open(info_file, 'r', encoding=encoding) as f:\n","                lines = f.readlines()\n","            break  # If successful, break out of the loop\n","        except UnicodeDecodeError:\n","            print(f\"Failed to read {info_file} with encoding {encoding}. Trying another encoding...\")\n","        except Exception as e:\n","            print(f\"Error reading {info_file}: {e}\")\n","            raise  # Raise the error if it's something unexpected\n","    \n","    # Process the lines if file was successfully read\n","    for line in lines:\n","        parts = line.strip().split()\n","        if len(parts) == 2:\n","            image_filename = parts[0]\n","            # Ensure label is always 0 or 1\n","            try:\n","                label = int(parts[1])\n","            except ValueError:\n","                continue # Skip malformed lines\n","                \n","            label = 1 if label == 1 else 0  # Map label '1' to CAM and '0' to Non-CAM\n","            image_full_path = os.path.join(image_dir, image_filename)  # Combine with image directory\n","            image_paths.append(image_full_path)\n","            labels.append(label)\n","    \n","    return image_paths, labels\n","\n","\n","# MultiDataset class with proper encoding handling\n","class MultiDataset(Dataset):\n","    def __init__(self, root_dirs, txt_files, testing_image_paths=None, testing_labels=None, weak_transform=None, strong_transform=None, use_masks=True):\n","        self.root_dirs = root_dirs\n","        self.weak_transform = weak_transform\n","        self.strong_transform = strong_transform\n","        self.use_masks = use_masks\n","        self.samples = []\n","\n","        # Handle the testing dataset (testing-dataset images and labels)\n","        if testing_image_paths is not None and testing_labels is not None:\n","            for img_path, label in zip(testing_image_paths, testing_labels):\n","                # Samples from testing-dataset are stored as 2-element tuples (img_path, label)\n","                self.samples.append((img_path, label)) \n","        \n","        # Process other datasets (COD10K, CAMO, etc.)\n","        if isinstance(txt_files, str):\n","            txt_files = [txt_files]\n","\n","        all_lines = []\n","        for t in txt_files:\n","            if not os.path.exists(t):\n","                raise RuntimeError(f\"TXT file not found: {t}\")\n","\n","            # Use the read_file_with_encoding function to handle different encodings\n","            lines = read_file_with_encoding(t)\n","            all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n","\n","        for line, src_txt in all_lines:\n","            parts = line.split()\n","            if len(parts) == 0:\n","                continue\n","\n","            fname = parts[0]\n","            if len(parts) >= 2:\n","                try:\n","                    lbl = int(parts[1])\n","                except:\n","                    # Fallback classification if label is not an integer\n","                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n","            else:\n","                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n","            \n","            # Map labels to binary (CAM=1, Non-CAM=0)\n","            lbl = 1 if lbl == 1 else 0\n","\n","            found = False\n","            search_subs = [\n","                \"\",  # If image is directly in root_dir (less common)\n","                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders \n","                \"Images/Train\", \"Images/Test\", # CAMO-COCO style paths\n","            ]\n","            \n","            base_fname = os.path.basename(fname)  \n","\n","            for rdir in self.root_dirs:\n","                for sub in search_subs:\n","                    img_path = os.path.join(rdir, sub, base_fname)\n","                    if os.path.exists(img_path):\n","                        # Samples from COD/CAMO are stored as 3-element tuples (img_path, lbl, rdir)\n","                        self.samples.append((img_path, lbl, rdir))\n","                        found = True\n","                        break\n","                if found:\n","                    break\n","\n","            if not found:\n","                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n","\n","        if len(self.samples) == 0:\n","            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n","\n","        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        global IMG_SIZE \n","        \n","        sample = self.samples[idx]\n","        \n","        # 1. SAFELY UNPACK sample tuple (length 2 or 3)\n","        img_path = sample[0]\n","        lbl = sample[1]\n","        \n","        # Define rdir for consistent mask lookup logic\n","        if len(sample) == 3:\n","            # If it's a 3-element tuple (COD/CAMO), rdir is the third element\n","            rdir = sample[2]\n","            # Root directory for testing-dataset (used as fallback for mask lookup)\n","            testing_root = None \n","        else:\n","            # If it's a 2-element tuple (testing-dataset split), rdir is undefined.\n","            # We set rdir to a base directory that will be searched for masks.\n","            # Assuming testing_images_dir is defined globally or passed, \n","            # we infer the testing-dataset root from the image path (two levels up)\n","            # This is a guess but required to prevent NameError in mask search.\n","            rdir = os.path.dirname(os.path.dirname(img_path))\n","            # Set testing_root explicitly for clarity if needed later, but rdir is now defined.\n","\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.weak_transform:\n","            weak = self.weak_transform(img)\n","        else:\n","            weak = transforms.ToTensor()(img)\n","            \n","        if self.strong_transform:\n","            strong = self.strong_transform(img)\n","        else:\n","            strong = weak.clone()\n","\n","        mask = None\n","        if self.use_masks:\n","            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n","            \n","            # Use the defined rdir for mask search\n","            found_mask = False\n","            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n","                mask_path = os.path.join(rdir, mask_dir, mask_name)\n","                \n","                if os.path.exists(mask_path):\n","                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n","                    m = np.array(m).astype(np.float32) / 255.0\n","                    # Convert to binary mask: 0 or 1\n","                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n","                    found_mask = True\n","                    break\n","\n","            if mask is None:\n","                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n","                \n","        return weak, strong, lbl, mask\n","\n","def build_weighted_sampler(dataset):\n","    \"\"\"\n","    Builds a WeightedRandomSampler based on class imbalance.\n","    FIXED: Safely extracts label (index 1) from both 2-element and 3-element tuples.\n","    \"\"\"\n","    \n","    # Safely extract labels (always index 1, regardless of tuple length)\n","    labels = [sample[1] for sample in dataset.samples]  \n","    \n","    counts = Counter(labels)\n","    total = len(labels)\n","    \n","    # Ensure there are at least two classes to calculate class_weights\n","    if len(counts) <= 1:\n","        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n","        weights = [1.0] * total\n","    else:\n","        # Calculate inverse frequency weights\n","        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n","        weights = [class_weights[lbl] for lbl in labels]\n","        \n","    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n","\n","# --- Path and Configuration Setup (as provided by user) ---\n","\n"]},{"cell_type":"code","execution_count":13,"id":"dd8a4824","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:38:36.539887Z","iopub.status.busy":"2025-11-01T10:38:36.539664Z","iopub.status.idle":"2025-11-01T10:40:12.867393Z","shell.execute_reply":"2025-11-01T10:40:12.866673Z"},"papermill":{"duration":96.340945,"end_time":"2025-11-01T10:40:12.87457","exception":false,"start_time":"2025-11-01T10:38:36.533625","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Loaded 14150 samples from 4 root directories.\n","✅ Loaded 6606 samples from 4 root directories.\n","Total Train samples: 14150 Total Val samples: 6606\n"]}],"source":["info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n","train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" \n","test_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Test\"  \n","    \n","# COD10K Info files\n","train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n","train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n","test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n","test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n","\n","# CAMO-COCO PATHS\n","info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n","\n","# CAMO-COCO Info files\n","train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n","train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n","test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n","test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n","\n","# CAMO-COCO Root Directories\n","train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n","train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n","\n","testing_info_file = \"/kaggle/input/testing-dataset/Info/image_labels.txt\"\n","testing_images_dir = \"/kaggle/input/testing-dataset/Images\"\n","testing_image_paths, testing_labels = load_testing_dataset_info(testing_info_file, testing_images_dir)\n","\n","# Split testing-dataset: 20% train (train_paths), 80% validation (val_paths)\n","train_paths, val_paths, train_labels, val_labels = train_test_split(\n","    testing_image_paths, testing_labels, test_size=0.8, random_state=42\n",")\n","\n","# 1. All Root Directories\n","ALL_ROOT_DIRS = [\n","    train_dir_cod,       \n","    test_dir_cod,       \n","    train_dir_camo_cam,  \n","    train_dir_camo_noncam\n","]\n","\n","# 2. Training TXT files: ALL COD10K/CAMO-COCO data (both train and test splits)\n","ALL_TRAIN_TXTS = [\n","    train_cam_txt, train_noncam_txt, test_cam_txt, test_noncam_txt,\n","    train_cam_txt2, train_noncam_txt2, test_cam_txt2, test_noncam_txt2,\n","]\n","\n","# 3. Validation TXT files: ONLY the 80% testing-dataset split will be used, so this list is empty.\n","ALL_VAL_TXTS = []\n","\n","# --- Create the final unified datasets ---\n","\n","# Training Dataset: All external data (via ALL_TRAIN_TXTS) + 20% testing-dataset split (via train_paths)\n","train_ds = MultiDataset(\n","    root_dirs=ALL_ROOT_DIRS, \n","    txt_files=ALL_TRAIN_TXTS,               \n","    testing_image_paths=train_paths,        \n","    testing_labels=train_labels,            \n","    weak_transform=weak_tf, \n","    strong_transform=strong_tf, \n","    use_masks=USE_SEGMENTATION\n",")\n","\n","# Validation Dataset: No external data (via empty ALL_VAL_TXTS) + 80% testing-dataset split (via val_paths)\n","val_ds = MultiDataset(\n","    root_dirs=ALL_ROOT_DIRS,  \n","    txt_files=ALL_VAL_TXTS,                 \n","    testing_image_paths=val_paths,          \n","    testing_labels=val_labels,              \n","    weak_transform=val_tf, \n","    strong_transform=None, \n","    use_masks=USE_SEGMENTATION\n",")\n","\n","# Build Sampler and DataLoader\n","train_sampler = build_weighted_sampler(train_ds)\n","\n","train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n","val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n","\n","print(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))\n"]},{"cell_type":"markdown","id":"1dd7ad0b","metadata":{"papermill":{"duration":0.005252,"end_time":"2025-11-01T10:40:12.885393","exception":false,"start_time":"2025-11-01T10:40:12.880141","status":"completed"},"tags":[]},"source":["## Backbones"]},{"cell_type":"code","execution_count":14,"id":"1bcc1ff0","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:12.897606Z","iopub.status.busy":"2025-11-01T10:40:12.897355Z","iopub.status.idle":"2025-11-01T10:40:12.903525Z","shell.execute_reply":"2025-11-01T10:40:12.902929Z"},"papermill":{"duration":0.013771,"end_time":"2025-11-01T10:40:12.904655","exception":false,"start_time":"2025-11-01T10:40:12.890884","status":"completed"},"tags":[]},"outputs":[],"source":["class DenseNetExtractor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.densenet201(pretrained=pretrained).features\n","    def forward(self, x):\n","        feats = []\n","        for name, layer in self.features._modules.items():\n","            x = layer(x)\n","            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n","                feats.append(x)\n","        return feats\n","\n","\n","class MobileNetExtractor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n","    def forward(self, x):\n","        feats = []\n","        out = x\n","        for i, layer in enumerate(self.features):\n","            out = layer(out)\n","            if i in (2,5,9,12):\n","                feats.append(out)\n","        if len(feats) < 4:\n","            feats.append(out)\n","        return feats"]},{"cell_type":"code","execution_count":15,"id":"56d5e6d7","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:12.916638Z","iopub.status.busy":"2025-11-01T10:40:12.916434Z","iopub.status.idle":"2025-11-01T10:40:12.920492Z","shell.execute_reply":"2025-11-01T10:40:12.919803Z"},"papermill":{"duration":0.011379,"end_time":"2025-11-01T10:40:12.921643","exception":false,"start_time":"2025-11-01T10:40:12.910264","status":"completed"},"tags":[]},"outputs":[],"source":["class SwinExtractor(nn.Module):\n","    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":16,"id":"42dabbed","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:12.933072Z","iopub.status.busy":"2025-11-01T10:40:12.932857Z","iopub.status.idle":"2025-11-01T10:40:12.937822Z","shell.execute_reply":"2025-11-01T10:40:12.937068Z"},"papermill":{"duration":0.011854,"end_time":"2025-11-01T10:40:12.93897","exception":false,"start_time":"2025-11-01T10:40:12.927116","status":"completed"},"tags":[]},"outputs":[],"source":["class CBAMlite(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super().__init__()\n","        self.se = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(channels, max(channels//reduction,4), 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(max(channels//reduction,4), channels, 1),\n","            nn.Sigmoid()\n","        )\n","        self.spatial = nn.Sequential(\n","            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels, 1, 1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        return x * self.se(x) * self.spatial(x)\n"]},{"cell_type":"code","execution_count":17,"id":"cb851872","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:12.950239Z","iopub.status.busy":"2025-11-01T10:40:12.950034Z","iopub.status.idle":"2025-11-01T10:40:12.954837Z","shell.execute_reply":"2025-11-01T10:40:12.954143Z"},"papermill":{"duration":0.011791,"end_time":"2025-11-01T10:40:12.95594","exception":false,"start_time":"2025-11-01T10:40:12.944149","status":"completed"},"tags":[]},"outputs":[],"source":["class GatedFusion(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.g_fc = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(dim, max(dim//4, 4), 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(max(dim//4,4), dim, 1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, H, X):\n","        if H.shape[2:] != X.shape[2:]:\n","            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n","        g = self.g_fc(H)\n","        return g * H + (1 - g) * X"]},{"cell_type":"code","execution_count":18,"id":"71f3d220","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:12.966775Z","iopub.status.busy":"2025-11-01T10:40:12.966534Z","iopub.status.idle":"2025-11-01T10:40:12.972244Z","shell.execute_reply":"2025-11-01T10:40:12.971549Z"},"papermill":{"duration":0.012268,"end_time":"2025-11-01T10:40:12.973241","exception":false,"start_time":"2025-11-01T10:40:12.960973","status":"completed"},"tags":[]},"outputs":[],"source":["class CrossAttention(nn.Module):\n","    def __init__(self, d_cnn, d_swin, d_out):\n","        super().__init__()\n","        self.q = nn.Linear(d_cnn, d_out)\n","        self.k = nn.Linear(d_swin, d_out)\n","        self.v = nn.Linear(d_swin, d_out)\n","        self.scale = d_out ** -0.5\n","    def forward(self, feat_cnn, feat_swin):\n","        B, Cc, H, W = feat_cnn.shape\n","        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n","        if feat_swin.dim() == 4:\n","            Bs, Cs, Hs, Ws = feat_swin.shape\n","            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n","        else:\n","            kv = feat_swin\n","        K = self.k(kv)\n","        V = self.v(kv)\n","        Q = self.q(q)\n","        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        out = torch.matmul(attn, V)\n","        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n","        return out\n"]},{"cell_type":"markdown","id":"0ade0a90","metadata":{"papermill":{"duration":0.004932,"end_time":"2025-11-01T10:40:12.983469","exception":false,"start_time":"2025-11-01T10:40:12.978537","status":"completed"},"tags":[]},"source":["## Segmentation Decoder"]},{"cell_type":"code","execution_count":19,"id":"4023b106","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:12.995096Z","iopub.status.busy":"2025-11-01T10:40:12.994871Z","iopub.status.idle":"2025-11-01T10:40:13.000385Z","shell.execute_reply":"2025-11-01T10:40:12.999657Z"},"papermill":{"duration":0.012733,"end_time":"2025-11-01T10:40:13.001441","exception":false,"start_time":"2025-11-01T10:40:12.988708","status":"completed"},"tags":[]},"outputs":[],"source":["class SegDecoder(nn.Module):\n","    def __init__(self, in_channels_list, mid_channels=128):\n","        super().__init__()\n","        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n","        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n","        self.out = nn.Conv2d(mid_channels, 1, 1)\n","    def forward(self, feat_list):\n","        target_size = feat_list[0].shape[2:]\n","        ups = []\n","        for f, p in zip(feat_list, self.projs):\n","            x = p(f)\n","            if x.shape[2:] != target_size:\n","                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n","            ups.append(x)\n","        x = torch.cat(ups, dim=1)\n","        x = self.conv(x)\n","        x = self.out(x)\n","        return x"]},{"cell_type":"markdown","id":"7653fa01","metadata":{"papermill":{"duration":0.005095,"end_time":"2025-11-01T10:40:13.011826","exception":false,"start_time":"2025-11-01T10:40:13.006731","status":"completed"},"tags":[]},"source":["## Probing Backbones"]},{"cell_type":"code","execution_count":20,"id":"7100181b","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:13.023378Z","iopub.status.busy":"2025-11-01T10:40:13.023174Z","iopub.status.idle":"2025-11-01T10:40:19.005652Z","shell.execute_reply":"2025-11-01T10:40:19.004749Z"},"papermill":{"duration":5.989528,"end_time":"2025-11-01T10:40:19.007003","exception":false,"start_time":"2025-11-01T10:40:13.017475","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n","100%|██████████| 77.4M/77.4M [00:00<00:00, 202MB/s]\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n","100%|██████████| 21.1M/21.1M [00:00<00:00, 149MB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cc333c262b04d39b6b032f9be1ed331","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DenseNet channels: [256, 512, 1792, 1920]\n","MobileNet channels: [24, 40, 80, 112]\n","Swin channels: [56, 28, 14, 7]\n"]}],"source":["dnet = DenseNetExtractor().to(device).eval()\n","mnet = MobileNetExtractor().to(device).eval()\n","snet = SwinExtractor().to(device).eval()\n","with torch.no_grad():\n","    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n","    featsA = dnet(dummy)\n","    featsB = mnet(dummy)\n","    featsS = snet(dummy)\n","chA = [f.shape[1] for f in featsA]\n","chB = [f.shape[1] for f in featsB]\n","chS = [f.shape[1] for f in featsS]\n","print(\"DenseNet channels:\", chA)\n","print(\"MobileNet channels:\", chB)\n","print(\"Swin channels:\", chS)"]},{"cell_type":"markdown","id":"8db333c0","metadata":{"papermill":{"duration":0.005721,"end_time":"2025-11-01T10:40:19.019291","exception":false,"start_time":"2025-11-01T10:40:19.01357","status":"completed"},"tags":[]},"source":["# Fusion Model (DenseNet + MobileNet + Swin cross attention)"]},{"cell_type":"code","execution_count":21,"id":"c58f654f","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:19.032006Z","iopub.status.busy":"2025-11-01T10:40:19.031788Z","iopub.status.idle":"2025-11-01T10:40:20.293491Z","shell.execute_reply":"2025-11-01T10:40:20.292716Z"},"papermill":{"duration":1.269416,"end_time":"2025-11-01T10:40:20.294651","exception":false,"start_time":"2025-11-01T10:40:19.025235","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model parameters (M): 51.586615\n"]}],"source":["class FusionWithSwin(nn.Module):\n","    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n","        super().__init__()\n","        self.backA = DenseNetExtractor()\n","        self.backB = MobileNetExtractor()\n","        self.backS = SwinExtractor()\n","        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n","        self.L = L\n","        self.d = d\n","        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n","        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n","        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n","        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n","        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n","        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n","        self.reduce = nn.Conv2d(d * L, d, 1)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(128, num_classes)\n","        )\n","        self.use_seg = use_seg\n","        if self.use_seg:\n","            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n","\n","        # Domain head for DANN (simple MLP)\n","        self.domain_head = nn.Sequential(\n","            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(256, 2)\n","        )\n","\n","    def forward(self, x, grl_lambda=0.0):\n","        fa = self.backA(x)\n","        fb = self.backB(x)\n","        fs = self.backS(x)\n","        fused_feats = []\n","        aligned_for_dec = []\n","        for i in range(self.L):\n","            a = self.alignA[i](fa[i])\n","            a = self.cbamA[i](a)\n","            b = self.alignB[i](fb[i])\n","            b = self.cbamB[i](b)\n","            if b.shape[2:] != a.shape[2:]:\n","                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n","            fused = self.gates[i](a, b)\n","            swin_feat = fs[i]\n","            swin_att = self.cross_atts[i](fused, swin_feat)\n","            if swin_att.shape[2:] != fused.shape[2:]:\n","                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n","            fused = fused + swin_att\n","            fused_feats.append(fused)\n","            aligned_for_dec.append(fused)\n","        target = fused_feats[-1]\n","        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n","        concat = torch.cat(upsampled, dim=1)\n","        fused = self.reduce(concat)\n","        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n","        logits = self.classifier(z)\n","        out = {\"logits\": logits, \"feat\": z}\n","        if self.use_seg:\n","            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n","\n","        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n","        if grl_lambda > 0.0:\n","            # GRL implemented outside (we'll pass z through GRL function)\n","            pass\n","        out[\"domain_logits\"] = self.domain_head(z)\n","        return out\n","\n","# instantiate model\n","model = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\n","print(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)"]},{"cell_type":"code","execution_count":22,"id":"98c51de1","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:20.308402Z","iopub.status.busy":"2025-11-01T10:40:20.308206Z","iopub.status.idle":"2025-11-01T10:40:20.317837Z","shell.execute_reply":"2025-11-01T10:40:20.317125Z"},"papermill":{"duration":0.017635,"end_time":"2025-11-01T10:40:20.318908","exception":false,"start_time":"2025-11-01T10:40:20.301273","status":"completed"},"tags":[]},"outputs":[],"source":["class LabelSmoothingCE(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        self.s = smoothing\n","    def forward(self, logits, target):\n","        c = logits.size(-1)\n","        logp = F.log_softmax(logits, dim=-1)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(logp)\n","            true_dist.fill_(self.s / (c - 1))\n","            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n","        return (-true_dist * logp).sum(dim=-1).mean()\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=1.5):\n","        super().__init__()\n","        self.gamma = gamma\n","    def forward(self, logits, target):\n","        prob = F.softmax(logits, dim=1)\n","        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n","        ce = F.cross_entropy(logits, target, reduction='none')\n","        loss = ((1 - pt) ** self.gamma) * ce\n","        return loss.mean()\n","\n","def dice_loss_logits(pred_logits, target):\n","    pred = torch.sigmoid(pred_logits)\n","    target = target.float()\n","    inter = (pred * target).sum(dim=(1,2,3))\n","    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n","    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n","    return 1.0 - dice.mean()\n","\n","clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n","clf_loss_focal = FocalLoss(gamma=1.5)\n","seg_bce = nn.BCEWithLogitsLoss()\n","\n","def dice_loss(pred, target, smooth=1.0):\n","    pred = torch.sigmoid(pred)\n","    num = 2 * (pred * target).sum() + smooth\n","    den = pred.sum() + target.sum() + smooth\n","    return 1 - (num / den)\n","\n","def seg_loss_fn(pred, mask):\n","    if pred.shape[-2:] != mask.shape[-2:]:\n","        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n","    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n"]},{"cell_type":"code","execution_count":23,"id":"11da7ed7","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:20.33183Z","iopub.status.busy":"2025-11-01T10:40:20.331616Z","iopub.status.idle":"2025-11-01T10:40:20.337948Z","shell.execute_reply":"2025-11-01T10:40:20.337238Z"},"papermill":{"duration":0.014001,"end_time":"2025-11-01T10:40:20.339028","exception":false,"start_time":"2025-11-01T10:40:20.325027","status":"completed"},"tags":[]},"outputs":[],"source":["#Supervised contrastive Loss\n","class SupConLoss(nn.Module):\n","    def __init__(self, temperature=0.07):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.cos = nn.CosineSimilarity(dim=-1)\n","    def forward(self, features, labels):\n","        # features: [N, D], labels: [N]\n","        device = features.device\n","        f = F.normalize(features, dim=1)\n","        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n","        labels = labels.contiguous().view(-1,1)\n","        mask = torch.eq(labels, labels.T).float().to(device)\n","        # remove diagonal\n","        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n","        logits = sim - logits_max.detach()\n","        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n","        denom = exp_logits.sum(1, keepdim=True)\n","        # for each i, positive samples are where mask==1 (excluding self)\n","        pos_mask = mask - torch.eye(len(features), device=device)\n","        pos_exp = (exp_logits * pos_mask).sum(1)\n","        # avoid divide by zero\n","        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n","        # average only across anchors that have positives\n","        valid = (pos_mask.sum(1) > 0).float()\n","        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n","        return loss\n","supcon_loss_fn = SupConLoss(temperature=0.07)"]},{"cell_type":"code","execution_count":24,"id":"e499fde2","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:20.351975Z","iopub.status.busy":"2025-11-01T10:40:20.351562Z","iopub.status.idle":"2025-11-01T10:40:20.355792Z","shell.execute_reply":"2025-11-01T10:40:20.355202Z"},"papermill":{"duration":0.011722,"end_time":"2025-11-01T10:40:20.356772","exception":false,"start_time":"2025-11-01T10:40:20.34505","status":"completed"},"tags":[]},"outputs":[],"source":["# Domain Adversarial: Gradient Reversal Layer (GRL)\n","\n","from torch.autograd import Function\n","class GradReverse(Function):\n","    @staticmethod\n","    def forward(ctx, x, l):\n","        ctx.l = l\n","        return x.view_as(x)\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return grad_output.neg() * ctx.l, None\n","\n","def grad_reverse(x, l=1.0):\n","    return GradReverse.apply(x, l)"]},{"cell_type":"code","execution_count":25,"id":"6f5699e4","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:20.369992Z","iopub.status.busy":"2025-11-01T10:40:20.369779Z","iopub.status.idle":"2025-11-01T10:40:20.384712Z","shell.execute_reply":"2025-11-01T10:40:20.383963Z"},"papermill":{"duration":0.022808,"end_time":"2025-11-01T10:40:20.385932","exception":false,"start_time":"2025-11-01T10:40:20.363124","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_19/876045050.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"]}],"source":["# Optimizer + scheduler + mixed precision + clipping\n","# -----------------------------\n","# param groups: smaller LR for backbones, larger for heads\n","backbone_params = []\n","head_params = []\n","for name, param in model.named_parameters():\n","    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n","        backbone_params.append(param)\n","    else:\n","        head_params.append(param)\n","\n","opt = torch.optim.AdamW([\n","    {'params': backbone_params, 'lr': LR * 0.2},\n","    {'params': head_params, 'lr': LR}\n","], lr=LR, weight_decay=1e-4)\n","\n","# warmup + cosine schedule\n","def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n","    def lr_lambda(epoch):\n","        if epoch < warmup_epochs:\n","            return float(epoch) / float(max(1.0, warmup_epochs))\n","        # cosine from warmup -> total\n","        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n","        return 0.5 * (1.0 + math.cos(math.pi * t))\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n","\n","scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","\n","# -----------------------------\n","# Mixup & CutMix helpers\n","# -----------------------------\n","def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)   # use builtin int\n","    cut_h = int(H * cut_rat)   # use builtin int\n","\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n","    lam = np.random.beta(alpha, alpha)\n","    idx = torch.randperm(x.size(0))\n","    mixed_x = lam * x + (1 - lam) * x[idx]\n","    y_a, y_b = y, y[idx]\n","    return mixed_x, y_a, y_b, lam\n","\n","def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n","    lam = np.random.beta(alpha, alpha)\n","    idx = torch.randperm(x.size(0))\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n","    new_x = x.clone()\n","    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n","    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n","    return new_x, y, y[idx], lam_adjusted\n"]},{"cell_type":"markdown","id":"214e6b17","metadata":{"papermill":{"duration":0.005742,"end_time":"2025-11-01T10:40:20.397887","exception":false,"start_time":"2025-11-01T10:40:20.392145","status":"completed"},"tags":[]},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"4c2aee9c","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:20.410574Z","iopub.status.busy":"2025-11-01T10:40:20.410377Z","iopub.status.idle":"2025-11-01T10:40:20.415582Z","shell.execute_reply":"2025-11-01T10:40:20.415091Z"},"papermill":{"duration":0.012886,"end_time":"2025-11-01T10:40:20.416634","exception":false,"start_time":"2025-11-01T10:40:20.403748","status":"completed"},"tags":[]},"outputs":[],"source":["# best_vf1 = 0.0\n","# best_epoch = 0\n","# patience_count = 0\n","\n","# def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n","#     # mix_info: (mode, y_a, y_b, lam) or None\n","#     if mix_info is None:\n","#         if use_focal:\n","#             return clf_loss_focal(logits, targets)\n","#         else:\n","#             return clf_loss_ce(logits, targets)\n","#     else:\n","#         # mixup/cutmix: soft labels\n","#         y_a, y_b, lam = mix_info\n","#         if use_focal:\n","#             # focal is not designed for soft labels; approximate by weighted CE\n","#             loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n","#         else:\n","#             loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n","#         return loss\n","# for epoch in range(1, EPOCHS+1):\n","#     # freeze/unfreeze strategy\n","#     if epoch <= FREEZE_EPOCHS:\n","#         # freeze early layers of backbones\n","#         for name, p in model.named_parameters():\n","#             if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n","#                 p.requires_grad = False\n","#     else:\n","#         for p in model.parameters():\n","#             p.requires_grad = True\n","\n","\n","#     model.train()\n","#     running_loss = 0.0\n","#     y_true, y_pred = [], []\n","#     n_batches = 0\n","\n","#     for weak_imgs, strong_imgs, labels, masks in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n","#         weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n","#         labels = labels.to(device)\n","#         if masks is not None:\n","#             masks = masks.to(device)\n","\n","#         # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n","#         imgs = weak_imgs\n","\n","#         # optionally apply mixup/cutmix on imgs (on weak view)\n","#         mix_info = None\n","#         rand = random.random()\n","#         if rand < PROB_MIXUP:\n","#             imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n","#             mix_info = (y_a.to(device), y_b.to(device), lam)\n","#         elif rand < PROB_MIXUP + PROB_CUTMIX:\n","#             imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n","#             mix_info = (y_a.to(device), y_b.to(device), lam)\n","\n","#         with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","#             out = model(imgs)  # returns logits, feat, seg, domain_logits\n","#             logits = out[\"logits\"]\n","#             feat = out[\"feat\"]\n","#             seg_out = out.get(\"seg\", None)\n","#             domain_logits = out.get(\"domain_logits\", None)\n","\n","#             # classification loss (label-smoothing or focal)\n","#             clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n","\n","#             # segmentation loss if available & mask present\n","#             seg_loss = 0.0\n","#             if USE_SEGMENTATION and (masks is not None):\n","#                 seg_pred = out[\"seg\"]\n","#                 seg_loss = seg_loss_fn(seg_pred, masks)\n","#             # supcon loss on features (use features from weak)\n","#             supcon_loss = supcon_loss_fn(feat, labels)\n","\n","#             # consistency: forward strong view and compare predictions\n","#             out_strong = model(strong_imgs)\n","#             logits_strong = out_strong[\"logits\"]\n","#             probs_weak = F.softmax(logits.detach(), dim=1)\n","#             probs_strong = F.softmax(logits_strong, dim=1)\n","#             # L2 between probability vectors (could be KL)\n","#             cons_loss = F.mse_loss(probs_weak, probs_strong)\n","#             # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n","#             # To support domain adaptation, user should provide target dataloader and stack batches with domain labels\n","#             dom_loss = 0.0\n","#             # (If domain labels are provided, compute dom logits after GRL: domain_logits_grl = domain_head(grad_reverse(feat, l)))\n","#             # then dom_loss = criterion(domain_logits_grl, domain_labels)\n","\n","#             total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n","\n","#         opt.zero_grad()\n","#         scaler.scale(total_loss).backward()\n","#         # gradient clipping\n","#         scaler.unscale_(opt)\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","#         scaler.step(opt)\n","#         scaler.update()\n","\n","#         running_loss += total_loss.item()\n","#         y_true.extend(labels.cpu().numpy())\n","#         y_pred.extend(logits.argmax(1).cpu().numpy())\n","#         n_batches += 1\n","\n","#     scheduler.step()\n","\n","#     # metrics\n","#     acc = accuracy_score(y_true, y_pred)\n","#     prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n","#     print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n","\n","#     # -------------------\n","#     # VALIDATION\n","#     # -------------------\n","#     model.eval()\n","#     val_y_true, val_y_pred = [], []\n","#     val_loss = 0.0\n","#     with torch.no_grad():\n","#         for weak_imgs, _, labels, masks in val_loader:\n","#             imgs = weak_imgs.to(device)\n","#             labels = labels.to(device)\n","#             if masks is not None:\n","#                 masks = masks.to(device)\n","\n","#             out = model(imgs)\n","#             logits = out[\"logits\"]\n","#             feat = out[\"feat\"]\n","#             seg_out = out.get(\"seg\", None)\n","#             loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n","#             if USE_SEGMENTATION and (masks is not None):\n","#                 loss += seg_loss_fn(seg_out, masks)\n","#             val_loss += loss.item()\n","\n","#             val_y_true.extend(labels.cpu().numpy())\n","#             val_y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","#     vacc = accuracy_score(val_y_true, val_y_pred)\n","#     vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n","#     print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n","\n","#     # early stopping & save best\n","#     if vf1 > best_vf1:\n","#         best_vf1 = vf1\n","#         best_epoch = epoch\n","#         torch.save({\n","#             \"epoch\": epoch,\n","#             \"model_state\": model.state_dict(),\n","#             \"opt_state\": opt.state_dict(),\n","#             \"best_vf1\": best_vf1\n","#         }, SAVE_PATH)\n","#         patience_count = 0\n","#         print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n","#     else:\n","#         patience_count += 1\n","#         if patience_count >= EARLY_STOPPING_PATIENCE:\n","#             print(\"Early stopping triggered.\")\n","#             break\n","\n","# print(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)\n"]},{"cell_type":"code","execution_count":27,"id":"17c8fb44","metadata":{"execution":{"iopub.execute_input":"2025-11-01T10:40:20.429755Z","iopub.status.busy":"2025-11-01T10:40:20.429514Z","iopub.status.idle":"2025-11-01T17:08:36.479219Z","shell.execute_reply":"2025-11-01T17:08:36.478245Z"},"papermill":{"duration":23297.628998,"end_time":"2025-11-01T17:08:38.051591","exception":false,"start_time":"2025-11-01T10:40:20.422593","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Train 1/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 1/20: 100%|██████████| 1769/1769 [16:14<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Train Loss: 3.8568 Acc: 0.5082 Prec: 0.5076 Rec: 0.5072 F1: 0.5010\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Val Loss: 2.2284 Acc: 0.4950 Prec: 0.4075 Rec: 0.4934 F1: 0.3409\n","Saved best model at epoch 1 (F1 0.3409)\n"]},{"name":"stderr","output_type":"stream","text":["Train 2/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 2/20: 100%|██████████| 1769/1769 [16:13<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 2] Train Loss: 2.6528 Acc: 0.7211 Prec: 0.7211 Rec: 0.7211 F1: 0.7211\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 2] Val Loss: 1.5041 Acc: 0.9699 Prec: 0.9707 Rec: 0.9699 F1: 0.9699\n","Saved best model at epoch 2 (F1 0.9699)\n"]},{"name":"stderr","output_type":"stream","text":["Train 3/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 3/20: 100%|██████████| 1769/1769 [16:12<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 3] Train Loss: 2.3164 Acc: 0.7989 Prec: 0.7992 Rec: 0.7987 F1: 0.7987\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 3] Val Loss: 1.5092 Acc: 0.9717 Prec: 0.9718 Rec: 0.9717 F1: 0.9717\n","Saved best model at epoch 3 (F1 0.9717)\n"]},{"name":"stderr","output_type":"stream","text":["Train 4/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 4/20: 100%|██████████| 1769/1769 [16:12<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 4] Train Loss: 2.2068 Acc: 0.8080 Prec: 0.8081 Rec: 0.8080 F1: 0.8080\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 4] Val Loss: 1.5011 Acc: 0.9753 Prec: 0.9761 Rec: 0.9754 F1: 0.9753\n","Saved best model at epoch 4 (F1 0.9753)\n"]},{"name":"stderr","output_type":"stream","text":["Train 5/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 5/20: 100%|██████████| 1769/1769 [16:11<00:00,  1.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 5] Train Loss: 2.1387 Acc: 0.8216 Prec: 0.8218 Rec: 0.8216 F1: 0.8216\n","[Epoch 5] Val Loss: 1.5259 Acc: 0.9675 Prec: 0.9691 Rec: 0.9676 F1: 0.9674\n"]},{"name":"stderr","output_type":"stream","text":["Train 6/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 6/20: 100%|██████████| 1769/1769 [16:13<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 6] Train Loss: 2.1208 Acc: 0.8177 Prec: 0.8179 Rec: 0.8175 F1: 0.8176\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 6] Val Loss: 1.5467 Acc: 0.9490 Prec: 0.9516 Rec: 0.9491 F1: 0.9489\n"]},{"name":"stderr","output_type":"stream","text":["Train 7/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 7/20: 100%|██████████| 1769/1769 [16:13<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 7] Train Loss: 2.0853 Acc: 0.8198 Prec: 0.8200 Rec: 0.8197 F1: 0.8197\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 7] Val Loss: 1.4579 Acc: 0.9631 Prec: 0.9646 Rec: 0.9632 F1: 0.9630\n"]},{"name":"stderr","output_type":"stream","text":["Train 8/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 8/20: 100%|██████████| 1769/1769 [16:14<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 8] Train Loss: 2.0062 Acc: 0.8342 Prec: 0.8344 Rec: 0.8341 F1: 0.8342\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 8] Val Loss: 1.4927 Acc: 0.9611 Prec: 0.9634 Rec: 0.9612 F1: 0.9611\n"]},{"name":"stderr","output_type":"stream","text":["Train 9/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 9/20: 100%|██████████| 1769/1769 [16:15<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 9] Train Loss: 1.9315 Acc: 0.8356 Prec: 0.8357 Rec: 0.8357 F1: 0.8356\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 9] Val Loss: 1.4618 Acc: 0.9656 Prec: 0.9675 Rec: 0.9657 F1: 0.9656\n"]},{"name":"stderr","output_type":"stream","text":["Train 10/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 10/20: 100%|██████████| 1769/1769 [16:15<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 10] Train Loss: 1.9062 Acc: 0.8337 Prec: 0.8337 Rec: 0.8338 F1: 0.8337\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 10] Val Loss: 1.5256 Acc: 0.9323 Prec: 0.9394 Rec: 0.9325 F1: 0.9321\n"]},{"name":"stderr","output_type":"stream","text":["Train 11/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 11/20: 100%|██████████| 1769/1769 [16:45<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 11] Train Loss: 1.8272 Acc: 0.8430 Prec: 0.8429 Rec: 0.8430 F1: 0.8430\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 11] Val Loss: 1.5258 Acc: 0.9355 Prec: 0.9407 Rec: 0.9357 F1: 0.9353\n"]},{"name":"stderr","output_type":"stream","text":["Train 12/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 12/20: 100%|██████████| 1769/1769 [16:46<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 12] Train Loss: 1.8174 Acc: 0.8389 Prec: 0.8390 Rec: 0.8389 F1: 0.8389\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 12] Val Loss: 1.4957 Acc: 0.9463 Prec: 0.9511 Rec: 0.9464 F1: 0.9461\n"]},{"name":"stderr","output_type":"stream","text":["Train 13/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 13/20: 100%|██████████| 1769/1769 [16:46<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 13] Train Loss: 1.8026 Acc: 0.8361 Prec: 0.8361 Rec: 0.8361 F1: 0.8361\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 13] Val Loss: 1.4791 Acc: 0.9650 Prec: 0.9668 Rec: 0.9651 F1: 0.9650\n"]},{"name":"stderr","output_type":"stream","text":["Train 14/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 14/20: 100%|██████████| 1769/1769 [16:47<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 14] Train Loss: 1.7212 Acc: 0.8440 Prec: 0.8440 Rec: 0.8440 F1: 0.8440\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 14] Val Loss: 1.5260 Acc: 0.9723 Prec: 0.9735 Rec: 0.9724 F1: 0.9723\n"]},{"name":"stderr","output_type":"stream","text":["Train 15/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 15/20: 100%|██████████| 1769/1769 [16:45<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 15] Train Loss: 1.6891 Acc: 0.8450 Prec: 0.8450 Rec: 0.8450 F1: 0.8450\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 15] Val Loss: 1.4951 Acc: 0.9662 Prec: 0.9681 Rec: 0.9664 F1: 0.9662\n"]},{"name":"stderr","output_type":"stream","text":["Train 16/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 16/20: 100%|██████████| 1769/1769 [16:46<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 16] Train Loss: 1.6370 Acc: 0.8534 Prec: 0.8534 Rec: 0.8534 F1: 0.8534\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 16] Val Loss: 1.4554 Acc: 0.9749 Prec: 0.9759 Rec: 0.9749 F1: 0.9749\n"]},{"name":"stderr","output_type":"stream","text":["Train 17/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 17/20: 100%|██████████| 1769/1769 [16:44<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 17] Train Loss: 1.6035 Acc: 0.8493 Prec: 0.8494 Rec: 0.8493 F1: 0.8493\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 17] Val Loss: 1.4484 Acc: 0.9809 Prec: 0.9815 Rec: 0.9810 F1: 0.9809\n","Saved best model at epoch 17 (F1 0.9809)\n"]},{"name":"stderr","output_type":"stream","text":["Train 18/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 18/20: 100%|██████████| 1769/1769 [16:43<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 18] Train Loss: 1.5899 Acc: 0.8552 Prec: 0.8551 Rec: 0.8553 F1: 0.8552\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 18] Val Loss: 1.4691 Acc: 0.9726 Prec: 0.9737 Rec: 0.9727 F1: 0.9726\n"]},{"name":"stderr","output_type":"stream","text":["Train 19/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 19/20: 100%|██████████| 1769/1769 [16:45<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 19] Train Loss: 1.6079 Acc: 0.8469 Prec: 0.8469 Rec: 0.8468 F1: 0.8468\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 19] Val Loss: 1.4536 Acc: 0.9753 Prec: 0.9762 Rec: 0.9754 F1: 0.9753\n"]},{"name":"stderr","output_type":"stream","text":["Train 20/20:   0%|          | 0/1769 [00:00<?, ?it/s]/tmp/ipykernel_19/3699574605.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 20/20: 100%|██████████| 1769/1769 [16:44<00:00,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["[Epoch 20] Train Loss: 1.5354 Acc: 0.8549 Prec: 0.8548 Rec: 0.8550 F1: 0.8549\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 20] Val Loss: 1.4583 Acc: 0.9758 Prec: 0.9766 Rec: 0.9759 F1: 0.9758\n","Training finished. Best val F1: 0.9809225677509259 at epoch 17\n"]}],"source":["best_vf1 = 0.0\n","best_epoch = 0\n","patience_count = 0\n","\n","def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n","    # mix_info: (mode, y_a, y_b, lam) or None\n","    if mix_info is None:\n","        if use_focal:\n","            return clf_loss_focal(logits, targets)\n","        else:\n","            return clf_loss_ce(logits, targets)\n","    else:\n","        # mixup/cutmix: soft labels\n","        y_a, y_b, lam = mix_info\n","        if use_focal:\n","            # focal is not designed for soft labels; approximate by weighted CE\n","            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n","        else:\n","            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n","        return loss\n","\n","# --- Ensure ACCUMULATION_STEPS is defined (e.g., in Code Cell 5) ---\n","# For this code to run, you MUST have ACCUMULATION_STEPS defined globally (e.g., set to 4 in Cell 5)\n","# -------------------------------------------------------------------\n","\n","for epoch in range(1, EPOCHS+1):\n","    # freeze/unfreeze strategy\n","    if epoch <= FREEZE_EPOCHS:\n","        # freeze early layers of backbones\n","        for name, p in model.named_parameters():\n","            if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n","                p.requires_grad = False\n","    else:\n","        for p in model.parameters():\n","            p.requires_grad = True\n","\n","    model.train()\n","    running_loss = 0.0\n","    y_true, y_pred = [], []\n","    n_batches = 0\n","\n","    # 1. Initialize zero_grad at the start of the epoch\n","    opt.zero_grad() \n","    \n","    for i, (weak_imgs, strong_imgs, labels, masks) in enumerate(tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")):\n","        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n","        labels = labels.to(device)\n","        if masks is not None:\n","            masks = masks.to(device)\n","\n","        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n","        imgs = weak_imgs\n","\n","        # optionally apply mixup/cutmix on imgs (on weak view)\n","        mix_info = None\n","        rand = random.random()\n","        if rand < PROB_MIXUP:\n","            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n","            mix_info = (y_a.to(device), y_b.to(device), lam)\n","        elif rand < PROB_MIXUP + PROB_CUTMIX:\n","            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n","            mix_info = (y_a.to(device), y_b.to(device), lam)\n","\n","        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","            out = model(imgs)  # returns logits, feat, seg, domain_logits\n","            logits = out[\"logits\"]\n","            feat = out[\"feat\"]\n","            seg_out = out.get(\"seg\", None)\n","            domain_logits = out.get(\"domain_logits\", None)\n","\n","            # classification loss (label-smoothing or focal)\n","            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n","\n","            # segmentation loss if available & mask present\n","            seg_loss = 0.0\n","            if USE_SEGMENTATION and (masks is not None):\n","                seg_pred = out[\"seg\"]\n","                seg_loss = seg_loss_fn(seg_pred, masks)\n","            # supcon loss on features (use features from weak)\n","            supcon_loss = supcon_loss_fn(feat, labels)\n","\n","            # consistency: forward strong view and compare predictions\n","            out_strong = model(strong_imgs)\n","            logits_strong = out_strong[\"logits\"]\n","            probs_weak = F.softmax(logits.detach(), dim=1)\n","            probs_strong = F.softmax(logits_strong, dim=1)\n","            # L2 between probability vectors (could be KL)\n","            cons_loss = F.mse_loss(probs_weak, probs_strong)\n","            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n","            dom_loss = 0.0\n","\n","            total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n","\n","            # 2. Scale the loss by accumulation steps to average the gradients\n","            total_loss = total_loss / ACCUMULATION_STEPS \n","\n","        # Perform backward pass (gradients are accumulated until step is called)\n","        scaler.scale(total_loss).backward()\n","\n","        # 3. Optimizer step only every ACCUMULATION_STEPS batches\n","        if (i + 1) % ACCUMULATION_STEPS == 0:\n","            # gradient clipping before step\n","            scaler.unscale_(opt)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            scaler.step(opt)\n","            scaler.update()\n","            opt.zero_grad() # Prepare for next accumulation cycle\n","\n","        running_loss += total_loss.item() * ACCUMULATION_STEPS # Re-scale back for correct loss tracking\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(logits.argmax(1).cpu().numpy())\n","        n_batches += 1\n","\n","    # 4. Take a final step if there are remaining gradients (i.e., last batch was not a multiple of ACCUMULATION_STEPS)\n","    if n_batches % ACCUMULATION_STEPS != 0:\n","        scaler.unscale_(opt)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        scaler.step(opt)\n","        scaler.update()\n","        opt.zero_grad()\n","\n","    scheduler.step()\n","\n","    # metrics (rest of the code remains the same)\n","    acc = accuracy_score(y_true, y_pred)\n","    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n","    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n","\n","    # -------------------\n","    # VALIDATION\n","    # -------------------\n","    model.eval()\n","    val_y_true, val_y_pred = [], []\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for weak_imgs, _, labels, masks in val_loader:\n","            imgs = weak_imgs.to(device)\n","            labels = labels.to(device)\n","            if masks is not None:\n","                masks = masks.to(device)\n","\n","            out = model(imgs)\n","            logits = out[\"logits\"]\n","            feat = out[\"feat\"]\n","            seg_out = out.get(\"seg\", None)\n","            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n","            if USE_SEGMENTATION and (masks is not None):\n","                loss += seg_loss_fn(seg_out, masks)\n","            val_loss += loss.item()\n","\n","            val_y_true.extend(labels.cpu().numpy())\n","            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","    vacc = accuracy_score(val_y_true, val_y_pred)\n","    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n","    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n","\n","    # early stopping & save best\n","    if vf1 > best_vf1:\n","        best_vf1 = vf1\n","        best_epoch = epoch\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"model_state\": model.state_dict(),\n","            \"opt_state\": opt.state_dict(),\n","            \"best_vf1\": best_vf1\n","        }, SAVE_PATH)\n","        patience_count = 0\n","        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n","    else:\n","        patience_count += 1\n","        if patience_count >= EARLY_STOPPING_PATIENCE:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","print(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)"]},{"cell_type":"code","execution_count":28,"id":"89f366c7","metadata":{"execution":{"iopub.execute_input":"2025-11-01T17:08:40.988058Z","iopub.status.busy":"2025-11-01T17:08:40.987753Z","iopub.status.idle":"2025-11-01T17:08:40.994308Z","shell.execute_reply":"2025-11-01T17:08:40.993544Z"},"papermill":{"duration":1.412793,"end_time":"2025-11-01T17:08:40.995426","exception":false,"start_time":"2025-11-01T17:08:39.582633","status":"completed"},"tags":[]},"outputs":[],"source":["# Test-time augmentation (TTA) helper\n","# -----------------------------\n","def tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n","    model.eval()\n","    logits_accum = None\n","    with torch.no_grad():\n","        for s in scales:\n","            tf = transforms.Compose([\n","                transforms.Resize((s, s)),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ])\n","            x = tf(img_pil).unsqueeze(0).to(device)\n","            out = model(x)\n","            logits = out[\"logits\"]\n","            if flip:\n","                x_f = torch.flip(x, dims=[3])\n","                logits_f = model(x_f)[\"logits\"]\n","                logits = (logits + logits_f) / 2.0\n","            if logits_accum is None:\n","                logits_accum = logits\n","            else:\n","                logits_accum += logits\n","    logits_accum /= len(scales)\n","    return logits_accum"]},{"cell_type":"code","execution_count":29,"id":"7accdebb","metadata":{"execution":{"iopub.execute_input":"2025-11-01T17:08:44.054003Z","iopub.status.busy":"2025-11-01T17:08:44.053369Z","iopub.status.idle":"2025-11-01T17:08:44.063307Z","shell.execute_reply":"2025-11-01T17:08:44.062585Z"},"papermill":{"duration":1.544338,"end_time":"2025-11-01T17:08:44.064443","exception":false,"start_time":"2025-11-01T17:08:42.520105","status":"completed"},"tags":[]},"outputs":[],"source":["# Grad-CAM helper (very simple)\n","# -----------------------------\n","def get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n","    \"\"\"\n","    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n","    Returns upsampled heatmap (H,W) normalized in [0,1].\n","    \"\"\"\n","    model.eval()\n","    # find layer\n","    target_module = None\n","    for name, module in model.named_modules():\n","        if name == layer_name:\n","            target_module = module\n","            break\n","    if target_module is None:\n","        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n","\n","    activations = []\n","    gradients = []\n","\n","    def forward_hook(module, input, output):\n","        activations.append(output.detach())\n","    def backward_hook(module, grad_in, grad_out):\n","        gradients.append(grad_out[0].detach())\n","\n","    h1 = target_module.register_forward_hook(forward_hook)\n","    h2 = target_module.register_full_backward_hook(backward_hook)\n","\n","    out = model(input_tensor)\n","    logits = out[\"logits\"]\n","    if target_class is None:\n","        target_class = logits.argmax(1).item()\n","    loss = logits[:, target_class].sum()\n","    model.zero_grad()\n","    loss.backward(retain_graph=True)\n","\n","    act = activations[0]  # [B,C,H,W]\n","    grad = gradients[0]   # [B,C,H,W]\n","    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n","    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n","    cam = F.relu(cam)\n","    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n","    cam = cam.squeeze().cpu().numpy()\n","    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n","    h1.remove(); h2.remove()\n","    return cam"]},{"cell_type":"code","execution_count":null,"id":"0be5b46e","metadata":{"papermill":{"duration":1.498193,"end_time":"2025-11-01T17:08:46.955404","exception":false,"start_time":"2025-11-01T17:08:45.457211","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2932761,"sourceId":5051281,"sourceType":"datasetVersion"},{"datasetId":8580489,"sourceId":13514489,"sourceType":"datasetVersion"},{"datasetId":8582404,"sourceId":13517101,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":23441.184193,"end_time":"2025-11-01T17:08:51.357944","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-01T10:38:10.173751","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"026449b0aa2e4307a41826ae1648c7a2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c47d18df934c9f8be61366951b14cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3ae6a5e8fbc1444ebb2e59a7cb23ad50","max":114286722.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_18c2eb7f01dd477593e48de3e5134dc1","tabbable":null,"tooltip":null,"value":114286722.0}},"18c2eb7f01dd477593e48de3e5134dc1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a19cc2dac2445bc898e55752a0e46dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2d197636129e4af8b3dea4d7f19ac2b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_026449b0aa2e4307a41826ae1648c7a2","placeholder":"​","style":"IPY_MODEL_850f1c261c3344d3a6f5e8c202a0e6d7","tabbable":null,"tooltip":null,"value":" 114M/114M [00:01&lt;00:00, 109MB/s]"}},"3ae6a5e8fbc1444ebb2e59a7cb23ad50":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cc570bdb0654ff3a543ee9b359e758d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cc333c262b04d39b6b032f9be1ed331":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9dcce90045e4bbfb4e02ea32727da31","IPY_MODEL_16c47d18df934c9f8be61366951b14cc","IPY_MODEL_2d197636129e4af8b3dea4d7f19ac2b4"],"layout":"IPY_MODEL_9ed15019c85848d488f741c26622f966","tabbable":null,"tooltip":null}},"850f1c261c3344d3a6f5e8c202a0e6d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9ed15019c85848d488f741c26622f966":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9dcce90045e4bbfb4e02ea32727da31":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4cc570bdb0654ff3a543ee9b359e758d","placeholder":"​","style":"IPY_MODEL_1a19cc2dac2445bc898e55752a0e46dc","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}