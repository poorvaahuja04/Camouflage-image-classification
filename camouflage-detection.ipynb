{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":5051281,"sourceType":"datasetVersion","datasetId":2932761}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.431199Z","iopub.execute_input":"2025-09-09T12:12:19.431430Z","iopub.status.idle":"2025-09-09T12:12:19.443657Z","shell.execute_reply.started":"2025-09-09T12:12:19.431406Z","shell.execute_reply":"2025-09-09T12:12:19.436957Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.445728Z","iopub.execute_input":"2025-09-09T12:12:19.445960Z","iopub.status.idle":"2025-09-09T12:12:19.458326Z","shell.execute_reply.started":"2025-09-09T12:12:19.445937Z","shell.execute_reply":"2025-09-09T12:12:19.453351Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom torchvision import datasets, transforms, models\nimport timm\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.459355Z","iopub.execute_input":"2025-09-09T12:12:19.459582Z","iopub.status.idle":"2025-09-09T12:12:19.473984Z","shell.execute_reply.started":"2025-09-09T12:12:19.459561Z","shell.execute_reply":"2025-09-09T12:12:19.469118Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.475194Z","iopub.execute_input":"2025-09-09T12:12:19.475435Z","iopub.status.idle":"2025-09-09T12:12:19.485558Z","shell.execute_reply.started":"2025-09-09T12:12:19.475413Z","shell.execute_reply":"2025-09-09T12:12:19.480592Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nseed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.486461Z","iopub.execute_input":"2025-09-09T12:12:19.486686Z","iopub.status.idle":"2025-09-09T12:12:19.503354Z","shell.execute_reply.started":"2025-09-09T12:12:19.486664Z","shell.execute_reply":"2025-09-09T12:12:19.497288Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Config/hyperparameters","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS = 10\n\ntrain_dir = \"/kaggle/input/cod10k/COD10K-v3/Train\"\ntest_dir = \"/kaggle/input/cod10k/COD10K-v3/Test\"\n\ntrain_txt = os.path.join(train_dir, \"CAM-NonCAM_Instance_Train.txt\")\ntest_txt = os.path.join(test_dir, \"CAM-NonCAM_Instance_Test.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.511329Z","iopub.execute_input":"2025-09-09T12:12:19.511554Z","iopub.status.idle":"2025-09-09T12:12:19.526754Z","shell.execute_reply.started":"2025-09-09T12:12:19.511531Z","shell.execute_reply":"2025-09-09T12:12:19.520349Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class COD10KDataset(Dataset):\n    def __init__(self, root_dir, txt_files, transform=None):\n        \"\"\"\n        txt_files: list of txt files (e.g. [CAM_train.txt, NonCAM_train.txt])\n        \"\"\"\n        self.root_dir = root_dir\n        self.img_dir = os.path.join(root_dir, \"Image\")\n        self.transform = transform\n\n        self.samples = []\n        for txt_file, label_value in txt_files:\n            with open(txt_file, \"r\") as f:\n                for line in f:\n                    img_name = line.strip().split()[0]   # first column = filename\n                    img_path = os.path.join(self.img_dir, img_name)\n                    if os.path.exists(img_path):\n                        self.samples.append((img_name, label_value))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_name, label = self.samples[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.529617Z","iopub.execute_input":"2025-09-09T12:12:19.529887Z","iopub.status.idle":"2025-09-09T12:12:19.563277Z","shell.execute_reply.started":"2025-09-09T12:12:19.529864Z","shell.execute_reply":"2025-09-09T12:12:19.557288Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# =============================\n# 3. Transforms\n# =============================\ntrain_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(30),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\nval_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:19.565080Z","iopub.execute_input":"2025-09-09T12:12:19.565324Z","iopub.status.idle":"2025-09-09T12:12:19.591217Z","shell.execute_reply.started":"2025-09-09T12:12:19.565302Z","shell.execute_reply":"2025-09-09T12:12:19.587793Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class DenseNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        base = models.densenet201(pretrained=pretrained).features\n        self.features = base\n    def forward(self, x):\n        feats = []\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n                feats.append(x)\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:39.403585Z","iopub.execute_input":"2025-09-09T12:12:39.403929Z","iopub.status.idle":"2025-09-09T12:12:39.415534Z","shell.execute_reply.started":"2025-09-09T12:12:39.403903Z","shell.execute_reply":"2025-09-09T12:12:39.410744Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class MobileNetExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        m = models.mobilenet_v3_large(pretrained=pretrained)\n        self.features = m.features\n    def forward(self, x):\n        feats = []\n        out = x\n        for i, layer in enumerate(self.features):\n            out = layer(out)\n            if i in (2,5,9,12):  # 4 scales\n                feats.append(out)\n        if len(feats)<4: feats.append(out)\n        return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:12:51.902882Z","iopub.execute_input":"2025-09-09T12:12:51.903269Z","iopub.status.idle":"2025-09-09T12:12:51.917011Z","shell.execute_reply.started":"2025-09-09T12:12:51.903237Z","shell.execute_reply":"2025-09-09T12:12:51.910475Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# CBAM-lite (SE + light spatial)","metadata":{}},{"cell_type":"code","source":"class CBAMlite(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, max(channels//reduction, 4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(channels//reduction, 4), channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial = nn.Sequential(\n            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels, 1, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.se(x) * self.spatial(x)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:13:22.433384Z","iopub.execute_input":"2025-09-09T12:13:22.433730Z","iopub.status.idle":"2025-09-09T12:13:22.446039Z","shell.execute_reply.started":"2025-09-09T12:13:22.433703Z","shell.execute_reply":"2025-09-09T12:13:22.441321Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class GatedFusion(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g_fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, max(dim//4,4), 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(max(dim//4,4), dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, H, X):\n        g = self.g_fc(H)\n        return g * H + (1-g) * X\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:13:38.132896Z","iopub.execute_input":"2025-09-09T12:13:38.133259Z","iopub.status.idle":"2025-09-09T12:13:38.145313Z","shell.execute_reply.started":"2025-09-09T12:13:38.133231Z","shell.execute_reply":"2025-09-09T12:13:38.140314Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Backbone extractors","metadata":{}},{"cell_type":"code","source":"def probe_backbones(img_size=224):\n    netA = DenseNetExtractor(True).to(device).eval()\n    netB = MobileNetExtractor(True).to(device).eval()\n    with torch.no_grad():\n        dummy = torch.randn(1,3,img_size,img_size).to(device)\n        featsA = netA(dummy)\n        featsB = netB(dummy)\n    return [f.shape[1] for f in featsA], [f.shape[1] for f in featsB]\n\ndense_chs, mobilenet_chs = probe_backbones()\nprint(\"DenseNet channels:\", dense_chs)\nprint(\"MobileNet channels:\", mobilenet_chs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:15:55.587564Z","iopub.execute_input":"2025-09-09T12:15:55.587939Z","iopub.status.idle":"2025-09-09T12:15:57.655016Z","shell.execute_reply.started":"2025-09-09T12:15:55.587911Z","shell.execute_reply":"2025-09-09T12:15:57.647386Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n100%|██████████| 77.4M/77.4M [00:00<00:00, 175MB/s] \n/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 131MB/s] \n","output_type":"stream"},{"name":"stdout","text":"DenseNet channels: [256, 512, 1792, 1920]\nMobileNet channels: [24, 40, 80, 112]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Main AGHCT-DA model","metadata":{}},{"cell_type":"code","source":"class DynamicFusionModel(nn.Module):\n    def __init__(self, num_classes, dense_chs, mobilenet_chs, d=256):\n        super().__init__()\n        self.backA = DenseNetExtractor(True)\n        self.backB = MobileNetExtractor(True)\n        L = min(len(dense_chs), len(mobilenet_chs))\n        self.L = L\n\n        self.alignA = nn.ModuleList([nn.Conv2d(in_c,d,1) for in_c in dense_chs[:L]])\n        self.alignB = nn.ModuleList([nn.Conv2d(in_c,d,1) for in_c in mobilenet_chs[:L]])\n        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n\n        self.fusion_reduce = nn.Conv2d(d*L, d, 1)\n        self.classifier = nn.Sequential(\n            nn.Linear(d,512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512,128), nn.ReLU(), nn.Dropout(0.5),\n            nn.Linear(128,num_classes)\n        )\n\n    def forward(self, x):\n        featsA = self.backA(x)\n        featsB = self.backB(x)\n        fused_feats=[]\n        for fA,convA,cbamA,fB,convB,cbamB,gate in zip(\n            featsA[:self.L], self.alignA, self.cbamA,\n            featsB[:self.L], self.alignB, self.cbamB,\n            self.gates):\n            a = cbamA(convA(fA))\n            b = cbamB(convB(fB))\n            if b.shape[2:] != a.shape[2:]:\n                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n            fused_feats.append(gate(a,b))\n        target = fused_feats[-1]\n        upsampled=[F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False)\n                   if f.shape[2:]!=target.shape[2:] else f for f in fused_feats]\n        concat = torch.cat(upsampled, dim=1)\n        fused = self.fusion_reduce(concat)\n        z = F.adaptive_avg_pool2d(fused,1).view(fused.shape[0],-1)\n        return self.classifier(z)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:16:09.092173Z","iopub.execute_input":"2025-09-09T12:16:09.092597Z","iopub.status.idle":"2025-09-09T12:16:09.113968Z","shell.execute_reply.started":"2025-09-09T12:16:09.092555Z","shell.execute_reply":"2025-09-09T12:16:09.106358Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# training","metadata":{}},{"cell_type":"code","source":"info_dir = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n\ntrain_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\ntrain_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n\ntest_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\ntest_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n\n# Create datasets\ntrain_ds = COD10KDataset(train_dir, [(train_cam_txt, 1), (train_noncam_txt, 0)], transform=train_tf)\nval_ds   = COD10KDataset(test_dir,  [(test_cam_txt, 1),  (test_noncam_txt, 0)],  transform=val_tf)\n\nnum_classes = 2\nprint(\"Train samples:\", len(train_ds), \" Test samples:\", len(val_ds))\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:16:13.130335Z","iopub.execute_input":"2025-09-09T12:16:13.130699Z","iopub.status.idle":"2025-09-09T12:16:31.491860Z","shell.execute_reply.started":"2025-09-09T12:16:13.130668Z","shell.execute_reply":"2025-09-09T12:16:31.487158Z"}},"outputs":[{"name":"stdout","text":"Train samples: 5998  Test samples: 4000\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model = DynamicFusionModel(num_classes=num_classes, dense_chs=dense_chs, mobilenet_chs=mobilenet_chs).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nloss_fn = nn.CrossEntropyLoss()\n\nfor epoch in range(EPOCHS):\n    model.train(); total_loss=0\n    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n        logits = model(imgs)\n        loss = loss_fn(logits, labels)\n\n        optimizer.zero_grad(); loss.backward(); optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1} Train Loss: {total_loss/len(train_loader):.4f}\")\n\n    # validation\n    model.eval(); correct,total=0,0\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n            preds = model(imgs).argmax(dim=1)\n            correct += (preds==labels).sum().item()\n            total += labels.size(0)\n    print(f\"Validation Accuracy: {100*correct/total:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T12:17:23.550007Z","iopub.execute_input":"2025-09-09T12:17:23.550429Z","execution_failed":"2025-09-09T14:27:14.802Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10:   0%|          | 0/375 [00:00<?, ?it/s]/tmp/ipykernel_10/2690994741.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\nEpoch 1/10: 100%|██████████| 375/375 [18:49<00:00,  3.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Train Loss: 0.4088\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_10/2690994741.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  imgs, labels = imgs.to(device), torch.tensor(labels).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 88.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 375/375 [19:32<00:00,  3.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Train Loss: 0.3072\nValidation Accuracy: 88.92%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 375/375 [19:30<00:00,  3.12s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Train Loss: 0.2623\nValidation Accuracy: 89.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 375/375 [20:19<00:00,  3.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Train Loss: 0.2435\nValidation Accuracy: 89.60%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 375/375 [20:57<00:00,  3.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Train Loss: 0.2138\nValidation Accuracy: 89.38%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10:   3%|▎         | 13/375 [01:32<21:04,  3.49s/it] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}