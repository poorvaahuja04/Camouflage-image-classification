{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anuhskaa/camouflage-improvement-research-2?scriptVersionId=271776260\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"813caf16","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-10-29T10:07:37.246266Z","iopub.status.busy":"2025-10-29T10:07:37.245611Z","iopub.status.idle":"2025-10-29T10:07:37.274595Z","shell.execute_reply":"2025-10-29T10:07:37.274094Z"},"papermill":{"duration":0.037268,"end_time":"2025-10-29T10:07:37.275656","exception":false,"start_time":"2025-10-29T10:07:37.238388","status":"completed"},"tags":[]},"outputs":[],"source":["import os, random, math, time\n","from pathlib import Path\n","from tqdm import tqdm\n","import numpy as np\n","from PIL import Image"]},{"cell_type":"code","execution_count":2,"id":"09c92441","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:37.286893Z","iopub.status.busy":"2025-10-29T10:07:37.286518Z","iopub.status.idle":"2025-10-29T10:07:49.187072Z","shell.execute_reply":"2025-10-29T10:07:49.186516Z"},"papermill":{"duration":11.907394,"end_time":"2025-10-29T10:07:49.188425","exception":false,"start_time":"2025-10-29T10:07:37.281031","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torchvision import transforms, models\n","from torchvision.transforms import RandAugment\n","import timm"]},{"cell_type":"code","execution_count":3,"id":"926936dc","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:49.199471Z","iopub.status.busy":"2025-10-29T10:07:49.199259Z","iopub.status.idle":"2025-10-29T10:07:50.351826Z","shell.execute_reply":"2025-10-29T10:07:50.351279Z"},"papermill":{"duration":1.159301,"end_time":"2025-10-29T10:07:50.353059","exception":false,"start_time":"2025-10-29T10:07:49.193758","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n","from collections import Counter"]},{"cell_type":"code","execution_count":4,"id":"c2dbbe11","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.36453Z","iopub.status.busy":"2025-10-29T10:07:50.364201Z","iopub.status.idle":"2025-10-29T10:07:50.452243Z","shell.execute_reply":"2025-10-29T10:07:50.451422Z"},"papermill":{"duration":0.094937,"end_time":"2025-10-29T10:07:50.453337","exception":false,"start_time":"2025-10-29T10:07:50.3584","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if device == \"cuda\": torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"code","execution_count":5,"id":"4978071f","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.464419Z","iopub.status.busy":"2025-10-29T10:07:50.464178Z","iopub.status.idle":"2025-10-29T10:07:50.468559Z","shell.execute_reply":"2025-10-29T10:07:50.46801Z"},"papermill":{"duration":0.010952,"end_time":"2025-10-29T10:07:50.469534","exception":false,"start_time":"2025-10-29T10:07:50.458582","status":"completed"},"tags":[]},"outputs":[],"source":["IMG_SIZE = 224\n","BATCH_SIZE = 8          # adjust if OOM\n","EPOCHS = 20\n","NUM_WORKERS = 0         # set 0 if worker issues on Kaggle\n","LR = 3e-4\n","LABEL_SMOOTH = 0.1\n","SAVE_PATH = \"best_model.pth\"\n","USE_SEGMENTATION = True\n","\n","# Loss weights from PDF suggestion\n","ALPHA_DOM = 0.5\n","BETA_SUPCON = 0.2\n","ETA_CONS = 0.1\n","\n","# Mixup/CutMix probabilities and alphas\n","PROB_MIXUP = 0.5\n","PROB_CUTMIX = 0.5\n","MIXUP_ALPHA = 0.2\n","CUTMIX_ALPHA = 1.0\n","\n","# warmup epochs\n","WARMUP_EPOCHS = 5\n","\n","# early stopping\n","EARLY_STOPPING_PATIENCE = 8\n","FREEZE_EPOCHS = 10"]},{"cell_type":"code","execution_count":6,"id":"4530e605","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.480573Z","iopub.status.busy":"2025-10-29T10:07:50.480016Z","iopub.status.idle":"2025-10-29T10:07:50.483734Z","shell.execute_reply":"2025-10-29T10:07:50.483259Z"},"papermill":{"duration":0.010296,"end_time":"2025-10-29T10:07:50.484736","exception":false,"start_time":"2025-10-29T10:07:50.47444","status":"completed"},"tags":[]},"outputs":[],"source":["info_dir  = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n","train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train/Image\"\n","test_dir_cod  = \"/kaggle/input/cod10k/COD10K-v3/Test/Image\"\n","\n","# these exist in Info/\n","combined_train_cam  = os.path.join(info_dir, \"CAM_train.txt\")\n","combined_train_noncam  = os.path.join(info_dir, \"NonCAM_train.txt\")\n","combined_test_cam = os.path.join(info_dir, \"CAM_test.txt\")\n","combined_test_noncam = os.path.join(info_dir, \"NonCAM_test.txt\")"]},{"cell_type":"code","execution_count":7,"id":"157d1866","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.495977Z","iopub.status.busy":"2025-10-29T10:07:50.49577Z","iopub.status.idle":"2025-10-29T10:07:50.499192Z","shell.execute_reply":"2025-10-29T10:07:50.498654Z"},"papermill":{"duration":0.009932,"end_time":"2025-10-29T10:07:50.500285","exception":false,"start_time":"2025-10-29T10:07:50.490353","status":"completed"},"tags":[]},"outputs":[],"source":["info_dir2 =\"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n","train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n","train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n","test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n","test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")"]},{"cell_type":"code","execution_count":8,"id":"e795b2b0","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.510685Z","iopub.status.busy":"2025-10-29T10:07:50.510493Z","iopub.status.idle":"2025-10-29T10:07:50.513596Z","shell.execute_reply":"2025-10-29T10:07:50.513087Z"},"papermill":{"duration":0.00939,"end_time":"2025-10-29T10:07:50.514591","exception":false,"start_time":"2025-10-29T10:07:50.505201","status":"completed"},"tags":[]},"outputs":[],"source":["train_dir_camo = {\n","    \"cam\": \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage/Images/Train\",\n","    \"noncam\": \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage/Images/Train\"\n","}\n","test_dir_camo = {\n","    \"cam\": \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage/Images/Test\",\n","    \"noncam\": \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage/Images/Test\"\n","}"]},{"cell_type":"code","execution_count":9,"id":"63450c84","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.525558Z","iopub.status.busy":"2025-10-29T10:07:50.525013Z","iopub.status.idle":"2025-10-29T10:07:50.528526Z","shell.execute_reply":"2025-10-29T10:07:50.527879Z"},"papermill":{"duration":0.010062,"end_time":"2025-10-29T10:07:50.529592","exception":false,"start_time":"2025-10-29T10:07:50.51953","status":"completed"},"tags":[]},"outputs":[],"source":["train_dir = [train_dir_cod, train_dir_camo[\"cam\"], train_dir_camo[\"noncam\"]]\n","test_dir  = [test_dir_cod, test_dir_camo[\"cam\"], test_dir_camo[\"noncam\"]]\n"]},{"cell_type":"code","execution_count":10,"id":"e7d68bcc","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.539935Z","iopub.status.busy":"2025-10-29T10:07:50.539767Z","iopub.status.idle":"2025-10-29T10:07:50.637631Z","shell.execute_reply":"2025-10-29T10:07:50.637146Z"},"papermill":{"duration":0.104177,"end_time":"2025-10-29T10:07:50.638636","exception":false,"start_time":"2025-10-29T10:07:50.534459","status":"completed"},"tags":[]},"outputs":[],"source":["# Combine train files\n","with open(combined_train_cam) as f1, open(train_cam_txt2) as f2:\n","    train_cam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","with open(combined_train_noncam) as f1, open(train_noncam_txt2) as f2:\n","    train_noncam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","# Combine test files\n","with open(combined_test_cam) as f1, open(test_cam_txt2) as f2:\n","    test_cam_txt = f1.read().splitlines() + f2.read().splitlines()\n","\n","with open(combined_test_noncam) as f1, open(test_noncam_txt2) as f2:\n","    test_noncam_txt= f1.read().splitlines() + f2.read().splitlines()\n"]},{"cell_type":"markdown","id":"2745faf5","metadata":{"papermill":{"duration":0.004792,"end_time":"2025-10-29T10:07:50.648553","exception":false,"start_time":"2025-10-29T10:07:50.643761","status":"completed"},"tags":[]},"source":["Noise + Transform"]},{"cell_type":"code","execution_count":11,"id":"b4b31b66","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.659002Z","iopub.status.busy":"2025-10-29T10:07:50.658806Z","iopub.status.idle":"2025-10-29T10:07:50.665806Z","shell.execute_reply":"2025-10-29T10:07:50.665145Z"},"papermill":{"duration":0.013351,"end_time":"2025-10-29T10:07:50.66681","exception":false,"start_time":"2025-10-29T10:07:50.653459","status":"completed"},"tags":[]},"outputs":[],"source":["class AddGaussianNoise(object):\n","    def __init__(self, mean=0., std=0.05):\n","        self.mean = mean\n","        self.std = std\n","    def __call__(self, tensor):\n","        noise = torch.randn(tensor.size()) * self.std + self.mean\n","        noisy_tensor = tensor + noise\n","        return torch.clamp(noisy_tensor, 0., 1.)\n","    def __repr__(self):\n","        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'\n","\n","weak_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(0., 0.02),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","strong_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n","    RandAugment(num_ops=2, magnitude=9),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(30),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(0., 0.05),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","val_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n"]},{"cell_type":"code","execution_count":12,"id":"e44587fe","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.677603Z","iopub.status.busy":"2025-10-29T10:07:50.677399Z","iopub.status.idle":"2025-10-29T10:07:50.692595Z","shell.execute_reply":"2025-10-29T10:07:50.69208Z"},"papermill":{"duration":0.021869,"end_time":"2025-10-29T10:07:50.693587","exception":false,"start_time":"2025-10-29T10:07:50.671718","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","from collections import Counter\n","from torch.utils.data import Dataset, WeightedRandomSampler\n","from PIL import Image\n","import torch\n","import numpy as np\n","from torchvision import transforms\n","\n","# NOTE: IMG_SIZE should be defined globally (e.g., 224 for Swin compatibility)\n","# IMG_SIZE = 352 # Use your defined IMG_SIZE\n","\n","class COD10KDataset(Dataset):\n","    \"\"\"\n","    Dataset supporting COD10K + other datasets (like CAMO).\n","    Each line in txt_file(s) must contain:\n","        <filename> <label>\n","    or just <filename> (then label inferred by name or folder).\n","    \"\"\"\n","\n","    def __init__(self, root_dirs, txt_files, weak_transform=None, strong_transform=None, use_masks=True):\n","        # root_dir can be a list of directories\n","        if isinstance(root_dirs, str):\n","            root_dirs = [root_dirs]\n","        self.root_dirs = root_dirs\n","        self.weak_transform = weak_transform\n","        self.strong_transform = strong_transform\n","        self.use_masks = use_masks\n","        self.samples = []\n","        \n","        # txt_file can also be a list\n","        if isinstance(txt_files, str):\n","            txt_files = [txt_files]\n","\n","        all_lines = []\n","        for t in txt_files:\n","            if not os.path.exists(t):\n","                raise RuntimeError(f\"TXT file not found: {t}\")\n","            with open(t, \"r\") as f:\n","                lines = f.readlines()\n","                all_lines.extend([(line.strip(), t) for line in lines if line.strip()])\n","\n","        for line, src_txt in all_lines:\n","            parts = line.split()\n","            if len(parts) == 0:\n","                continue\n","\n","            fname = parts[0]\n","            if len(parts) >= 2:\n","                try:\n","                    lbl = int(parts[1])\n","                except:\n","                    lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n","            else:\n","                lbl = 1 if \"CAM\" in fname or \"cam\" in fname else 0\n","\n","            # Find which dataset this file belongs to\n","            found = False\n","            # Search in the immediate root, or common Image subfolders, or deep nested folders (CAMO-COCO style)\n","            search_subs = [\n","                \"\",  # If image is directly in root_dir (less common)\n","                \"Image\", \"Imgs\", \"images\", \"JPEGImages\", \"img\", # Common image folders (COD10K style)\n","                # Deep CAMO-COCO style paths (Images/Train/ or Images/Test/ subfolders)\n","                \"Images/Train\", \"Images/Test\",\n","            ]\n","            \n","            # Since fnames might contain sub-directories, we only want the filename itself for checking\n","            base_fname = os.path.basename(fname) \n","\n","            for rdir in self.root_dirs:\n","                for sub in search_subs:\n","                    img_path = os.path.join(rdir, sub, base_fname)\n","                    # For CAMO-COCO Non-Camouflage roots, the image structure is often Images/Train/[fname] \n","                    # but the noncam root is already specific (Non_Camouflage). The robust search handles this.\n","                    \n","                    if os.path.exists(img_path):\n","                        self.samples.append((img_path, lbl, rdir))\n","                        found = True\n","                        break\n","                if found:\n","                    break\n","\n","            if not found:\n","                print(f\"[WARN] File not found in any root: {base_fname} (Searched in {self.root_dirs})\")\n","\n","        if len(self.samples) == 0:\n","            raise RuntimeError(f\"No valid samples found from {txt_files}\")\n","\n","        print(f\"✅ Loaded {len(self.samples)} samples from {len(self.root_dirs)} root directories.\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        # We need IMG_SIZE defined here or passed in during object creation. \n","        # Since it is a global variable in the notebook, we'll assume it's available.\n","        global IMG_SIZE \n","        \n","        img_path, lbl, rdir = self.samples[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.weak_transform:\n","            weak = self.weak_transform(img)\n","        else:\n","            weak = transforms.ToTensor()(img)\n","        if self.strong_transform:\n","            strong = self.strong_transform(img)\n","        else:\n","            strong = weak.clone()\n","\n","        mask = None\n","        if self.use_masks:\n","            mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n","            \n","            # Try multiple common mask dirs\n","            found_mask = False\n","            for mask_dir in [\"GT_Object\", \"GT\", \"masks\", \"Mask\"]:\n","                mask_path = os.path.join(rdir, mask_dir, mask_name)\n","                \n","                # Special handling for CAMO-COCO Non-Camouflage which is missing mask subfolders\n","                if \"Non_Camouflage\" in rdir and mask_dir in [\"GT\", \"GT_Object\"]:\n","                    # Non-Camouflage generally doesn't have GTs, skip searching for them explicitly\n","                    # This check is redundant if the file doesn't exist, but serves as a quick exit\n","                    continue \n","                \n","                if os.path.exists(mask_path):\n","                    m = Image.open(mask_path).convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n","                    m = np.array(m).astype(np.float32) / 255.0\n","                    mask = torch.from_numpy((m > 0.5).astype(np.float32)).unsqueeze(0)\n","                    found_mask = True\n","                    break\n","\n","            # FIX: If mask is not found but use_masks is True, return a zero mask Tensor\n","            if mask is None:\n","                # Create a zero mask of size [1, IMG_SIZE, IMG_SIZE] (1 channel, H, W)\n","                # This ensures the batch collator only deals with Tensors, resolving the error.\n","                mask = torch.zeros((1, IMG_SIZE, IMG_SIZE), dtype=torch.float32)\n","                # print(f\"[DEBUG] No mask found for {mask_name}, returning zero mask.\") # Debugging line\n","                \n","        return weak, strong, lbl, mask\n","\n","\n","def build_weighted_sampler(dataset):\n","    # This must be updated to use the new sample structure (img_path, lbl, rdir)\n","    labels = [lbl for (_, lbl, _) in dataset.samples] \n","    counts = Counter(labels)\n","    total = len(labels)\n","    \n","    # Ensure there are at least two classes to calculate class_weights\n","    if len(counts) <= 1:\n","        print(f\"[WARN] Only {len(counts)} class(es) found. Using equal weights.\")\n","        weights = [1.0] * total\n","    else:\n","        class_weights = {c: total / (counts[c] * len(counts)) for c in counts}\n","        weights = [class_weights[lbl] for lbl in labels]\n","        \n","    return WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n"]},{"cell_type":"code","execution_count":13,"id":"9b925e70","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:07:50.704297Z","iopub.status.busy":"2025-10-29T10:07:50.704066Z","iopub.status.idle":"2025-10-29T10:09:00.292281Z","shell.execute_reply":"2025-10-29T10:09:00.291338Z"},"papermill":{"duration":69.595085,"end_time":"2025-10-29T10:09:00.293611","exception":false,"start_time":"2025-10-29T10:07:50.698526","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Loaded 7999 samples from 4 root directories.\n","✅ Loaded 4500 samples from 4 root directories.\n","Total Train samples: 7999 Total Val samples: 4500\n"]}],"source":["# Assuming ALL_ROOT_DIRS, ALL_TRAIN_TXTS, and ALL_VAL_TXTS have been defined correctly\n","# in the cells preceding this one.\n","\n","# --- PATH AND VARIABLE DEFINITIONS (Consolidated based on user's history and images) ---\n","\n","# [9] COD10K PATHS (Based on image_22ec1d.png)\n","info_dir  = \"/kaggle/input/cod10k/COD10K-v3/Info\"\n","train_dir_cod = \"/kaggle/input/cod10k/COD10K-v3/Train\" # Renamed from original train_dir\n","test_dir_cod  = \"/kaggle/input/cod10k/COD10K-v3/Test\"  # Renamed from original test_dir\n","\n","# COD10K Info files (filenames lists) - Using info_dir\n","train_cam_txt = os.path.join(info_dir, \"CAM_train.txt\")\n","train_noncam_txt = os.path.join(info_dir, \"NonCAM_train.txt\")\n","test_cam_txt = os.path.join(info_dir, \"CAM_test.txt\")\n","test_noncam_txt = os.path.join(info_dir, \"NonCAM_test.txt\")\n","\n","# [10] CAMO-COCO PATHS (Based on image_22ec1d.png)\n","info_dir2 = \"/kaggle/input/camo-coco/CAMO_COCO/Info\"\n","\n","# CAMO-COCO Info files (filenames lists) - Using info_dir2\n","train_cam_txt2 = os.path.join(info_dir2, \"camo_train.txt\")\n","train_noncam_txt2 = os.path.join(info_dir2, \"non_camo_train.txt\")\n","test_cam_txt2 = os.path.join(info_dir2, \"camo_test.txt\")\n","test_noncam_txt2 = os.path.join(info_dir2, \"non_camo_test.txt\")\n","\n","# [11] CAMO-COCO ROOT DIRECTORIES (rdir passed to Dataset, contains Images/ and GT/)\n","# These are the actual root folders containing the image subdirectories (Images/Train, Images/Test)\n","train_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\"\n","train_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\"\n","test_dir_camo_cam = \"/kaggle/input/camo-coco/CAMO_COCO/Camouflage\" # Same root as train for cam\n","test_dir_camo_noncam = \"/kaggle/input/camo-coco/CAMO_COCO/Non_Camouflage\" # Same root as train for noncam\n","\n","\n","# CONSOLIDATED LISTS FOR DATASET INITIALIZATION\n","\n","# 1. All Root Directories where the images/masks are physically stored:\n","ALL_ROOT_DIRS = [\n","    train_dir_cod,          # COD10K Train/\n","    test_dir_cod,           # COD10K Test/\n","    train_dir_camo_cam,     # CAMO-COCO Camouflage/\n","    train_dir_camo_noncam   # CAMO-COCO Non_Camouflage/\n","]\n","\n","# 2. All Train TXT files (containing filenames):\n","ALL_TRAIN_TXTS = [\n","    train_cam_txt, \n","    train_noncam_txt,\n","    train_cam_txt2,\n","    train_noncam_txt2\n","]\n","\n","# 3. All Test/Validation TXT files (containing filenames):\n","ALL_VAL_TXTS = [\n","    test_cam_txt,\n","    test_noncam_txt,\n","    test_cam_txt2,\n","    test_noncam_txt2\n","]\n","\n","# --- END PATH DEFINITIONS ---\n","\n","# Create the final unified datasets\n","train_ds = COD10KDataset(\n","    root_dirs=ALL_ROOT_DIRS, \n","    txt_files=ALL_TRAIN_TXTS, \n","    weak_transform=weak_tf, \n","    strong_transform=strong_tf, \n","    use_masks=USE_SEGMENTATION\n",")\n","\n","val_ds = COD10KDataset(\n","    root_dirs=ALL_ROOT_DIRS,  \n","    txt_files=ALL_VAL_TXTS,  \n","    weak_transform=val_tf, \n","    strong_transform=None, \n","    use_masks=USE_SEGMENTATION\n",")\n","\n","# Build Sampler and DataLoaders\n","train_sampler = build_weighted_sampler(train_ds)\n","\n","# Make sure DataLoader is imported (it should be in the initial imports of your notebook)\n","# BATCH_SIZE and NUM_WORKERS should also be defined previously\n","\n","train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=NUM_WORKERS)\n","val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n","\n","# Corrected print statement: len(val_ds) is an integer, so you cannot call len() on it.\n","print(\"Total Train samples:\", len(train_ds), \"Total Val samples:\", len(val_ds))\n"]},{"cell_type":"markdown","id":"b14bd37b","metadata":{"papermill":{"duration":0.005107,"end_time":"2025-10-29T10:09:00.304179","exception":false,"start_time":"2025-10-29T10:09:00.299072","status":"completed"},"tags":[]},"source":["## Backbones"]},{"cell_type":"code","execution_count":14,"id":"c1b43a2f","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.315041Z","iopub.status.busy":"2025-10-29T10:09:00.314841Z","iopub.status.idle":"2025-10-29T10:09:00.321052Z","shell.execute_reply":"2025-10-29T10:09:00.320367Z"},"papermill":{"duration":0.012942,"end_time":"2025-10-29T10:09:00.322136","exception":false,"start_time":"2025-10-29T10:09:00.309194","status":"completed"},"tags":[]},"outputs":[],"source":["class DenseNetExtractor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.densenet201(pretrained=pretrained).features\n","    def forward(self, x):\n","        feats = []\n","        for name, layer in self.features._modules.items():\n","            x = layer(x)\n","            if name in [\"denseblock1\",\"denseblock2\",\"denseblock3\",\"denseblock4\"]:\n","                feats.append(x)\n","        return feats\n","\n","\n","class MobileNetExtractor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.mobilenet_v3_large(pretrained=pretrained).features\n","    def forward(self, x):\n","        feats = []\n","        out = x\n","        for i, layer in enumerate(self.features):\n","            out = layer(out)\n","            if i in (2,5,9,12):\n","                feats.append(out)\n","        if len(feats) < 4:\n","            feats.append(out)\n","        return feats"]},{"cell_type":"code","execution_count":15,"id":"4b9b5857","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.332972Z","iopub.status.busy":"2025-10-29T10:09:00.332776Z","iopub.status.idle":"2025-10-29T10:09:00.336363Z","shell.execute_reply":"2025-10-29T10:09:00.335873Z"},"papermill":{"duration":0.010203,"end_time":"2025-10-29T10:09:00.337356","exception":false,"start_time":"2025-10-29T10:09:00.327153","status":"completed"},"tags":[]},"outputs":[],"source":["class SwinExtractor(nn.Module):\n","    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", pretrained=True):\n","        super().__init__()\n","        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":16,"id":"9ad4ef20","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.348329Z","iopub.status.busy":"2025-10-29T10:09:00.348073Z","iopub.status.idle":"2025-10-29T10:09:00.352585Z","shell.execute_reply":"2025-10-29T10:09:00.352079Z"},"papermill":{"duration":0.011245,"end_time":"2025-10-29T10:09:00.353567","exception":false,"start_time":"2025-10-29T10:09:00.342322","status":"completed"},"tags":[]},"outputs":[],"source":["class CBAMlite(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super().__init__()\n","        self.se = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(channels, max(channels//reduction,4), 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(max(channels//reduction,4), channels, 1),\n","            nn.Sigmoid()\n","        )\n","        self.spatial = nn.Sequential(\n","            nn.Conv2d(channels, channels, 3, padding=1, groups=channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(channels, 1, 1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        return x * self.se(x) * self.spatial(x)\n"]},{"cell_type":"code","execution_count":17,"id":"a84b0cff","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.364439Z","iopub.status.busy":"2025-10-29T10:09:00.36425Z","iopub.status.idle":"2025-10-29T10:09:00.368638Z","shell.execute_reply":"2025-10-29T10:09:00.368137Z"},"papermill":{"duration":0.010928,"end_time":"2025-10-29T10:09:00.369616","exception":false,"start_time":"2025-10-29T10:09:00.358688","status":"completed"},"tags":[]},"outputs":[],"source":["class GatedFusion(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.g_fc = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(dim, max(dim//4, 4), 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(max(dim//4,4), dim, 1),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, H, X):\n","        if H.shape[2:] != X.shape[2:]:\n","            X = F.interpolate(X, size=H.shape[2:], mode='bilinear', align_corners=False)\n","        g = self.g_fc(H)\n","        return g * H + (1 - g) * X"]},{"cell_type":"code","execution_count":18,"id":"ada9f6b2","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.380318Z","iopub.status.busy":"2025-10-29T10:09:00.380077Z","iopub.status.idle":"2025-10-29T10:09:00.385476Z","shell.execute_reply":"2025-10-29T10:09:00.384974Z"},"papermill":{"duration":0.011837,"end_time":"2025-10-29T10:09:00.386553","exception":false,"start_time":"2025-10-29T10:09:00.374716","status":"completed"},"tags":[]},"outputs":[],"source":["class CrossAttention(nn.Module):\n","    def __init__(self, d_cnn, d_swin, d_out):\n","        super().__init__()\n","        self.q = nn.Linear(d_cnn, d_out)\n","        self.k = nn.Linear(d_swin, d_out)\n","        self.v = nn.Linear(d_swin, d_out)\n","        self.scale = d_out ** -0.5\n","    def forward(self, feat_cnn, feat_swin):\n","        B, Cc, H, W = feat_cnn.shape\n","        q = feat_cnn.permute(0,2,3,1).reshape(B, H*W, Cc)\n","        if feat_swin.dim() == 4:\n","            Bs, Cs, Hs, Ws = feat_swin.shape\n","            kv = feat_swin.permute(0,2,3,1).reshape(Bs, Hs*Ws, Cs)\n","        else:\n","            kv = feat_swin\n","        K = self.k(kv)\n","        V = self.v(kv)\n","        Q = self.q(q)\n","        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        out = torch.matmul(attn, V)\n","        out = out.reshape(B, H, W, -1).permute(0,3,1,2)\n","        return out\n"]},{"cell_type":"markdown","id":"ec3a70f3","metadata":{"papermill":{"duration":0.005006,"end_time":"2025-10-29T10:09:00.396848","exception":false,"start_time":"2025-10-29T10:09:00.391842","status":"completed"},"tags":[]},"source":["## Segmentation Decoder"]},{"cell_type":"code","execution_count":19,"id":"c0765db7","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.407629Z","iopub.status.busy":"2025-10-29T10:09:00.407441Z","iopub.status.idle":"2025-10-29T10:09:00.412619Z","shell.execute_reply":"2025-10-29T10:09:00.412094Z"},"papermill":{"duration":0.011756,"end_time":"2025-10-29T10:09:00.413636","exception":false,"start_time":"2025-10-29T10:09:00.40188","status":"completed"},"tags":[]},"outputs":[],"source":["class SegDecoder(nn.Module):\n","    def __init__(self, in_channels_list, mid_channels=128):\n","        super().__init__()\n","        self.projs = nn.ModuleList([nn.Conv2d(c, mid_channels, 1) for c in in_channels_list])\n","        self.conv = nn.Sequential(nn.Conv2d(mid_channels * len(in_channels_list), mid_channels, 3, padding=1), nn.ReLU(inplace=True))\n","        self.out = nn.Conv2d(mid_channels, 1, 1)\n","    def forward(self, feat_list):\n","        target_size = feat_list[0].shape[2:]\n","        ups = []\n","        for f, p in zip(feat_list, self.projs):\n","            x = p(f)\n","            if x.shape[2:] != target_size:\n","                x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n","            ups.append(x)\n","        x = torch.cat(ups, dim=1)\n","        x = self.conv(x)\n","        x = self.out(x)\n","        return x"]},{"cell_type":"markdown","id":"743bef01","metadata":{"papermill":{"duration":0.004822,"end_time":"2025-10-29T10:09:00.423432","exception":false,"start_time":"2025-10-29T10:09:00.41861","status":"completed"},"tags":[]},"source":["## Probing Backbones"]},{"cell_type":"code","execution_count":20,"id":"20b56582","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:00.435205Z","iopub.status.busy":"2025-10-29T10:09:00.434978Z","iopub.status.idle":"2025-10-29T10:09:05.136341Z","shell.execute_reply":"2025-10-29T10:09:05.135539Z"},"papermill":{"duration":4.708909,"end_time":"2025-10-29T10:09:05.137568","exception":false,"start_time":"2025-10-29T10:09:00.428659","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n","100%|██████████| 77.4M/77.4M [00:00<00:00, 208MB/s]\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n","100%|██████████| 21.1M/21.1M [00:00<00:00, 144MB/s] \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"169246f7f3e74f849548a906291dbc3e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DenseNet channels: [256, 512, 1792, 1920]\n","MobileNet channels: [24, 40, 80, 112]\n","Swin channels: [56, 28, 14, 7]\n"]}],"source":["dnet = DenseNetExtractor().to(device).eval()\n","mnet = MobileNetExtractor().to(device).eval()\n","snet = SwinExtractor().to(device).eval()\n","with torch.no_grad():\n","    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(device)\n","    featsA = dnet(dummy)\n","    featsB = mnet(dummy)\n","    featsS = snet(dummy)\n","chA = [f.shape[1] for f in featsA]\n","chB = [f.shape[1] for f in featsB]\n","chS = [f.shape[1] for f in featsS]\n","print(\"DenseNet channels:\", chA)\n","print(\"MobileNet channels:\", chB)\n","print(\"Swin channels:\", chS)"]},{"cell_type":"markdown","id":"70575120","metadata":{"papermill":{"duration":0.005769,"end_time":"2025-10-29T10:09:05.149494","exception":false,"start_time":"2025-10-29T10:09:05.143725","status":"completed"},"tags":[]},"source":["# Fusion Model (DenseNet + MobileNet + Swin cross attention)"]},{"cell_type":"code","execution_count":21,"id":"cb8d8b2d","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:05.161865Z","iopub.status.busy":"2025-10-29T10:09:05.161628Z","iopub.status.idle":"2025-10-29T10:09:06.397841Z","shell.execute_reply":"2025-10-29T10:09:06.396965Z"},"papermill":{"duration":1.243925,"end_time":"2025-10-29T10:09:06.399134","exception":false,"start_time":"2025-10-29T10:09:05.155209","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model parameters (M): 51.586615\n"]}],"source":["class FusionWithSwin(nn.Module):\n","    def __init__(self, dense_chs, mobile_chs, swin_chs, d=256, use_seg=True, num_classes=2):\n","        super().__init__()\n","        self.backA = DenseNetExtractor()\n","        self.backB = MobileNetExtractor()\n","        self.backS = SwinExtractor()\n","        L = min(len(dense_chs), len(mobile_chs), len(swin_chs))\n","        self.L = L\n","        self.d = d\n","        self.alignA = nn.ModuleList([nn.Conv2d(c, d, 1) for c in dense_chs[:L]])\n","        self.alignB = nn.ModuleList([nn.Conv2d(c, d, 1) for c in mobile_chs[:L]])\n","        self.cbamA = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n","        self.cbamB = nn.ModuleList([CBAMlite(d) for _ in range(L)])\n","        self.gates = nn.ModuleList([GatedFusion(d) for _ in range(L)])\n","        self.cross_atts = nn.ModuleList([CrossAttention(d, swin_chs[i], d) for i in range(L)])\n","        self.reduce = nn.Conv2d(d * L, d, 1)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(d, 512), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(128, num_classes)\n","        )\n","        self.use_seg = use_seg\n","        if self.use_seg:\n","            self.segdecoder = SegDecoder([d] * L, mid_channels=128)\n","\n","        # Domain head for DANN (simple MLP)\n","        self.domain_head = nn.Sequential(\n","            nn.Linear(d, 256), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(256, 2)\n","        )\n","\n","    def forward(self, x, grl_lambda=0.0):\n","        fa = self.backA(x)\n","        fb = self.backB(x)\n","        fs = self.backS(x)\n","        fused_feats = []\n","        aligned_for_dec = []\n","        for i in range(self.L):\n","            a = self.alignA[i](fa[i])\n","            a = self.cbamA[i](a)\n","            b = self.alignB[i](fb[i])\n","            b = self.cbamB[i](b)\n","            if b.shape[2:] != a.shape[2:]:\n","                b = F.interpolate(b, size=a.shape[2:], mode='bilinear', align_corners=False)\n","            fused = self.gates[i](a, b)\n","            swin_feat = fs[i]\n","            swin_att = self.cross_atts[i](fused, swin_feat)\n","            if swin_att.shape[2:] != fused.shape[2:]:\n","                swin_att = F.interpolate(swin_att, size=fused.shape[2:], mode='bilinear', align_corners=False)\n","            fused = fused + swin_att\n","            fused_feats.append(fused)\n","            aligned_for_dec.append(fused)\n","        target = fused_feats[-1]\n","        upsampled = [F.interpolate(f, size=target.shape[2:], mode='bilinear', align_corners=False) if f.shape[2:] != target.shape[2:] else f for f in fused_feats]\n","        concat = torch.cat(upsampled, dim=1)\n","        fused = self.reduce(concat)\n","        z = F.adaptive_avg_pool2d(fused, (1,1)).view(fused.size(0), -1)\n","        logits = self.classifier(z)\n","        out = {\"logits\": logits, \"feat\": z}\n","        if self.use_seg:\n","            out[\"seg\"] = self.segdecoder(aligned_for_dec)\n","\n","        # Domain prediction with GRL effect applied by multiplying lambda and reversing sign in custom grad fn\n","        if grl_lambda > 0.0:\n","            # GRL implemented outside (we'll pass z through GRL function)\n","            pass\n","        out[\"domain_logits\"] = self.domain_head(z)\n","        return out\n","\n","# instantiate model\n","model = FusionWithSwin(dense_chs=chA, mobile_chs=chB, swin_chs=chS, d=256, use_seg=USE_SEGMENTATION, num_classes=2).to(device)\n","print(\"Model parameters (M):\", sum(p.numel() for p in model.parameters())/1e6)"]},{"cell_type":"code","execution_count":22,"id":"af9a5be7","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:06.413923Z","iopub.status.busy":"2025-10-29T10:09:06.41347Z","iopub.status.idle":"2025-10-29T10:09:06.423982Z","shell.execute_reply":"2025-10-29T10:09:06.423247Z"},"papermill":{"duration":0.018358,"end_time":"2025-10-29T10:09:06.425103","exception":false,"start_time":"2025-10-29T10:09:06.406745","status":"completed"},"tags":[]},"outputs":[],"source":["class LabelSmoothingCE(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        self.s = smoothing\n","    def forward(self, logits, target):\n","        c = logits.size(-1)\n","        logp = F.log_softmax(logits, dim=-1)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(logp)\n","            true_dist.fill_(self.s / (c - 1))\n","            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.s)\n","        return (-true_dist * logp).sum(dim=-1).mean()\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=1.5):\n","        super().__init__()\n","        self.gamma = gamma\n","    def forward(self, logits, target):\n","        prob = F.softmax(logits, dim=1)\n","        pt = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n","        ce = F.cross_entropy(logits, target, reduction='none')\n","        loss = ((1 - pt) ** self.gamma) * ce\n","        return loss.mean()\n","\n","def dice_loss_logits(pred_logits, target):\n","    pred = torch.sigmoid(pred_logits)\n","    target = target.float()\n","    inter = (pred * target).sum(dim=(1,2,3))\n","    denom = pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3))\n","    dice = (2 * inter + 1e-6) / (denom + 1e-6)\n","    return 1.0 - dice.mean()\n","\n","clf_loss_ce = LabelSmoothingCE(LABEL_SMOOTH)\n","clf_loss_focal = FocalLoss(gamma=1.5)\n","seg_bce = nn.BCEWithLogitsLoss()\n","\n","def dice_loss(pred, target, smooth=1.0):\n","    pred = torch.sigmoid(pred)\n","    num = 2 * (pred * target).sum() + smooth\n","    den = pred.sum() + target.sum() + smooth\n","    return 1 - (num / den)\n","\n","def seg_loss_fn(pred, mask):\n","    if pred.shape[-2:] != mask.shape[-2:]:\n","        pred = F.interpolate(pred, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n","    return F.binary_cross_entropy_with_logits(pred, mask) + dice_loss(pred, mask)\n"]},{"cell_type":"code","execution_count":23,"id":"56d50600","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:06.437521Z","iopub.status.busy":"2025-10-29T10:09:06.437072Z","iopub.status.idle":"2025-10-29T10:09:06.443909Z","shell.execute_reply":"2025-10-29T10:09:06.443029Z"},"papermill":{"duration":0.014473,"end_time":"2025-10-29T10:09:06.445384","exception":false,"start_time":"2025-10-29T10:09:06.430911","status":"completed"},"tags":[]},"outputs":[],"source":["#Supervised contrastive Loss\n","class SupConLoss(nn.Module):\n","    def __init__(self, temperature=0.07):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.cos = nn.CosineSimilarity(dim=-1)\n","    def forward(self, features, labels):\n","        # features: [N, D], labels: [N]\n","        device = features.device\n","        f = F.normalize(features, dim=1)\n","        sim = torch.matmul(f, f.T) / self.temperature  # [N,N]\n","        labels = labels.contiguous().view(-1,1)\n","        mask = torch.eq(labels, labels.T).float().to(device)\n","        # remove diagonal\n","        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n","        logits = sim - logits_max.detach()\n","        exp_logits = torch.exp(logits) * (1 - torch.eye(len(features), device=device))\n","        denom = exp_logits.sum(1, keepdim=True)\n","        # for each i, positive samples are where mask==1 (excluding self)\n","        pos_mask = mask - torch.eye(len(features), device=device)\n","        pos_exp = (exp_logits * pos_mask).sum(1)\n","        # avoid divide by zero\n","        loss = -torch.log((pos_exp + 1e-8) / (denom + 1e-8) + 1e-12)\n","        # average only across anchors that have positives\n","        valid = (pos_mask.sum(1) > 0).float()\n","        loss = (loss * valid).sum() / (valid.sum() + 1e-8)\n","        return loss\n","supcon_loss_fn = SupConLoss(temperature=0.07)"]},{"cell_type":"code","execution_count":24,"id":"68a265ed","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:06.45822Z","iopub.status.busy":"2025-10-29T10:09:06.45773Z","iopub.status.idle":"2025-10-29T10:09:06.462069Z","shell.execute_reply":"2025-10-29T10:09:06.461407Z"},"papermill":{"duration":0.011736,"end_time":"2025-10-29T10:09:06.463098","exception":false,"start_time":"2025-10-29T10:09:06.451362","status":"completed"},"tags":[]},"outputs":[],"source":["# Domain Adversarial: Gradient Reversal Layer (GRL)\n","\n","from torch.autograd import Function\n","class GradReverse(Function):\n","    @staticmethod\n","    def forward(ctx, x, l):\n","        ctx.l = l\n","        return x.view_as(x)\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return grad_output.neg() * ctx.l, None\n","\n","def grad_reverse(x, l=1.0):\n","    return GradReverse.apply(x, l)"]},{"cell_type":"code","execution_count":25,"id":"10d8be37","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:06.475913Z","iopub.status.busy":"2025-10-29T10:09:06.475682Z","iopub.status.idle":"2025-10-29T10:09:06.489874Z","shell.execute_reply":"2025-10-29T10:09:06.48915Z"},"papermill":{"duration":0.022321,"end_time":"2025-10-29T10:09:06.491132","exception":false,"start_time":"2025-10-29T10:09:06.468811","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_19/876045050.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"]}],"source":["# Optimizer + scheduler + mixed precision + clipping\n","# -----------------------------\n","# param groups: smaller LR for backbones, larger for heads\n","backbone_params = []\n","head_params = []\n","for name, param in model.named_parameters():\n","    if any(k in name for k in ['backA', 'backB', 'backS']):  # backbone names\n","        backbone_params.append(param)\n","    else:\n","        head_params.append(param)\n","\n","opt = torch.optim.AdamW([\n","    {'params': backbone_params, 'lr': LR * 0.2},\n","    {'params': head_params, 'lr': LR}\n","], lr=LR, weight_decay=1e-4)\n","\n","# warmup + cosine schedule\n","def get_cosine_with_warmup_scheduler(optimizer, warmup_epochs, total_epochs, last_epoch=-1):\n","    def lr_lambda(epoch):\n","        if epoch < warmup_epochs:\n","            return float(epoch) / float(max(1.0, warmup_epochs))\n","        # cosine from warmup -> total\n","        t = (epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n","        return 0.5 * (1.0 + math.cos(math.pi * t))\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n","\n","scheduler = get_cosine_with_warmup_scheduler(opt, WARMUP_EPOCHS, EPOCHS)\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n","\n","# -----------------------------\n","# Mixup & CutMix helpers\n","# -----------------------------\n","def rand_bbox(size, lam):\n","    W = size[2]\n","    H = size[3]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)   # use builtin int\n","    cut_h = int(H * cut_rat)   # use builtin int\n","\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","def apply_mixup(x, y, alpha=MIXUP_ALPHA):\n","    lam = np.random.beta(alpha, alpha)\n","    idx = torch.randperm(x.size(0))\n","    mixed_x = lam * x + (1 - lam) * x[idx]\n","    y_a, y_b = y, y[idx]\n","    return mixed_x, y_a, y_b, lam\n","\n","def apply_cutmix(x, y, alpha=CUTMIX_ALPHA):\n","    lam = np.random.beta(alpha, alpha)\n","    idx = torch.randperm(x.size(0))\n","    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n","    new_x = x.clone()\n","    new_x[:, :, bby1:bby2, bbx1:bbx2] = x[idx, :, bby1:bby2, bbx1:bbx2]\n","    lam_adjusted = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n","    return new_x, y, y[idx], lam_adjusted\n"]},{"cell_type":"markdown","id":"37d32d07","metadata":{"papermill":{"duration":0.0057,"end_time":"2025-10-29T10:09:06.502628","exception":false,"start_time":"2025-10-29T10:09:06.496928","status":"completed"},"tags":[]},"source":["## Training"]},{"cell_type":"code","execution_count":26,"id":"7ca810ea","metadata":{"execution":{"iopub.execute_input":"2025-10-29T10:09:06.515272Z","iopub.status.busy":"2025-10-29T10:09:06.515018Z","iopub.status.idle":"2025-10-29T15:32:30.509446Z","shell.execute_reply":"2025-10-29T15:32:30.508647Z"},"papermill":{"duration":19404.002374,"end_time":"2025-10-29T15:32:30.510836","exception":false,"start_time":"2025-10-29T10:09:06.508462","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Train 1/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 1/20: 100%|██████████| 1000/1000 [14:04<00:00,  1.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 1] Train Loss: 4.1229 Acc: 0.5057 Prec: 0.5048 Rec: 0.5045 F1: 0.4961\n","[Epoch 1] Val Loss: 2.4864 Acc: 0.5838 Prec: 0.5766 Rec: 0.5437 F1: 0.5079\n","Saved best model at epoch 1 (F1 0.5079)\n"]},{"name":"stderr","output_type":"stream","text":["Train 2/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 2/20: 100%|██████████| 1000/1000 [13:18<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 2] Train Loss: 2.8040 Acc: 0.6960 Prec: 0.6960 Rec: 0.6961 F1: 0.6960\n","[Epoch 2] Val Loss: 2.0169 Acc: 0.8076 Prec: 0.8227 Rec: 0.7915 F1: 0.7970\n","Saved best model at epoch 2 (F1 0.7970)\n"]},{"name":"stderr","output_type":"stream","text":["Train 3/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 3/20: 100%|██████████| 1000/1000 [12:58<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 3] Train Loss: 2.4700 Acc: 0.7731 Prec: 0.7733 Rec: 0.7733 F1: 0.7731\n","[Epoch 3] Val Loss: 1.4709 Acc: 0.9029 Prec: 0.9010 Rec: 0.9023 F1: 0.9016\n","Saved best model at epoch 3 (F1 0.9016)\n"]},{"name":"stderr","output_type":"stream","text":["Train 4/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 4/20: 100%|██████████| 1000/1000 [12:47<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 4] Train Loss: 2.3447 Acc: 0.7886 Prec: 0.7886 Rec: 0.7886 F1: 0.7886\n","[Epoch 4] Val Loss: 1.5159 Acc: 0.8982 Prec: 0.8979 Rec: 0.8950 F1: 0.8963\n"]},{"name":"stderr","output_type":"stream","text":["Train 5/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 5/20: 100%|██████████| 1000/1000 [12:49<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 5] Train Loss: 2.3592 Acc: 0.7820 Prec: 0.7820 Rec: 0.7820 F1: 0.7820\n","[Epoch 5] Val Loss: 1.4274 Acc: 0.8771 Prec: 0.8940 Rec: 0.8642 F1: 0.8714\n"]},{"name":"stderr","output_type":"stream","text":["Train 6/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 6/20: 100%|██████████| 1000/1000 [12:51<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 6] Train Loss: 2.3152 Acc: 0.7892 Prec: 0.7894 Rec: 0.7893 F1: 0.7892\n","[Epoch 6] Val Loss: 1.5198 Acc: 0.9042 Prec: 0.9063 Rec: 0.8992 F1: 0.9020\n","Saved best model at epoch 6 (F1 0.9020)\n"]},{"name":"stderr","output_type":"stream","text":["Train 7/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 7/20: 100%|██████████| 1000/1000 [12:50<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 7] Train Loss: 2.3145 Acc: 0.7952 Prec: 0.7954 Rec: 0.7955 F1: 0.7952\n","[Epoch 7] Val Loss: 1.4672 Acc: 0.8447 Prec: 0.8792 Rec: 0.8254 F1: 0.8335\n"]},{"name":"stderr","output_type":"stream","text":["Train 8/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 8/20: 100%|██████████| 1000/1000 [12:45<00:00,  1.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 8] Train Loss: 2.2546 Acc: 0.8025 Prec: 0.8026 Rec: 0.8024 F1: 0.8024\n","[Epoch 8] Val Loss: 1.4292 Acc: 0.8896 Prec: 0.8987 Rec: 0.8801 F1: 0.8856\n"]},{"name":"stderr","output_type":"stream","text":["Train 9/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 9/20: 100%|██████████| 1000/1000 [12:46<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 9] Train Loss: 2.2150 Acc: 0.8060 Prec: 0.8062 Rec: 0.8059 F1: 0.8059\n","[Epoch 9] Val Loss: 1.3897 Acc: 0.9098 Prec: 0.9077 Rec: 0.9097 F1: 0.9086\n","Saved best model at epoch 9 (F1 0.9086)\n"]},{"name":"stderr","output_type":"stream","text":["Train 10/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 10/20: 100%|██████████| 1000/1000 [12:46<00:00,  1.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 10] Train Loss: 2.1977 Acc: 0.8115 Prec: 0.8115 Rec: 0.8115 F1: 0.8115\n","[Epoch 10] Val Loss: 1.3956 Acc: 0.9078 Prec: 0.9080 Rec: 0.9044 F1: 0.9060\n"]},{"name":"stderr","output_type":"stream","text":["Train 11/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 11/20: 100%|██████████| 1000/1000 [13:00<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 11] Train Loss: 2.1152 Acc: 0.8240 Prec: 0.8243 Rec: 0.8237 F1: 0.8238\n","[Epoch 11] Val Loss: 1.3947 Acc: 0.9184 Prec: 0.9168 Rec: 0.9179 F1: 0.9173\n","Saved best model at epoch 11 (F1 0.9173)\n"]},{"name":"stderr","output_type":"stream","text":["Train 12/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 12/20: 100%|██████████| 1000/1000 [12:58<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 12] Train Loss: 2.0701 Acc: 0.8265 Prec: 0.8265 Rec: 0.8265 F1: 0.8265\n","[Epoch 12] Val Loss: 1.3643 Acc: 0.9180 Prec: 0.9213 Rec: 0.9127 F1: 0.9160\n"]},{"name":"stderr","output_type":"stream","text":["Train 13/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 13/20: 100%|██████████| 1000/1000 [13:00<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 13] Train Loss: 2.0014 Acc: 0.8449 Prec: 0.8449 Rec: 0.8448 F1: 0.8449\n","[Epoch 13] Val Loss: 1.3647 Acc: 0.9100 Prec: 0.9176 Rec: 0.9021 F1: 0.9071\n"]},{"name":"stderr","output_type":"stream","text":["Train 14/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 14/20: 100%|██████████| 1000/1000 [12:57<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 14] Train Loss: 1.9578 Acc: 0.8387 Prec: 0.8387 Rec: 0.8387 F1: 0.8387\n","[Epoch 14] Val Loss: 1.3658 Acc: 0.9129 Prec: 0.9139 Rec: 0.9090 F1: 0.9111\n"]},{"name":"stderr","output_type":"stream","text":["Train 15/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 15/20: 100%|██████████| 1000/1000 [13:01<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 15] Train Loss: 1.9128 Acc: 0.8504 Prec: 0.8504 Rec: 0.8504 F1: 0.8504\n","[Epoch 15] Val Loss: 1.3242 Acc: 0.9293 Prec: 0.9290 Rec: 0.9273 F1: 0.9281\n","Saved best model at epoch 15 (F1 0.9281)\n"]},{"name":"stderr","output_type":"stream","text":["Train 16/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 16/20: 100%|██████████| 1000/1000 [13:01<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 16] Train Loss: 1.8889 Acc: 0.8501 Prec: 0.8501 Rec: 0.8501 F1: 0.8501\n","[Epoch 16] Val Loss: 1.3285 Acc: 0.9200 Prec: 0.9208 Rec: 0.9166 F1: 0.9184\n"]},{"name":"stderr","output_type":"stream","text":["Train 17/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 17/20: 100%|██████████| 1000/1000 [13:03<00:00,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 17] Train Loss: 1.8370 Acc: 0.8626 Prec: 0.8627 Rec: 0.8627 F1: 0.8626\n","[Epoch 17] Val Loss: 1.3108 Acc: 0.9273 Prec: 0.9279 Rec: 0.9244 F1: 0.9259\n"]},{"name":"stderr","output_type":"stream","text":["Train 18/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 18/20: 100%|██████████| 1000/1000 [13:13<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 18] Train Loss: 1.8399 Acc: 0.8602 Prec: 0.8602 Rec: 0.8602 F1: 0.8602\n","[Epoch 18] Val Loss: 1.3038 Acc: 0.9273 Prec: 0.9296 Rec: 0.9231 F1: 0.9257\n"]},{"name":"stderr","output_type":"stream","text":["Train 19/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 19/20: 100%|██████████| 1000/1000 [13:13<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 19] Train Loss: 1.8066 Acc: 0.8625 Prec: 0.8625 Rec: 0.8625 F1: 0.8625\n","[Epoch 19] Val Loss: 1.3017 Acc: 0.9296 Prec: 0.9298 Rec: 0.9270 F1: 0.9283\n","Saved best model at epoch 19 (F1 0.9283)\n"]},{"name":"stderr","output_type":"stream","text":["Train 20/20:   0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_19/1965969712.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","Train 20/20: 100%|██████████| 1000/1000 [13:14<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 20] Train Loss: 1.8155 Acc: 0.8497 Prec: 0.8498 Rec: 0.8497 F1: 0.8497\n","[Epoch 20] Val Loss: 1.3001 Acc: 0.9298 Prec: 0.9301 Rec: 0.9271 F1: 0.9285\n","Saved best model at epoch 20 (F1 0.9285)\n","Training finished. Best val F1: 0.9284544716860835 at epoch 20\n"]}],"source":["best_vf1 = 0.0\n","best_epoch = 0\n","patience_count = 0\n","\n","def compute_combined_clf_loss(logits, targets, mix_info=None, use_focal=False):\n","    # mix_info: (mode, y_a, y_b, lam) or None\n","    if mix_info is None:\n","        if use_focal:\n","            return clf_loss_focal(logits, targets)\n","        else:\n","            return clf_loss_ce(logits, targets)\n","    else:\n","        # mixup/cutmix: soft labels\n","        y_a, y_b, lam = mix_info\n","        if use_focal:\n","            # focal is not designed for soft labels; approximate by weighted CE\n","            loss = lam * F.cross_entropy(logits, y_a) + (1 - lam) * F.cross_entropy(logits, y_b)\n","        else:\n","            loss = lam * clf_loss_ce(logits, y_a) + (1 - lam) * clf_loss_ce(logits, y_b)\n","        return loss\n","for epoch in range(1, EPOCHS+1):\n","    # freeze/unfreeze strategy\n","    if epoch <= FREEZE_EPOCHS:\n","        # freeze early layers of backbones\n","        for name, p in model.named_parameters():\n","            if any(k in name for k in ['backA.features.conv0','backA.features.norm0','backA.features.denseblock1']):\n","                p.requires_grad = False\n","    else:\n","        for p in model.parameters():\n","            p.requires_grad = True\n","\n","\n","    model.train()\n","    running_loss = 0.0\n","    y_true, y_pred = [], []\n","    n_batches = 0\n","\n","    for weak_imgs, strong_imgs, labels, masks in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n","        weak_imgs = weak_imgs.to(device); strong_imgs = strong_imgs.to(device)\n","        labels = labels.to(device)\n","        if masks is not None:\n","            masks = masks.to(device)\n","\n","        # combine weak and strong optionally for the classifier path; we'll feed weak to model for main forward\n","        imgs = weak_imgs\n","\n","        # optionally apply mixup/cutmix on imgs (on weak view)\n","        mix_info = None\n","        rand = random.random()\n","        if rand < PROB_MIXUP:\n","            imgs, y_a, y_b, lam = apply_mixup(imgs, labels)\n","            mix_info = (y_a.to(device), y_b.to(device), lam)\n","        elif rand < PROB_MIXUP + PROB_CUTMIX:\n","            imgs, y_a, y_b, lam = apply_cutmix(imgs, labels)\n","            mix_info = (y_a.to(device), y_b.to(device), lam)\n","\n","        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n","            out = model(imgs)  # returns logits, feat, seg, domain_logits\n","            logits = out[\"logits\"]\n","            feat = out[\"feat\"]\n","            seg_out = out.get(\"seg\", None)\n","            domain_logits = out.get(\"domain_logits\", None)\n","\n","            # classification loss (label-smoothing or focal)\n","            clf_loss = compute_combined_clf_loss(logits, labels, mix_info=mix_info, use_focal=False)\n","\n","            # segmentation loss if available & mask present\n","            seg_loss = 0.0\n","            if USE_SEGMENTATION and (masks is not None):\n","                seg_pred = out[\"seg\"]\n","                seg_loss = seg_loss_fn(seg_pred, masks)\n","            # supcon loss on features (use features from weak)\n","            supcon_loss = supcon_loss_fn(feat, labels)\n","\n","            # consistency: forward strong view and compare predictions\n","            out_strong = model(strong_imgs)\n","            logits_strong = out_strong[\"logits\"]\n","            probs_weak = F.softmax(logits.detach(), dim=1)\n","            probs_strong = F.softmax(logits_strong, dim=1)\n","            # L2 between probability vectors (could be KL)\n","            cons_loss = F.mse_loss(probs_weak, probs_strong)\n","            # domain adversarial: need domain labels; for now assume source-only (skip) unless domain label available\n","            # To support domain adaptation, user should provide target dataloader and stack batches with domain labels\n","            dom_loss = 0.0\n","            # (If domain labels are provided, compute dom logits after GRL: domain_logits_grl = domain_head(grad_reverse(feat, l)))\n","            # then dom_loss = criterion(domain_logits_grl, domain_labels)\n","\n","            total_loss = clf_loss + seg_loss + BETA_SUPCON * supcon_loss + ETA_CONS * cons_loss + ALPHA_DOM * dom_loss\n","\n","        opt.zero_grad()\n","        scaler.scale(total_loss).backward()\n","        # gradient clipping\n","        scaler.unscale_(opt)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        scaler.step(opt)\n","        scaler.update()\n","\n","        running_loss += total_loss.item()\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(logits.argmax(1).cpu().numpy())\n","        n_batches += 1\n","\n","    scheduler.step()\n","\n","    # metrics\n","    acc = accuracy_score(y_true, y_pred)\n","    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n","    print(f\"[Epoch {epoch}] Train Loss: {running_loss/max(1,n_batches):.4f} Acc: {acc:.4f} Prec: {prec:.4f} Rec: {rec:.4f} F1: {f1:.4f}\")\n","\n","    # -------------------\n","    # VALIDATION\n","    # -------------------\n","    model.eval()\n","    val_y_true, val_y_pred = [], []\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for weak_imgs, _, labels, masks in val_loader:\n","            imgs = weak_imgs.to(device)\n","            labels = labels.to(device)\n","            if masks is not None:\n","                masks = masks.to(device)\n","\n","            out = model(imgs)\n","            logits = out[\"logits\"]\n","            feat = out[\"feat\"]\n","            seg_out = out.get(\"seg\", None)\n","            loss = compute_combined_clf_loss(logits, labels, mix_info=None, use_focal=False)\n","            if USE_SEGMENTATION and (masks is not None):\n","                loss += seg_loss_fn(seg_out, masks)\n","            val_loss += loss.item()\n","\n","            val_y_true.extend(labels.cpu().numpy())\n","            val_y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","    vacc = accuracy_score(val_y_true, val_y_pred)\n","    vprec, vrec, vf1, _ = precision_recall_fscore_support(val_y_true, val_y_pred, average=\"macro\", zero_division=0)\n","    print(f\"[Epoch {epoch}] Val Loss: {val_loss/max(1,len(val_loader)):.4f} Acc: {vacc:.4f} Prec: {vprec:.4f} Rec: {vrec:.4f} F1: {vf1:.4f}\")\n","\n","    # early stopping & save best\n","    if vf1 > best_vf1:\n","        best_vf1 = vf1\n","        best_epoch = epoch\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"model_state\": model.state_dict(),\n","            \"opt_state\": opt.state_dict(),\n","            \"best_vf1\": best_vf1\n","        }, SAVE_PATH)\n","        patience_count = 0\n","        print(f\"Saved best model at epoch {epoch} (F1 {best_vf1:.4f})\")\n","    else:\n","        patience_count += 1\n","        if patience_count >= EARLY_STOPPING_PATIENCE:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","print(\"Training finished. Best val F1:\", best_vf1, \"at epoch\", best_epoch)\n"]},{"cell_type":"code","execution_count":27,"id":"f83a43a5","metadata":{"execution":{"iopub.execute_input":"2025-10-29T15:32:32.163709Z","iopub.status.busy":"2025-10-29T15:32:32.163443Z","iopub.status.idle":"2025-10-29T15:32:32.169351Z","shell.execute_reply":"2025-10-29T15:32:32.168635Z"},"papermill":{"duration":0.79793,"end_time":"2025-10-29T15:32:32.170556","exception":false,"start_time":"2025-10-29T15:32:31.372626","status":"completed"},"tags":[]},"outputs":[],"source":["# Test-time augmentation (TTA) helper\n","# -----------------------------\n","def tta_predict(model, img_pil, device=device, scales=[224, 288, 320], flip=True):\n","    model.eval()\n","    logits_accum = None\n","    with torch.no_grad():\n","        for s in scales:\n","            tf = transforms.Compose([\n","                transforms.Resize((s, s)),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","            ])\n","            x = tf(img_pil).unsqueeze(0).to(device)\n","            out = model(x)\n","            logits = out[\"logits\"]\n","            if flip:\n","                x_f = torch.flip(x, dims=[3])\n","                logits_f = model(x_f)[\"logits\"]\n","                logits = (logits + logits_f) / 2.0\n","            if logits_accum is None:\n","                logits_accum = logits\n","            else:\n","                logits_accum += logits\n","    logits_accum /= len(scales)\n","    return logits_accum"]},{"cell_type":"code","execution_count":28,"id":"efc1b37a","metadata":{"execution":{"iopub.execute_input":"2025-10-29T15:32:33.830285Z","iopub.status.busy":"2025-10-29T15:32:33.829609Z","iopub.status.idle":"2025-10-29T15:32:33.838237Z","shell.execute_reply":"2025-10-29T15:32:33.837479Z"},"papermill":{"duration":0.792018,"end_time":"2025-10-29T15:32:33.839342","exception":false,"start_time":"2025-10-29T15:32:33.047324","status":"completed"},"tags":[]},"outputs":[],"source":["# Grad-CAM helper (very simple)\n","# -----------------------------\n","def get_gradcam_heatmap(model, input_tensor, target_class=None, layer_name='backA.features.denseblock4'):\n","    \"\"\"\n","    Very light Grad-CAM: find a conv layer by name, register hook, compute gradients wrt target logit.\n","    Returns upsampled heatmap (H,W) normalized in [0,1].\n","    \"\"\"\n","    model.eval()\n","    # find layer\n","    target_module = None\n","    for name, module in model.named_modules():\n","        if name == layer_name:\n","            target_module = module\n","            break\n","    if target_module is None:\n","        raise RuntimeError(\"Layer not found for Grad-CAM: \" + layer_name)\n","\n","    activations = []\n","    gradients = []\n","\n","    def forward_hook(module, input, output):\n","        activations.append(output.detach())\n","    def backward_hook(module, grad_in, grad_out):\n","        gradients.append(grad_out[0].detach())\n","\n","    h1 = target_module.register_forward_hook(forward_hook)\n","    h2 = target_module.register_full_backward_hook(backward_hook)\n","\n","    out = model(input_tensor)\n","    logits = out[\"logits\"]\n","    if target_class is None:\n","        target_class = logits.argmax(1).item()\n","    loss = logits[:, target_class].sum()\n","    model.zero_grad()\n","    loss.backward(retain_graph=True)\n","\n","    act = activations[0]  # [B,C,H,W]\n","    grad = gradients[0]   # [B,C,H,W]\n","    weights = grad.mean(dim=(2,3), keepdim=True)  # [B,C,1,1]\n","    cam = (weights * act).sum(dim=1, keepdim=True)  # [B,1,H,W]\n","    cam = F.relu(cam)\n","    cam = F.interpolate(cam, size=(input_tensor.size(2), input_tensor.size(3)), mode='bilinear', align_corners=False)\n","    cam = cam.squeeze().cpu().numpy()\n","    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n","    h1.remove(); h2.remove()\n","    return cam"]},{"cell_type":"code","execution_count":null,"id":"6b38aa37","metadata":{"papermill":{"duration":0.83123,"end_time":"2025-10-29T15:32:35.586391","exception":false,"start_time":"2025-10-29T15:32:34.755161","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2932761,"sourceId":5051281,"sourceType":"datasetVersion"},{"datasetId":8580489,"sourceId":13514489,"sourceType":"datasetVersion"},{"datasetId":8582404,"sourceId":13517101,"sourceType":"datasetVersion"},{"datasetId":8591775,"sourceId":13530866,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":19505.125911,"end_time":"2025-10-29T15:32:38.341213","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-29T10:07:33.215302","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"062ba5276c7148fcbe8239d6eb115545":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c9d8aa76969a46b4b5b4f12dce50be60","placeholder":"​","style":"IPY_MODEL_ebf4a82fe4a2459f97511ecf969574c7","tabbable":null,"tooltip":null,"value":" 114M/114M [00:01&lt;00:00, 113MB/s]"}},"0818385b7b244a68aa9fc399862804e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d39fd8d01d7040de9be5fb6873f0dc09","placeholder":"​","style":"IPY_MODEL_56c391c04bf7479ead79bcbc9510ec2e","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"169246f7f3e74f849548a906291dbc3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0818385b7b244a68aa9fc399862804e2","IPY_MODEL_502576ca89c9413db42554154481ff28","IPY_MODEL_062ba5276c7148fcbe8239d6eb115545"],"layout":"IPY_MODEL_73625cb8b068493b818fa30c7f368cf6","tabbable":null,"tooltip":null}},"1dd2a8b0486443b8a638c758a95f375e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"502576ca89c9413db42554154481ff28":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_1dd2a8b0486443b8a638c758a95f375e","max":114286722.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_f1e046591daf4d328635d25855bbda0a","tabbable":null,"tooltip":null,"value":114286722.0}},"56c391c04bf7479ead79bcbc9510ec2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"73625cb8b068493b818fa30c7f368cf6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9d8aa76969a46b4b5b4f12dce50be60":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d39fd8d01d7040de9be5fb6873f0dc09":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebf4a82fe4a2459f97511ecf969574c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f1e046591daf4d328635d25855bbda0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}